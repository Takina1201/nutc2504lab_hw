"""
LangChain é€²éšè¨è«– - å¯¦ä½œ(ch4-3)
ç§‘æŠ€æ–‡ç« æ‘˜è¦ç”Ÿæˆå™¨ + è·¯ç”±åˆ¤æ–·

åŠŸèƒ½ï¼š
- ç§‘æŠ€æ–‡ç«  â†’ ä½¿ç”¨ generate_tech_summary å·¥å…·ç”Ÿæˆæ‘˜è¦
- é–’èŠ/éç§‘æŠ€æ–‡ç«  â†’ ç›´æ¥å›è¦†

API: https://ws-02.wade0426.me/v1
Model: google/gemma-3-27b-it
"""

# ============================================================
# Import æ‰€æœ‰å¿…è¦çš„å‡½å¼åº«
# ============================================================
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser


# ============================================================
# 1. è¨­å®š LLM
# ============================================================

llm = ChatOpenAI(
    base_url="https://ws-02.wade0426.me/v1",
    api_key="",  # KEY ç•™ç©º
    model="google/gemma-3-27b-it",
    temperature=0
)


# ============================================================
# 2. å®šç¾©ç§‘æŠ€æ–‡ç« æ‘˜è¦å·¥å…·
# ============================================================

@tool
def generate_tech_summary(article_content: str):
    """
    ç§‘æŠ€æ–‡ç« å°ˆç”¨æ‘˜è¦ç”Ÿæˆå·¥å…·ã€‚
    ã€åˆ¤æ–·é‚è¼¯ã€‘ï¼š
    1. åªæœ‰ç•¶è¼¸å…¥å…§å®¹å±¬æ–¼ã€Œç§‘æŠ€ã€ã€ã€Œç¨‹å¼è¨­è¨ˆã€ã€ã€ŒAIã€ã€ã€Œè»Ÿé«”å·¥ç¨‹ã€æˆ–ã€ŒIT æŠ€è¡“ã€é ˜åŸŸæ™‚ï¼Œæ‰ä½¿ç”¨æ­¤å·¥å…·ã€‚
    2. å¦‚æœå…§å®¹æ˜¯ã€Œé–’èŠã€ã€ã€Œé£Ÿè­œã€ã€ã€Œå¤©æ°£ã€ã€ã€Œæ—¥å¸¸æ—¥è¨˜ã€ç­‰éæŠ€è¡“å…§å®¹ï¼Œè«‹å‹¿ä½¿ç”¨æ­¤å·¥å…·ã€‚
    
    åŠŸèƒ½ï¼šå°‡è¼¸å…¥çš„æŠ€è¡“æ–‡ç« æ­¸ç´å‡º 3 å€‹é‡é»ã€‚
    """
    # å®šç¾©æ‘˜è¦å°ˆç”¨çš„ Prompt
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€å€‹è³‡æ·±çš„ç§‘æŠ€ä¸»ç·¨ã€‚è«‹å°‡è¼¸å…¥çš„æŠ€è¡“æ–‡ç« å…§å®¹ï¼Œç²¾ç°¡åœ°æ­¸ç´å‡º 3 å€‹é—œéµé‡é» (Key Points)ï¼Œä¸¦ä»¥ç¹é«”ä¸­æ–‡æ¢åˆ—å¼è¼¸å‡ºã€‚"),
        ("user", "{text}")
    ])
    
    chain = prompt | llm | StrOutputParser()
    
    result = chain.invoke({"text": article_content})
    
    return result


# ============================================================
# 3. è¨»å†Šå·¥å…·ä¸¦å»ºç«‹è·¯ç”± Chain
# ============================================================

llm_with_tools = llm.bind_tools([generate_tech_summary])

router_prompt = ChatPromptTemplate.from_messages([
    ("user", "{input}")
])

chain = router_prompt | llm_with_tools


# ============================================================
# 4. äº’å‹•å¼å°è©±è¿´åœˆ
# ============================================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ”¬ ç§‘æŠ€æ–‡ç« æ‘˜è¦åŠ©æ‰‹ (ch4-3)")
    print("=" * 60)
    print("è¼¸å…¥ç§‘æŠ€æ–‡ç«  â†’ ç”Ÿæˆ 3 å€‹é‡é»æ‘˜è¦")
    print("è¼¸å…¥é–’èŠ â†’ ç›´æ¥å›è¦†")
    print("è¼¸å…¥ 'exit' æˆ– 'q' é›¢é–‹")
    print("=" * 60)
    
    while True:
        # å–å¾—ç”¨æˆ¶è¼¸å…¥
        user_input = input("\nUser: ")
        
        # æª¢æŸ¥æ˜¯å¦è¦é›¢é–‹
        if user_input.lower() in ["exit", "q"]:
            print("Bye!")
            break
        
        # å¦‚æœè¼¸å…¥ç‚ºç©ºï¼Œè·³é
        if not user_input.strip():
            continue
        
        # åŸ·è¡Œ chain
        ai_msg = chain.invoke({"input": user_input})
        
        # åˆ¤æ–·æ˜¯å¦æœ‰å·¥å…·å‘¼å«
        if ai_msg.tool_calls:
            # âœ… æœ‰å·¥å…·å‘¼å« â†’ åˆ¤æ–·ç‚ºç§‘æŠ€æ–‡ç« 
            print(f"âœ… [æ±ºç­–] åˆ¤æ–·ç‚ºç§‘æŠ€æ–‡ç« ")
            
            # å–å¾—å·¥å…·åƒæ•¸
            tool_args = ai_msg.tool_calls[0]['args']
            
            # åŸ·è¡Œå·¥å…·ï¼ˆç”Ÿæˆæ‘˜è¦ï¼‰
            final_result = generate_tech_summary.invoke(tool_args)
            
            print(f"ğŸ“„ [åŸ·è¡Œçµæœ]:\n{final_result}")
        
        else:
            # âŒ æ²’æœ‰å·¥å…·å‘¼å« â†’ åˆ¤æ–·ç‚ºé–’èŠ/éç§‘æŠ€æ–‡ç« ï¼Œç›´æ¥å›è¦†
            print(f"âŒ [æ±ºç­–] åˆ¤æ–·ç‚ºé–’èŠ/éç§‘æŠ€æ–‡ç« ï¼Œç›´æ¥å›è¦†ã€‚")
            print(f"ğŸ’¬ [AI å›æ‡‰]: {ai_msg.content}")