"""
èª²å ‚ä½œæ¥­-03ï¼šRAG + Query ReWrite
==================================
1. æŠŠ data_01~05.txt åˆ‡å¡Šå¾ŒåµŒå…¥åˆ° VDB
2. å¯¦ä½œ Query_ReWriteï¼ˆå¤šè¼ªå°è©± â†’ ç¨ç«‹æœå°‹èªå¥ï¼‰
3. ç”¨ Query_ReWrite çš„çµæœå» Retrieval
4. çµåˆ LLM å»å›ç­”
5. å®Œæˆ Re_Write_questions.csv å’Œ questions.csv
"""

import os
import re
import csv
import time
import json
import requests
import numpy as np
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct


# ============================================================
# è¨­å®š
# ============================================================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = SCRIPT_DIR  # data æª”æ¡ˆåœ¨åŒä¸€å±¤

# API
EMBED_API_URL = "https://ws-04.wade0426.me/embed"
LLM_API_URL = "https://ws-02.wade0426.me/v1/chat/completions"
LLM_MODEL = "google/gemma-3-27b-it"

# åˆ‡å¡Šåƒæ•¸
CHUNK_SIZE = 300
CHUNK_OVERLAP = 100

# Qdrant
COLLECTION_NAME = "cw03_chunks"


# ============================================================
# å·¥å…·å‡½æ•¸
# ============================================================
def get_embedding(texts: list[str]) -> tuple:
    """ä½¿ç”¨ Embedding API å–å¾—æ–‡æœ¬å‘é‡"""
    data = {
        "texts": texts,
        "task_description": "æª¢ç´¢æŠ€è¡“æ–‡ä»¶",
        "normalize": True,
    }
    try:
        resp = requests.post(EMBED_API_URL, json=data, timeout=60)
        if resp.status_code == 200:
            result = resp.json()
            return result["embeddings"], result["dimension"]
        print(f"  âŒ Embedding API éŒ¯èª¤: {resp.status_code} - {resp.text}")
        return None, None
    except Exception as e:
        print(f"  âŒ Embedding é€£ç·šå¤±æ•—: {e}")
        return None, None


def call_llm(system_prompt: str, user_prompt: str, temperature: float = 0.3) -> str:
    """å‘¼å« LLM API"""
    payload = {
        "model": LLM_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": temperature,
        "max_tokens": 1024,
    }
    try:
        resp = requests.post(LLM_API_URL, json=payload, timeout=120)
        if resp.status_code == 200:
            result = resp.json()
            return result["choices"][0]["message"]["content"].strip()
        print(f"  âŒ LLM API éŒ¯èª¤: {resp.status_code} - {resp.text}")
        return ""
    except Exception as e:
        print(f"  âŒ LLM é€£ç·šå¤±æ•—: {e}")
        return ""


def call_llm_multi_turn(messages: list[dict], temperature: float = 0.3) -> str:
    """å‘¼å« LLM APIï¼ˆå¤šè¼ªå°è©±ï¼‰"""
    payload = {
        "model": LLM_MODEL,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": 1024,
    }
    try:
        resp = requests.post(LLM_API_URL, json=payload, timeout=120)
        if resp.status_code == 200:
            return resp.json()["choices"][0]["message"]["content"].strip()
        print(f"  âŒ LLM API éŒ¯èª¤: {resp.status_code}")
        return ""
    except Exception as e:
        print(f"  âŒ LLM é€£ç·šå¤±æ•—: {e}")
        return ""


def read_csv(path: str) -> list[dict]:
    """è®€å– CSV"""
    with open(path, "r", encoding="utf-8-sig") as f:
        return list(csv.DictReader(f))


def write_csv(path: str, rows: list[dict], fieldnames: list[str]):
    """å¯«å…¥ CSV (utf-8-sig)"""
    with open(path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)
    print(f"  âœ… CSV å·²å„²å­˜ï¼š{path}ï¼ˆ{len(rows)} ç­†ï¼‰")


# ============================================================
# Step 1ï¼šè®€å–è³‡æ–™ â†’ åˆ‡å¡Š â†’ åµŒå…¥ VDB
# ============================================================
def load_and_chunk_data() -> list[dict]:
    """è®€å– data_01~05.txt ä¸¦é€²è¡Œæ»‘å‹•è¦–çª—åˆ‡å¡Š"""
    splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", ""],
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
    )

    all_chunks = []
    for i in range(1, 6):
        filename = f"data_0{i}.txt"
        filepath = os.path.join(DATA_DIR, filename)
        if not os.path.exists(filepath):
            print(f"  âš ï¸ {filename} ä¸å­˜åœ¨ï¼Œè·³é")
            continue

        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()

        chunks = splitter.split_text(content)
        for chunk in chunks:
            all_chunks.append({"text": chunk, "source": filename})
        print(f"  âœ… {filename}ï¼š{len(content)} å­—å…ƒ â†’ {len(chunks)} å¡Š")

    return all_chunks


def build_vdb(client: QdrantClient, chunks: list[dict], dim: int):
    """å»ºç«‹ Qdrant Collection ä¸¦åµŒå…¥æ‰€æœ‰åˆ‡å¡Š"""
    existing = [c.name for c in client.get_collections().collections]
    if COLLECTION_NAME in existing:
        client.delete_collection(COLLECTION_NAME)

    client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=VectorParams(size=dim, distance=Distance.COSINE),
    )

    all_points = []
    batch_size = 50
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i + batch_size]
        embs, _ = get_embedding([c["text"] for c in batch])
        if embs is None:
            continue
        for j, (chunk, emb) in enumerate(zip(batch, embs)):
            all_points.append(PointStruct(
                id=i + j, vector=emb,
                payload={"text": chunk["text"], "source": chunk["source"]},
            ))
        time.sleep(0.3)

    client.upsert(collection_name=COLLECTION_NAME, points=all_points)
    print(f"  âœ… {COLLECTION_NAME}ï¼š{len(all_points)} å€‹å‘é‡å·²åµŒå…¥")


def retrieve(client: QdrantClient, query: str, top_k: int = 3) -> list[dict]:
    """å¾ VDB æª¢ç´¢æœ€ç›¸é—œçš„åˆ‡å¡Š"""
    emb, _ = get_embedding([query])
    if emb is None:
        return []

    results = client.query_points(
        collection_name=COLLECTION_NAME, query=emb[0], limit=top_k
    )

    return [
        {"text": p.payload["text"], "source": p.payload["source"], "score": p.score}
        for p in results.points
    ]


# ============================================================
# Step 2ï¼šQuery ReWrite
# ============================================================
def load_rewrite_prompt() -> str:
    """è®€å– Prompt_ReWrite.txt"""
    path = os.path.join(DATA_DIR, "Prompt_ReWrite.txt")
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    # å‚™ç”¨ prompt
    return """ä½ æ˜¯ä¸€å€‹ RAG ç³»çµ±çš„æŸ¥è©¢é‡å¯«å°ˆå®¶ã€‚
å°‡ä½¿ç”¨è€…çš„ã€Œæœ€æ–°å•é¡Œã€çµåˆã€Œå°è©±æ­·å²ã€ï¼Œé‡å¯«æˆé©åˆå‘é‡è³‡æ–™åº«æœå°‹çš„ã€Œç¨ç«‹æœå°‹èªå¥ã€ã€‚
è¦å‰‡ï¼š1.æŒ‡ä»£æ¶ˆè§£ 2.è£œå…¨ä¸Šä¸‹æ–‡ 3.ä¿ç•™åŸæ„ 4.é—œéµå­—å¢å¼· 5.ç¹é«”ä¸­æ–‡
ç›´æ¥è¼¸å‡ºé‡å¯«å¾Œçš„èªå¥ï¼Œä¸è¦ä»»ä½•è§£é‡‹ã€‚"""


def query_rewrite(rewrite_prompt: str, conversation_history: list[dict],
                  current_question: str) -> str:
    """
    ä½¿ç”¨ LLM å°‡å¤šè¼ªå°è©±ä¸­çš„å•é¡Œé‡å¯«ç‚ºç¨ç«‹æœå°‹èªå¥
    conversation_history: [{"role": "user"/"assistant", "content": "..."}, ...]
    """
    # çµ„åˆå°è©±æ­·å²æˆæ–‡å­—
    history_text = ""
    if conversation_history:
        history_text = "ã€å°è©±æ­·å²ã€‘\n"
        for msg in conversation_history:
            role = "ä½¿ç”¨è€…" if msg["role"] == "user" else "åŠ©ç†"
            history_text += f"{role}ï¼š{msg['content']}\n"
        history_text += "\n"

    user_msg = f"""{history_text}ã€æœ€æ–°å•é¡Œã€‘
{current_question}

è«‹å°‡ä¸Šè¿°æœ€æ–°å•é¡Œé‡å¯«ç‚ºä¸€å€‹ç¨ç«‹çš„æœå°‹èªå¥ï¼š"""

    rewritten = call_llm(rewrite_prompt, user_msg, temperature=0.1)
    # æ¸…ç†å¯èƒ½çš„å¤šé¤˜æ–‡å­—
    rewritten = rewritten.strip().strip('"').strip("'")
    return rewritten


# ============================================================
# Step 3 & 4ï¼šRAG å›ç­”ï¼ˆæª¢ç´¢ + LLM ç”Ÿæˆï¼‰
# ============================================================
RAG_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å•ç­”åŠ©ç†ã€‚è«‹æ ¹æ“šä»¥ä¸‹ã€Œåƒè€ƒè³‡æ–™ã€å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
è¦å‰‡ï¼š
1. åªæ ¹æ“šåƒè€ƒè³‡æ–™ä¸­çš„å…§å®¹å›ç­”ï¼Œä¸è¦è‡ªè¡Œç·¨é€ 
2. å›ç­”è¦ç°¡æ½”ç²¾ç¢ºï¼Œç›´æ¥å›ç­”å•é¡Œé‡é»
3. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”"""


def rag_answer(client: QdrantClient, question: str, top_k: int = 3) -> tuple:
    """
    RAG æµç¨‹ï¼šæª¢ç´¢ â†’ çµ„åˆ context â†’ LLM å›ç­”
    å›å‚³: (answer, source, retrieved_texts)
    """
    results = retrieve(client, question, top_k)
    if not results:
        return "ç„¡æ³•æª¢ç´¢åˆ°ç›¸é—œè³‡æ–™", "", []

    # çµ„åˆ context
    context = ""
    sources = set()
    for i, r in enumerate(results):
        context += f"ã€æ®µè½ {i+1}ã€‘ï¼ˆä¾†æºï¼š{r['source']}ï¼Œç›¸ä¼¼åº¦ï¼š{r['score']:.4f}ï¼‰\n{r['text']}\n\n"
        sources.add(r["source"])

    # ä¸»è¦ä¾†æºï¼ˆåˆ†æ•¸æœ€é«˜çš„ï¼‰
    main_source = results[0]["source"]

    user_msg = f"""ã€åƒè€ƒè³‡æ–™ã€‘
{context}

ã€å•é¡Œã€‘
{question}

è«‹æ ¹æ“šåƒè€ƒè³‡æ–™å›ç­”ä¸Šè¿°å•é¡Œï¼š"""

    answer = call_llm(RAG_SYSTEM_PROMPT, user_msg, temperature=0.2)
    return answer, main_source, results


# ============================================================
# ä¸»ç¨‹å¼
# ============================================================
def main():
    print("=" * 60)
    print("èª²å ‚ä½œæ¥­-03ï¼šRAG + Query ReWrite")
    print("=" * 60)

    # â”€â”€â”€ Step 1ï¼šåˆ‡å¡Š & åµŒå…¥ VDB â”€â”€â”€
    print("\nğŸ“¦ Step 1ï¼šè®€å–è³‡æ–™ â†’ åˆ‡å¡Š â†’ åµŒå…¥ VDB")
    print("-" * 40)

    chunks = load_and_chunk_data()
    if not chunks:
        print("âŒ æ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆï¼")
        return

    print(f"\n  ç¸½è¨ˆ {len(chunks)} å€‹åˆ‡å¡Š")

    # å–å¾—ç¶­åº¦
    _, dim = get_embedding(["æ¸¬è©¦"])
    if dim is None:
        print("âŒ Embedding API ä¸å¯ç”¨")
        return
    print(f"  å‘é‡ç¶­åº¦ï¼š{dim}")

    # å»ºç«‹ VDB
    client = QdrantClient(url="http://localhost:6333")
    build_vdb(client, chunks, dim)

    # â”€â”€â”€ Step 2 & 3ï¼šè™•ç† questions.csvï¼ˆç›´æ¥æª¢ç´¢ï¼‰ â”€â”€â”€
    print("\nğŸ“‹ Step 2ï¼šè™•ç† questions.csvï¼ˆç›´æ¥ RAGï¼‰")
    print("-" * 40)

    q_path = os.path.join(DATA_DIR, "questions.csv")
    questions = read_csv(q_path)

    q_results = []
    for q in questions:
        q_id = q["é¡Œç›®_ID"]
        q_text = q["é¡Œç›®"]
        print(f"\n  Q{q_id}: {q_text[:45]}...")

        answer, source, hits = rag_answer(client, q_text)
        print(f"      ç­”æ¡ˆ: {answer[:60]}...")
        print(f"      ä¾†æº: {source}")

        q_results.append({
            "é¡Œç›®_ID": q_id,
            "é¡Œç›®": q_text,
            "æ¨™æº–ç­”æ¡ˆ": answer,
            "ä¾†æºæ–‡ä»¶": source,
        })
        time.sleep(0.5)

    # è¼¸å‡º questions_answer.csv
    q_out_path = os.path.join(DATA_DIR, "questions_answer.csv")
    write_csv(q_out_path, q_results, ["é¡Œç›®_ID", "é¡Œç›®", "æ¨™æº–ç­”æ¡ˆ", "ä¾†æºæ–‡ä»¶"])

    # â”€â”€â”€ Step 4 & 5ï¼šè™•ç† Re_Write_questions.csvï¼ˆQuery ReWrite + RAGï¼‰ â”€â”€â”€
    print("\nğŸ”„ Step 3ï¼šè™•ç† Re_Write_questions.csvï¼ˆQuery ReWrite + RAGï¼‰")
    print("-" * 40)

    rewrite_prompt = load_rewrite_prompt()
    print("  âœ… å·²è¼‰å…¥ Prompt_ReWrite.txt")

    rw_path = os.path.join(DATA_DIR, "Re_Write_questions.csv")
    rw_questions = read_csv(rw_path)

    # æŒ‰ conversation_id åˆ†çµ„
    conversations = {}
    for q in rw_questions:
        conv_id = q["conversation_id"]
        if conv_id not in conversations:
            conversations[conv_id] = []
        conversations[conv_id].append(q)

    rw_results = []

    for conv_id, conv_questions in conversations.items():
        print(f"\n  === å°è©± {conv_id} ===")
        conversation_history = []  # ç´¯ç©å°è©±æ­·å²

        for q in conv_questions:
            q_id = q["questions_id"]
            q_text = q["questions"]
            print(f"\n    Q{conv_id}-{q_id}: {q_text}")

            # Step Aï¼šQuery ReWrite
            if len(conversation_history) > 0:
                # æœ‰å°è©±æ­·å² â†’ éœ€è¦é‡å¯«
                rewritten = query_rewrite(rewrite_prompt, conversation_history, q_text)
                print(f"    âœï¸  é‡å¯«å¾Œ: {rewritten}")
            else:
                # ç¬¬ä¸€å€‹å•é¡Œ â†’ ä¸éœ€è¦é‡å¯«
                rewritten = q_text
                print(f"    âœï¸  é¦–é¡Œï¼Œç„¡éœ€é‡å¯«")

            # Step Bï¼šç”¨é‡å¯«å¾Œçš„ query æª¢ç´¢
            answer, source, hits = rag_answer(client, rewritten)
            print(f"    ğŸ’¬ ç­”æ¡ˆ: {answer[:60]}...")
            print(f"    ğŸ“‚ ä¾†æº: {source}")

            rw_results.append({
                "conversation_id": conv_id,
                "questions_id": q_id,
                "questions": q_text,
                "answer": answer,
                "source": source,
            })

            # ç´¯ç©å°è©±æ­·å²
            conversation_history.append({"role": "user", "content": q_text})
            conversation_history.append({"role": "assistant", "content": answer})

            time.sleep(0.5)

    # è¼¸å‡º Re_Write_answer.csv
    rw_out_path = os.path.join(DATA_DIR, "Re_Write_answer.csv")
    write_csv(rw_out_path, rw_results,
              ["conversation_id", "questions_id", "questions", "answer", "source"])

    # â”€â”€â”€ å®Œæˆ â”€â”€â”€
    print(f"""
{'=' * 60}
âœ… èª²å ‚ä½œæ¥­-03 å®Œæˆï¼
{'=' * 60}

ğŸ“Š åŸ·è¡Œæ‘˜è¦ï¼š
  1. âœ… è³‡æ–™åˆ‡å¡Šï¼š{len(chunks)} å¡Š â†’ Qdrant VDB
  2. âœ… questions.csvï¼š{len(q_results)} é¡Œå·²å›ç­” â†’ questions_answer.csv
  3. âœ… Re_Write_questions.csvï¼š{len(rw_results)} é¡Œå·²é‡å¯«+å›ç­” â†’ Re_Write_answer.csv

ğŸ“ è¼¸å‡ºæª”æ¡ˆï¼š
  - {q_out_path}
  - {rw_out_path}

ğŸ“Œ ä¸Šå‚³åˆ° GitHub (CW/03/)
""")


if __name__ == "__main__":
    main()