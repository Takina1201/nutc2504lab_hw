"""
èª²å ‚ä½œæ¥­-02ï¼šæ–‡æœ¬åˆ‡å¡Šèˆ‡å‘é‡è³‡æ–™åº«æª¢ç´¢æ¯”è¼ƒ
============================================
1. ä¸‹è¼‰ç¯„ä¾‹æª”æ¡ˆï¼ˆtext.txtï¼‰
2. å¯¦ä½œå›ºå®šåˆ‡å¡Š
3. å¯¦ä½œæ»‘å‹•è¦–çª—åˆ‡å¡Š
4. åµŒå…¥åˆ° VDB (Qdrant)
5. è©¦è‘—å¬å›ä¸¦æ¯”è¼ƒå…©ç¨®åˆ‡å¡Šæ–¹æ³•
6. è©¦è‘—è™•ç†è¡¨æ ¼ï¼ˆtable è³‡æ–™å¤¾ï¼‰
7. ä¸Šå‚³åˆ° GitHub (CW/02)

åµŒå…¥æ¨¡å‹ï¼šä½¿ç”¨ TF-IDF + TruncatedSVDï¼ˆLSAï¼‰ç”¢ç”Ÿå¯†é›†å‘é‡
å‘é‡è³‡æ–™åº«ï¼šQdrantï¼ˆè¨˜æ†¶é«”æ¨¡å¼ï¼‰
"""

import os
import re
import numpy as np
from langchain_text_splitters import (
    CharacterTextSplitter,
    RecursiveCharacterTextSplitter,
)
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import normalize


# ============================================================
# åµŒå…¥æ¨¡å‹ï¼šTF-IDF + SVD (LSA) â€” ä¸éœ€ä¸‹è¼‰æ¨¡å‹
# ============================================================
class LocalEmbedding:
    """
    ä½¿ç”¨ TF-IDF + TruncatedSVDï¼ˆLatent Semantic Analysisï¼‰
    ç”¢ç”Ÿå¯†é›†å‘é‡åµŒå…¥ï¼Œç„¡éœ€ä¸‹è¼‰é è¨“ç·´æ¨¡å‹ã€‚
    
    åŸç†ï¼š
    1. TF-IDF å°‡æ–‡æœ¬è½‰ç‚ºç¨€ç–å‘é‡ï¼ˆè©é »-é€†æ–‡ä»¶é »ç‡ï¼‰
    2. TruncatedSVD é™ç¶­åˆ°å›ºå®šç¶­åº¦ï¼ˆé¡ä¼¼ LSAï¼‰
    3. L2 æ­£è¦åŒ–ï¼Œä½¿å‘é‡é•·åº¦ç‚º 1ï¼ˆæ–¹ä¾¿è¨ˆç®— cosine similarityï¼‰
    """
    def __init__(self, dim: int = 128):
        self.dim = dim
        self.vectorizer = TfidfVectorizer(
            analyzer="char_wb",  # å­—å…ƒç´šåˆ†æï¼Œé©åˆä¸­æ–‡
            ngram_range=(1, 3),  # 1~3 å­—å…ƒçš„ n-gram
            max_features=5000,   # æœ€å¤š 5000 å€‹ç‰¹å¾µ
        )
        self.svd = TruncatedSVD(n_components=dim, random_state=42)
        self.is_fitted = False

    def fit(self, texts: list[str]):
        """ç”¨æ‰€æœ‰æ–‡æœ¬è¨“ç·´åµŒå…¥æ¨¡å‹"""
        tfidf_matrix = self.vectorizer.fit_transform(texts)
        # SVD ç¶­åº¦ä¸èƒ½è¶…é min(æ¨£æœ¬æ•¸, ç‰¹å¾µæ•¸) - 1
        max_dim = min(tfidf_matrix.shape[0], tfidf_matrix.shape[1]) - 1
        actual_dim = min(self.dim, max_dim)
        if actual_dim != self.dim:
            self.svd = TruncatedSVD(n_components=actual_dim, random_state=42)
            self.dim = actual_dim
        self.svd.fit(tfidf_matrix)
        self.is_fitted = True
        print(f"âœ… åµŒå…¥æ¨¡å‹è¨“ç·´å®Œæˆï¼šç¶­åº¦={self.dim}")

    def embed(self, texts: list[str]) -> np.ndarray:
        """å°‡æ–‡æœ¬è½‰ç‚ºå¯†é›†å‘é‡"""
        if not self.is_fitted:
            raise ValueError("è«‹å…ˆå‘¼å« fit() è¨“ç·´æ¨¡å‹")
        tfidf_matrix = self.vectorizer.transform(texts)
        dense_vectors = self.svd.transform(tfidf_matrix)
        dense_vectors = normalize(dense_vectors, norm="l2")
        return dense_vectors


# ============================================================
# æ­¥é©Ÿ 1ï¼šè®€å–ç¯„ä¾‹æª”æ¡ˆ text.txt
# ============================================================
print("=" * 60)
print("æ­¥é©Ÿ 1ï¼šè®€å–ç¯„ä¾‹æª”æ¡ˆ")
print("=" * 60)

# å–å¾—è…³æœ¬æ‰€åœ¨ç›®éŒ„ï¼Œç¢ºä¿è·¯å¾‘æ­£ç¢º
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
TEXT_PATH = os.path.join(SCRIPT_DIR, "text.txt")

with open(TEXT_PATH, "r", encoding="utf-8") as f:
    raw_text = f.read()

print(f"åŸå§‹æ–‡æœ¬é•·åº¦ï¼š{len(raw_text)} å­—å…ƒ")
print(f"å‰ 200 å­—ï¼š{raw_text[:200]}...")
print()


# ============================================================
# æ­¥é©Ÿ 2ï¼šå¯¦ä½œå›ºå®šåˆ‡å¡Š (Fixed-size Chunking)
# ============================================================
print("=" * 60)
print("æ­¥é©Ÿ 2ï¼šå›ºå®šåˆ‡å¡Š (Fixed-size Chunking)")
print("=" * 60)

fixed_splitter = CharacterTextSplitter(
    separator="",          # ä¸ä½¿ç”¨ç‰¹å®šåˆ†éš”ç¬¦ï¼Œç´”ç²¹æŒ‰å­—å…ƒæ•¸åˆ‡å‰²
    chunk_size=200,        # æ¯å€‹ chunk 200 å­—å…ƒ
    chunk_overlap=0,       # å›ºå®šåˆ‡å¡Šä¸é‡ç–Š
    length_function=len,
)

fixed_chunks = fixed_splitter.split_text(raw_text)

print(f"å›ºå®šåˆ‡å¡Šæ•¸é‡ï¼š{len(fixed_chunks)} å€‹\n")
for i, chunk in enumerate(fixed_chunks[:5]):
    print(f"--- å›ºå®šåˆ‡å¡Š [{i+1}] (é•·åº¦: {len(chunk)}) ---")
    print(chunk[:100] + "..." if len(chunk) > 100 else chunk)
    print()
print(f"...ï¼ˆå…± {len(fixed_chunks)} å€‹åˆ‡å¡Šï¼‰\n")


# ============================================================
# æ­¥é©Ÿ 3ï¼šå¯¦ä½œæ»‘å‹•è¦–çª—åˆ‡å¡Š (Sliding Window Chunking)
# ============================================================
print("=" * 60)
print("æ­¥é©Ÿ 3ï¼šæ»‘å‹•è¦–çª—åˆ‡å¡Š (Sliding Window Chunking)")
print("=" * 60)

sliding_splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", ""],
    chunk_size=200,
    chunk_overlap=50,
    length_function=len,
)

sliding_chunks = sliding_splitter.split_text(raw_text)

print(f"æ»‘å‹•è¦–çª—åˆ‡å¡Šæ•¸é‡ï¼š{len(sliding_chunks)} å€‹\n")
for i, chunk in enumerate(sliding_chunks[:5]):
    print(f"--- æ»‘å‹•è¦–çª—åˆ‡å¡Š [{i+1}] (é•·åº¦: {len(chunk)}) ---")
    print(chunk[:100] + "..." if len(chunk) > 100 else chunk)
    print()
print(f"...ï¼ˆå…± {len(sliding_chunks)} å€‹åˆ‡å¡Šï¼‰\n")


# ============================================================
# æ­¥é©Ÿ 4ï¼šåµŒå…¥åˆ° VDB (Qdrant)
# ============================================================
print("=" * 60)
print("æ­¥é©Ÿ 4ï¼šåµŒå…¥åˆ° Qdrant å‘é‡è³‡æ–™åº«")
print("=" * 60)

embedding_model = LocalEmbedding(dim=128)
all_texts = fixed_chunks + sliding_chunks
embedding_model.fit(all_texts)

qdrant_client = QdrantClient(":memory:")
embedding_dim = embedding_model.dim

# å›ºå®šåˆ‡å¡Š Collection
qdrant_client.create_collection(
    collection_name="fixed_chunks",
    vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE),
)
fixed_embeddings = embedding_model.embed(fixed_chunks)
fixed_points = [
    PointStruct(id=i, vector=emb.tolist(),
                payload={"text": chunk, "chunk_id": i, "method": "fixed"})
    for i, (chunk, emb) in enumerate(zip(fixed_chunks, fixed_embeddings))
]
qdrant_client.upsert(collection_name="fixed_chunks", points=fixed_points)
print(f"âœ… å›ºå®šåˆ‡å¡Šå·²åµŒå…¥ Qdrantï¼š{len(fixed_points)} å€‹å‘é‡")

# æ»‘å‹•è¦–çª—åˆ‡å¡Š Collection
qdrant_client.create_collection(
    collection_name="sliding_chunks",
    vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE),
)
sliding_embeddings = embedding_model.embed(sliding_chunks)
sliding_points = [
    PointStruct(id=i, vector=emb.tolist(),
                payload={"text": chunk, "chunk_id": i, "method": "sliding"})
    for i, (chunk, emb) in enumerate(zip(sliding_chunks, sliding_embeddings))
]
qdrant_client.upsert(collection_name="sliding_chunks", points=sliding_points)
print(f"âœ… æ»‘å‹•è¦–çª—åˆ‡å¡Šå·²åµŒå…¥ Qdrantï¼š{len(sliding_points)} å€‹å‘é‡\n")


# ============================================================
# æ­¥é©Ÿ 5ï¼šè©¦è‘—å¬å›ä¸¦æ¯”è¼ƒå…©ç¨®åˆ‡å¡Šæ–¹æ³•
# ============================================================
print("=" * 60)
print("æ­¥é©Ÿ 5ï¼šå¬å›æ¯”è¼ƒå…©ç¨®åˆ‡å¡Šæ–¹æ³•")
print("=" * 60)

test_queries = [
    "Graph RAG æœ‰å“ªäº›æª¢ç´¢ç­–ç•¥ï¼Ÿ",
    "å¾®è»Ÿ GraphRAG çš„æ ¸å¿ƒç‰¹é»æ˜¯ä»€éº¼ï¼Ÿ",
    "çŸ¥è­˜åœ–è­œå¦‚ä½•è§£æ±ºå¹»è¦ºå•é¡Œï¼Ÿ",
]

for query in test_queries:
    print(f"\nğŸ” æŸ¥è©¢ï¼š{query}")
    print("-" * 50)

    query_vector = embedding_model.embed([query])[0].tolist()

    fixed_results = qdrant_client.query_points(
        collection_name="fixed_chunks", query=query_vector, limit=3).points

    print("\nğŸ“¦ ã€å›ºå®šåˆ‡å¡Šã€‘Top-3 çµæœï¼š")
    for rank, result in enumerate(fixed_results, 1):
        text_preview = result.payload["text"][:80].replace("\n", " ")
        print(f"  [{rank}] åˆ†æ•¸: {result.score:.4f} | {text_preview}...")

    sliding_results = qdrant_client.query_points(
        collection_name="sliding_chunks", query=query_vector, limit=3).points

    print("\nğŸªŸ ã€æ»‘å‹•è¦–çª—åˆ‡å¡Šã€‘Top-3 çµæœï¼š")
    for rank, result in enumerate(sliding_results, 1):
        text_preview = result.payload["text"][:80].replace("\n", " ")
        print(f"  [{rank}] åˆ†æ•¸: {result.score:.4f} | {text_preview}...")

    fixed_best = fixed_results[0].score if fixed_results else 0
    sliding_best = sliding_results[0].score if sliding_results else 0
    winner = "æ»‘å‹•è¦–çª—" if sliding_best > fixed_best else "å›ºå®šåˆ‡å¡Š"
    print(f"\n  â­ æœ€ä½³åŒ¹é…ï¼š{winner}ï¼ˆå›ºå®š: {fixed_best:.4f} vs æ»‘å‹•: {sliding_best:.4f}ï¼‰")


# æ¯”è¼ƒåˆ†æç¸½çµ
print("\n" + "=" * 60)
print("æ¯”è¼ƒåˆ†æç¸½çµ")
print("=" * 60)
print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     æ¯”è¼ƒé …ç›®       â”‚    å›ºå®šåˆ‡å¡Š       â”‚    æ»‘å‹•è¦–çª—åˆ‡å¡Š       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   åˆ‡å¡Šæ•¸é‡         â”‚    {len(fixed_chunks):>4} å€‹        â”‚     {len(sliding_chunks):>4} å€‹           â”‚
â”‚   chunk_size      â”‚    200 å­—å…ƒ       â”‚     200 å­—å…ƒ          â”‚
â”‚   chunk_overlap   â”‚    0 å­—å…ƒ         â”‚     50 å­—å…ƒ           â”‚
â”‚   åˆ†éš”ç¬¦ç­–ç•¥       â”‚    ç„¡ï¼ˆç´”å­—æ•¸ï¼‰    â”‚     èªæ„é‚Šç•Œ          â”‚
â”‚   èªæ„å®Œæ•´æ€§       â”‚    å¯èƒ½è¢«æˆªæ–·      â”‚     ç›¡é‡ä¿æŒå®Œæ•´       â”‚
â”‚   è³‡è¨Šé‡ç–Š         â”‚    ç„¡             â”‚     æœ‰é‡ç–Šå€åŸŸ         â”‚
â”‚   é©ç”¨å ´æ™¯         â”‚    å¿«é€Ÿåˆ†å‰²        â”‚     é«˜å“è³ª RAG        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é‡é»å·®ç•°ï¼š
1. å›ºå®šåˆ‡å¡Šå¯èƒ½åœ¨å¥å­ä¸­é–“æˆªæ–·ï¼Œå°è‡´èªæ„ä¸å®Œæ•´
2. æ»‘å‹•è¦–çª—é€é overlap ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œæ¸›å°‘è³‡è¨Šéºå¤±
3. æ»‘å‹•è¦–çª—ä½¿ç”¨èªæ„é‚Šç•Œï¼ˆå¥è™Ÿã€å•è™Ÿç­‰ï¼‰åˆ‡å‰²ï¼Œèªæ„æ›´å®Œæ•´
4. æ»‘å‹•è¦–çª—çš„åˆ‡å¡Šæ•¸é‡é€šå¸¸è¼ƒå¤šï¼ˆå› ç‚ºæœ‰é‡ç–Šï¼‰
""")


# ============================================================
# æ­¥é©Ÿ 6ï¼šè©¦è‘—è™•ç†è¡¨æ ¼ï¼ˆtable è³‡æ–™å¤¾ï¼‰
# ============================================================
print("=" * 60)
print("æ­¥é©Ÿ 6ï¼šè™•ç†è¡¨æ ¼è³‡æ–™ï¼ˆtable è³‡æ–™å¤¾ï¼‰")
print("=" * 60)

TABLE_DIR = os.path.join(SCRIPT_DIR, "table")

# â”€â”€â”€ æ–¹æ³• 1ï¼šMarkdown è¡¨æ ¼ï¼ˆtable_txt.mdï¼‰â”€â”€â”€
print("\nğŸ“„ æ–¹æ³• 1ï¼šMarkdown è¡¨æ ¼è™•ç†ï¼ˆtable_txt.mdï¼‰")
print("-" * 40)

md_path = os.path.join(TABLE_DIR, "table_txt.md")
with open(md_path, "r", encoding="utf-8") as f:
    table_md_content = f.read().strip()

print("åŸå§‹ Markdown è¡¨æ ¼ï¼š")
print(table_md_content[:300] + "..." if len(table_md_content) > 300 else table_md_content)

# è§£æ Markdown è¡¨æ ¼ï¼šé€åˆ—åˆ‡å¡Šï¼Œæ¯åˆ—ä¿ç•™è¡¨é ­
table_lines = table_md_content.strip().split("\n")
header = table_lines[0]
separator_line = table_lines[1] if len(table_lines) > 1 and set(table_lines[1].replace("|","").replace("-","").strip()) <= {""} else ""
data_start = 2 if separator_line else 1
data_rows = table_lines[data_start:]

table_chunks_md = []
for row in data_rows:
    if row.strip():
        chunk = f"{header}\n{separator_line}\n{row}" if separator_line else f"{header}\n{row}"
        table_chunks_md.append(chunk)

print(f"\nâœ… Markdown è¡¨æ ¼åˆ‡å¡Šæ•¸é‡ï¼š{len(table_chunks_md)} å€‹")
for i, row in enumerate(data_rows):
    cols = [c.strip() for c in row.split("|") if c.strip()]
    if cols:
        print(f"  åˆ‡å¡Š [{i+1}] é …ç›®: {cols[0]}")


# â”€â”€â”€ æ–¹æ³• 2ï¼šHTML è¡¨æ ¼ï¼ˆtable_html.htmlï¼‰â”€â”€â”€
print("\n\nğŸ“„ æ–¹æ³• 2ï¼šHTML è¡¨æ ¼è™•ç†ï¼ˆtable_html.htmlï¼‰")
print("-" * 40)

html_path = os.path.join(TABLE_DIR, "table_html.html")
with open(html_path, "r", encoding="utf-8") as f:
    html_content = f.read()

print(f"HTML æª”æ¡ˆå¤§å°ï¼š{len(html_content)} å­—å…ƒ")

# å¾ HTML ä¸­æå– <table> å€å¡Š
table_match = re.search(r"<table.*?>(.*?)</table>", html_content, re.DOTALL)
if not table_match:
    print("âš ï¸ æœªæ‰¾åˆ° <table> æ¨™ç±¤")
    table_chunks_html = []
else:
    table_html_content = table_match.group(0)

    # æå–æ‰€æœ‰ <tr> åˆ—
    rows_html = re.findall(r"<tr.*?>(.*?)</tr>", table_html_content, re.DOTALL)

    # æ‰¾åˆ°è¡¨é ­ï¼ˆ<th> æ¨™ç±¤ï¼‰
    html_headers = []
    header_row_idx = 0
    for idx, row in enumerate(rows_html):
        ths = re.findall(r"<th.*?>(.*?)</th>", row, re.DOTALL)
        if ths:
            # æ¸…é™¤ HTML æ¨™ç±¤
            html_headers = [re.sub(r"<.*?>", "", h).strip() for h in ths]
            header_row_idx = idx
            break

    print(f"è¡¨é ­æ¬„ä½ï¼š{html_headers}")

    # æå–æ¯ä¸€åˆ—è³‡æ–™ï¼Œè½‰æ›ç‚ºè‡ªç„¶èªè¨€æè¿°
    table_chunks_html = []
    for row_html in rows_html[header_row_idx + 1:]:
        # æå– <td> ä¸­çš„å…§å®¹ï¼Œæ¸…é™¤å…§éƒ¨ HTML æ¨™ç±¤ï¼ˆå¦‚ <strong>ã€<br>ï¼‰
        cells_raw = re.findall(r"<td.*?>(.*?)</td>", row_html, re.DOTALL)
        cells = [re.sub(r"<.*?>", "", c).strip() for c in cells_raw]

        if cells and len(cells) == len(html_headers):
            # æ–¹å¼ Aï¼šçµæ§‹åŒ–æ ¼å¼ï¼ˆè¡¨é ­ï¼šå…§å®¹ï¼‰
            parts = [f"{h}ï¼š{c}" for h, c in zip(html_headers, cells)]
            chunk = "ï¼›".join(parts)
            table_chunks_html.append(chunk)

    print(f"âœ… HTML è¡¨æ ¼åˆ‡å¡Šæ•¸é‡ï¼š{len(table_chunks_html)} å€‹")
    for i, chunk in enumerate(table_chunks_html):
        preview = chunk[:100] + "..." if len(chunk) > 100 else chunk
        print(f"  åˆ‡å¡Š [{i+1}]ï¼š{preview}")


# â”€â”€â”€ è¡¨æ ¼åˆ‡å¡ŠåµŒå…¥ VDB â”€â”€â”€
print("\n\nğŸ“¦ å°‡è¡¨æ ¼åˆ‡å¡ŠåµŒå…¥ Qdrant")

all_table_chunks = table_chunks_md + table_chunks_html
embedding_model_table = LocalEmbedding(dim=64)
embedding_model_table.fit(all_texts + all_table_chunks)

qdrant_client.create_collection(
    collection_name="table_chunks",
    vectors_config=VectorParams(size=embedding_model_table.dim, distance=Distance.COSINE),
)

table_embeddings = embedding_model_table.embed(all_table_chunks)
table_points = [
    PointStruct(id=i, vector=emb.tolist(),
                payload={"text": chunk, "chunk_id": i,
                         "source": "markdown" if i < len(table_chunks_md) else "html"})
    for i, (chunk, emb) in enumerate(zip(all_table_chunks, table_embeddings))
]
qdrant_client.upsert(collection_name="table_chunks", points=table_points)
print(f"âœ… è¡¨æ ¼åˆ‡å¡Šå·²åµŒå…¥ Qdrantï¼š{len(table_points)} å€‹å‘é‡")

# è¡¨æ ¼æŸ¥è©¢æ¸¬è©¦
table_queries = [
    "ä¸‰æ°‘æ ¡å€çš„é‡é»ç™¼å±•è¨ˆç•«æ˜¯ä»€éº¼ï¼Ÿ",
    "å“ªå€‹æ ¡å€è·Ÿèˆªå¤ªæœ‰é—œï¼Ÿ",
]

for query in table_queries:
    print(f"\nğŸ” è¡¨æ ¼æŸ¥è©¢ï¼š{query}")
    query_vector = embedding_model_table.embed([query])[0].tolist()
    table_results = qdrant_client.query_points(
        collection_name="table_chunks", query=query_vector, limit=3).points

    print("ğŸ“Š Top-3 çµæœï¼š")
    for rank, result in enumerate(table_results, 1):
        src = result.payload["source"]
        text = result.payload["text"][:100]
        print(f"  [{rank}] åˆ†æ•¸: {result.score:.4f} | ä¾†æº: {src} | {text}...")

print("""
ğŸ“ è¡¨æ ¼è™•ç†æ–¹æ³•ç¸½çµï¼š
  1. Markdown è¡¨æ ¼ â†’ é€åˆ—åˆ‡å¡Šï¼ˆæ¯åˆ—ä¿ç•™è¡¨é ­ï¼‰
  2. HTML è¡¨æ ¼ â†’ è§£æå¾Œè½‰è‡ªç„¶èªè¨€æè¿°
  å…©ç¨®æ–¹å¼éƒ½ç¢ºä¿æ¯å€‹åˆ‡å¡ŠåŒ…å«å®Œæ•´çš„ã€Œé …ç›®-å€¼ã€å°æ‡‰é—œä¿‚ã€‚
""")


# ============================================================
# å®Œæˆï¼
# ============================================================
print("=" * 60)
print("âœ… èª²å ‚ä½œæ¥­-02 å…¨éƒ¨å®Œæˆï¼")
print("=" * 60)
print(f"""
å®Œæˆé …ç›®ï¼š
  1. âœ… è®€å–ç¯„ä¾‹æª”æ¡ˆ text.txtï¼ˆ{len(raw_text)} å­—å…ƒï¼‰
  2. âœ… å¯¦ä½œå›ºå®šåˆ‡å¡Šï¼ˆCharacterTextSplitter, size=200, overlap=0 â†’ {len(fixed_chunks)} å¡Šï¼‰
  3. âœ… å¯¦ä½œæ»‘å‹•è¦–çª—åˆ‡å¡Šï¼ˆRecursiveCharacterTextSplitter, size=200, overlap=50 â†’ {len(sliding_chunks)} å¡Šï¼‰
  4. âœ… åµŒå…¥åˆ° Qdrant VDBï¼ˆTF-IDF + SVD åµŒå…¥ï¼Œdim={embedding_dim}ï¼‰
  5. âœ… å¬å›æ¯”è¼ƒå…©ç¨®åˆ‡å¡Šæ–¹æ³•ï¼ˆ3 å€‹æ¸¬è©¦æŸ¥è©¢ï¼‰
  6. âœ… è™•ç†è¡¨æ ¼ï¼ˆMarkdown {len(table_chunks_md)} å¡Š + HTML {len(table_chunks_html)} å¡Šï¼‰
""")