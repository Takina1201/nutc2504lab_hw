"""
èª²å ‚ä½œæ¥­-04ï¼šRAG + Hybrid Search + ReRank + Query ReWrite
==========================================================
æµç¨‹ï¼š
  1. data_01~05.txt â†’ æ»‘å‹•è¦–çª—åˆ‡å¡Š â†’ Embedding â†’ Qdrant VDB
  2. å»ºç«‹ BM25 ç´¢å¼•ï¼ˆé—œéµå­—æœå°‹ï¼‰
  3. Hybrid Search = Dense(å‘é‡) + Sparse(BM25) â†’ RRF èžåˆ
  4. ReRankï¼šLLM åˆ¤æ–·ç›¸é—œæ€§é‡æ–°æŽ’åº
  5. LLM å¾ž Top-3 èƒå–ç²¾æº–ç­”æ¡ˆ
  6. Re_Writeï¼šå¤šè¼ªå°è©± Query Rewrite â†’ åŒä¸Šæµç¨‹
  7. è¼¸å‡º questions_answer.csvã€Re_Write_answer.csv
"""

import os
import re
import csv
import json
import time
import math
import requests
import numpy as np
import jieba
from collections import Counter
from rank_bm25 import BM25Okapi
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct


# ============================================================
# è¨­å®š
# ============================================================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# API
EMBED_API_URL = "https://ws-04.wade0426.me/embed"
LLM_API_URL = "https://ws-02.wade0426.me/v1/chat/completions"
LLM_MODEL = "google/gemma-3-27b-it"

# åˆ‡å¡Šåƒæ•¸
CHUNK_SIZE = 300
CHUNK_OVERLAP = 100

# æª¢ç´¢åƒæ•¸
DENSE_TOP_K = 10       # å‘é‡æœå°‹å–å‰ 10
BM25_TOP_K = 10        # BM25 æœå°‹å–å‰ 10
HYBRID_TOP_K = 10      # RRF èžåˆå¾Œå–å‰ 10
RERANK_TOP_K = 3       # ReRank å¾Œå–å‰ 3
RRF_K = 60             # RRF å¸¸æ•¸

# Collection åç¨±
COLLECTION_NAME = "cw04_chunks"


# ============================================================
# å·¥å…·å‡½æ•¸
# ============================================================
def get_embedding(texts: list[str]) -> tuple:
    """Embedding API"""
    data = {"texts": texts, "normalize": True, "batch_size": 32}
    try:
        resp = requests.post(EMBED_API_URL, json=data, timeout=60)
        if resp.status_code == 200:
            result = resp.json()
            return result["embeddings"], result["dimension"]
        print(f"  âŒ Embedding API éŒ¯èª¤: {resp.status_code}")
        return None, None
    except Exception as e:
        print(f"  âŒ Embedding API é€£ç·šå¤±æ•—: {e}")
        return None, None


def call_llm(system_prompt: str, user_prompt: str,
             temperature: float = 0.1, max_tokens: int = 512) -> str:
    """LLM API"""
    payload = {
        "model": LLM_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    try:
        resp = requests.post(LLM_API_URL, json=payload, timeout=120)
        if resp.status_code == 200:
            return resp.json()["choices"][0]["message"]["content"].strip()
        print(f"  âŒ LLM API éŒ¯èª¤: {resp.status_code} - {resp.text[:200]}")
        return ""
    except Exception as e:
        print(f"  âŒ LLM é€£ç·šå¤±æ•—: {e}")
        return ""


def call_llm_messages(messages: list[dict],
                      temperature: float = 0.1, max_tokens: int = 256) -> str:
    """LLM APIï¼ˆè‡ªè¨‚ messagesï¼‰"""
    payload = {
        "model": LLM_MODEL,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    try:
        resp = requests.post(LLM_API_URL, json=payload, timeout=120)
        if resp.status_code == 200:
            return resp.json()["choices"][0]["message"]["content"].strip()
        print(f"  âŒ LLM API éŒ¯èª¤: {resp.status_code}")
        return ""
    except Exception as e:
        print(f"  âŒ LLM é€£ç·šå¤±æ•—: {e}")
        return ""


# ============================================================
# æ–‡æœ¬åˆ‡å¡Š
# ============================================================
def chunk_texts(data_files: dict) -> list[dict]:
    """æ»‘å‹•è¦–çª—åˆ‡å¡Š"""
    splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", ""],
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
    )
    chunks = []
    for filename, content in data_files.items():
        texts = splitter.split_text(content)
        for t in texts:
            chunks.append({"text": t, "source": filename})
        print(f"  âœ… {filename}ï¼š{len(texts)} å¡Š")
    return chunks


# ============================================================
# BM25 ç´¢å¼•
# ============================================================
def tokenize_chinese(text: str) -> list[str]:
    """ä¸­æ–‡åˆ†è©žï¼ˆjiebaï¼‰"""
    words = jieba.lcut(text)
    # éŽæ¿¾åœç”¨è©žå’Œå–®å­—å…ƒ
    stop_words = {"çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±",
                  "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€å€‹", "ä¸Š", "ä¹Ÿ", "å¾ˆ",
                  "åˆ°", "èªª", "è¦", "åŽ»", "ä½ ", "æœƒ", "è‘—", "æ²’æœ‰",
                  "çœ‹", "å¥½", "è‡ªå·±", "é€™", "ä»–", "å¥¹", "å®ƒ", "å€‘",
                  "é‚£", "è¢«", "å¾ž", "å°", "ç‚º", "èˆ‡", "ç­‰", "ä½†",
                  "è€Œ", "åŠ", "æˆ–", "ä¹‹", "å…¶", "ä¸­", "æ‰€", "ä»¥",
                  "å¯", "èƒ½", "å°‡", "é‚„", "å› ", "æ­¤", "å‰‡", "å¦‚",
                  "æ–¼", "å€‹", "æ¯", "åˆ", "æŠŠ", "è®“", "ç”¨", "åš"}
    return [w for w in words if len(w) > 1 and w not in stop_words]


class BM25Index:
    """BM25 é—œéµå­—æœå°‹ç´¢å¼•"""

    def __init__(self, chunks: list[dict]):
        self.chunks = chunks
        self.tokenized = [tokenize_chinese(c["text"]) for c in chunks]
        self.bm25 = BM25Okapi(self.tokenized)
        print(f"  âœ… BM25 ç´¢å¼•å»ºç«‹å®Œæˆï¼š{len(chunks)} å€‹æ–‡æª”")

    def search(self, query: str, top_k: int = BM25_TOP_K) -> list[dict]:
        """BM25 æœå°‹"""
        query_tokens = tokenize_chinese(query)
        scores = self.bm25.get_scores(query_tokens)
        top_indices = np.argsort(scores)[::-1][:top_k]
        results = []
        for idx in top_indices:
            if scores[idx] > 0:
                results.append({
                    "index": int(idx),
                    "text": self.chunks[idx]["text"],
                    "source": self.chunks[idx]["source"],
                    "bm25_score": float(scores[idx]),
                })
        return results


# ============================================================
# Hybrid Searchï¼ˆRRF èžåˆï¼‰
# ============================================================
def reciprocal_rank_fusion(dense_results: list[dict],
                           bm25_results: list[dict],
                           k: int = RRF_K,
                           top_k: int = HYBRID_TOP_K) -> list[dict]:
    """
    Reciprocal Rank Fusion (RRF)
    score(d) = Î£ 1/(k + rank_i(d))
    """
    # å»ºç«‹æ–‡æª” â†’ RRF åˆ†æ•¸çš„æ˜ å°„ï¼ˆç”¨ chunk index ä½œç‚º keyï¼‰
    rrf_scores = {}
    doc_info = {}

    # Dense results
    for rank, hit in enumerate(dense_results):
        idx = hit["index"]
        rrf_scores[idx] = rrf_scores.get(idx, 0) + 1.0 / (k + rank + 1)
        doc_info[idx] = {"text": hit["text"], "source": hit["source"]}

    # BM25 results
    for rank, hit in enumerate(bm25_results):
        idx = hit["index"]
        rrf_scores[idx] = rrf_scores.get(idx, 0) + 1.0 / (k + rank + 1)
        doc_info[idx] = {"text": hit["text"], "source": hit["source"]}

    # æŽ’åº
    sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)

    results = []
    for idx, score in sorted_docs[:top_k]:
        results.append({
            "index": idx,
            "text": doc_info[idx]["text"],
            "source": doc_info[idx]["source"],
            "rrf_score": score,
        })
    return results


# ============================================================
# ReRankï¼ˆä½¿ç”¨ LLM åˆ¤æ–·ç›¸é—œæ€§ï¼‰
# ============================================================
RERANK_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€å€‹æ–‡ä»¶ç›¸é—œæ€§è©•ä¼°å°ˆå®¶ã€‚è«‹åˆ¤æ–·çµ¦å®šçš„ã€Œæ–‡ä»¶æ®µè½ã€èˆ‡ã€ŒæŸ¥è©¢å•é¡Œã€çš„ç›¸é—œç¨‹åº¦ã€‚

è©•åˆ†æ¨™æº–ï¼ˆ0-10 åˆ†ï¼‰ï¼š
- 9-10ï¼šæ–‡ä»¶ç›´æŽ¥ä¸”å®Œæ•´åœ°å›žç­”äº†å•é¡Œ
- 7-8ï¼šæ–‡ä»¶åŒ…å«å›žç­”å•é¡Œæ‰€éœ€çš„å¤§éƒ¨åˆ†é—œéµè³‡è¨Š
- 5-6ï¼šæ–‡ä»¶éƒ¨åˆ†ç›¸é—œï¼ŒåŒ…å«ä¸€äº›æœ‰ç”¨è³‡è¨Š
- 3-4ï¼šæ–‡ä»¶ç•¥æœ‰ç›¸é—œï¼Œä½†ä¸»è¦å…§å®¹ä¸åŒ
- 0-2ï¼šæ–‡ä»¶èˆ‡å•é¡Œå®Œå…¨ç„¡é—œ

è«‹åªè¼¸å‡ºä¸€å€‹æ•¸å­—ï¼ˆ0-10ï¼‰ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""


def rerank_with_llm(query: str, candidates: list[dict],
                    top_k: int = RERANK_TOP_K) -> list[dict]:
    """ä½¿ç”¨ LLM å°å€™é¸æ–‡æª”é€²è¡Œ ReRank"""
    scored = []

    for cand in candidates:
        user_prompt = f"ã€æŸ¥è©¢å•é¡Œã€‘\n{query}\n\nã€æ–‡ä»¶æ®µè½ã€‘\n{cand['text']}"
        response = call_llm(RERANK_SYSTEM_PROMPT, user_prompt,
                            temperature=0.0, max_tokens=16)

        # è§£æžåˆ†æ•¸
        try:
            # å˜—è©¦å¾žå›žæ‡‰ä¸­æå–æ•¸å­—
            nums = re.findall(r'\d+(?:\.\d+)?', response)
            score = float(nums[0]) if nums else 5.0
            score = min(max(score, 0), 10)
        except (ValueError, IndexError):
            score = 5.0

        scored.append({
            **cand,
            "rerank_score": score,
        })
        time.sleep(0.1)

    # æŒ‰ ReRank åˆ†æ•¸æŽ’åº
    scored.sort(key=lambda x: x["rerank_score"], reverse=True)
    return scored[:top_k]


# ============================================================
# LLM ç­”æ¡ˆç”Ÿæˆ
# ============================================================
ANSWER_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€å€‹ç²¾æº–çš„å•ç­”åŠ©ç†ã€‚è«‹æ ¹æ“šæä¾›çš„ã€Œåƒè€ƒæ®µè½ã€å›žç­”å•é¡Œã€‚

è¦å‰‡ï¼š
1. åªæ ¹æ“šåƒè€ƒæ®µè½ä¸­çš„å…§å®¹å›žç­”ï¼Œä¸è¦ç·¨é€ 
2. å›žç­”è¦ç°¡æ½”ã€ç²¾ç¢ºã€å®Œæ•´ï¼Œç›´æŽ¥å›žç­”å•é¡Œçš„é‡é»ž
3. åŒ…å«æ‰€æœ‰ç›¸é—œçš„é—œéµæ•¸æ“šã€åç¨±ã€ç´°ç¯€
4. ä½¿ç”¨ç¹é«”ä¸­æ–‡
5. ä¸è¦åŠ ä¸Šã€Œæ ¹æ“šåƒè€ƒæ®µè½ã€ç­‰å‰ç¶´ï¼Œç›´æŽ¥å›žç­”"""


def generate_answer(question: str, chunks: list[dict]) -> str:
    """å¾ž Top-K chunks ç”¨ LLM èƒå–ç²¾æº–ç­”æ¡ˆ"""
    context = ""
    for i, c in enumerate(chunks):
        context += f"ã€æ®µè½ {i+1}ã€‘ï¼ˆ{c['source']}ï¼‰\n{c['text']}\n\n"

    user_prompt = f"ã€åƒè€ƒæ®µè½ã€‘\n{context}\nã€å•é¡Œã€‘\n{question}\n\nè«‹æ ¹æ“šåƒè€ƒæ®µè½ç²¾æº–å›žç­”ä¸Šè¿°å•é¡Œï¼š"

    return call_llm(ANSWER_SYSTEM_PROMPT, user_prompt)


# ============================================================
# Query ReWriteï¼ˆå¤šè¼ªå°è©±æŸ¥è©¢é‡å¯«ï¼‰
# ============================================================
def rewrite_query(prompt_rewrite: str, conversation_history: list[dict],
                  current_question: str) -> str:
    """ä½¿ç”¨ Prompt_ReWrite.txt é‡å¯«å¤šè¼ªå°è©±ä¸­çš„æŸ¥è©¢"""
    # çµ„å»ºæ­·å²å°è©±æ–‡å­—
    history_text = ""
    for msg in conversation_history:
        role = "ä½¿ç”¨è€…" if msg["role"] == "user" else "åŠ©ç†"
        history_text += f"{role}ï¼š{msg['content']}\n"

    user_prompt = f"ã€å°è©±æ­·å²ã€‘\n{history_text}\nã€æœ€æ–°å•é¡Œã€‘\n{current_question}"

    rewritten = call_llm(prompt_rewrite, user_prompt,
                         temperature=0.1, max_tokens=256)
    return rewritten.strip() if rewritten else current_question


# ============================================================
# å®Œæ•´ RAG Pipeline
# ============================================================
def rag_pipeline(query: str, qdrant_client: QdrantClient,
                 bm25_index: BM25Index, chunks: list[dict],
                 label: str = "") -> tuple[str, str]:
    """
    å®Œæ•´ RAG æµç¨‹ï¼šHybrid Search â†’ ReRank â†’ LLM Answer
    å›žå‚³ (answer, source)
    """
    prefix = f"    [{label}] " if label else "    "

    # Step 1: Dense Searchï¼ˆå‘é‡æœå°‹ï¼‰
    emb, _ = get_embedding([query])
    if emb is None:
        return "", ""

    dense_raw = qdrant_client.query_points(
        collection_name=COLLECTION_NAME, query=emb[0], limit=DENSE_TOP_K
    )
    dense_results = []
    for p in dense_raw.points:
        # æ‰¾åˆ°å°æ‡‰çš„ chunk index
        idx = p.id
        dense_results.append({
            "index": idx,
            "text": p.payload["text"],
            "source": p.payload["source"],
            "dense_score": p.score,
        })

    # Step 2: BM25 Searchï¼ˆé—œéµå­—æœå°‹ï¼‰
    bm25_results = bm25_index.search(query, BM25_TOP_K)

    print(f"{prefix}Dense: {len(dense_results)} ç­† | BM25: {len(bm25_results)} ç­†", end="")

    # Step 3: Hybrid Fusionï¼ˆRRFï¼‰
    hybrid_results = reciprocal_rank_fusion(dense_results, bm25_results,
                                            top_k=HYBRID_TOP_K)
    print(f" â†’ RRF: {len(hybrid_results)} ç­†", end="")

    # Step 4: ReRankï¼ˆLLM é‡æ–°æŽ’åºï¼‰
    reranked = rerank_with_llm(query, hybrid_results, RERANK_TOP_K)
    print(f" â†’ ReRank Top-{len(reranked)}", end="")

    # Step 5: LLM Answer Generation
    answer = generate_answer(query, reranked)
    source = reranked[0]["source"] if reranked else ""
    print(f" â†’ âœ…")

    return answer, source


# ============================================================
# ä¸»ç¨‹å¼
# ============================================================
def main():
    print("=" * 65)
    print("èª²å ‚ä½œæ¥­-04ï¼šRAG + Hybrid Search + ReRank + Query ReWrite")
    print("=" * 65)

    # â”€â”€ 1. è®€å–è³‡æ–™ â”€â”€
    print("\nðŸ“‚ æ­¥é©Ÿ 1ï¼šè®€å–è³‡æ–™æª”æ¡ˆ")
    print("-" * 40)

    data_files = {}
    for i in range(1, 6):
        fn = f"data_{i:02d}.txt"
        path = os.path.join(SCRIPT_DIR, fn)
        if not os.path.exists(path):
            # å˜—è©¦å…¶ä»–è·¯å¾‘
            for alt in [os.path.join(SCRIPT_DIR, "data", fn),
                        os.path.join(SCRIPT_DIR, "..", fn)]:
                if os.path.exists(alt):
                    path = alt
                    break
        with open(path, "r", encoding="utf-8") as f:
            data_files[fn] = f.read()
        print(f"  âœ… {fn}ï¼š{len(data_files[fn])} å­—å…ƒ")

    # è®€å– Prompt_ReWrite.txt
    prompt_rewrite_path = os.path.join(SCRIPT_DIR, "Prompt_ReWrite.txt")
    if not os.path.exists(prompt_rewrite_path):
        for alt in [os.path.join(SCRIPT_DIR, "..", "Prompt_ReWrite.txt")]:
            if os.path.exists(alt):
                prompt_rewrite_path = alt
                break
    with open(prompt_rewrite_path, "r", encoding="utf-8") as f:
        prompt_rewrite = f.read()
    print(f"  âœ… Prompt_ReWrite.txtï¼š{len(prompt_rewrite)} å­—å…ƒ")

    # è®€å– questions.csv
    q_path = os.path.join(SCRIPT_DIR, "questions.csv")
    if not os.path.exists(q_path):
        for alt in [os.path.join(SCRIPT_DIR, "..", "questions.csv")]:
            if os.path.exists(alt):
                q_path = alt
                break
    with open(q_path, "r", encoding="utf-8-sig") as f:
        questions = list(csv.DictReader(f))
    print(f"  âœ… questions.csvï¼š{len(questions)} é¡Œ")

    # è®€å– Re_Write_questions.csv
    rw_path = os.path.join(SCRIPT_DIR, "Re_Write_questions.csv")
    if not os.path.exists(rw_path):
        for alt in [os.path.join(SCRIPT_DIR, "..", "Re_Write_questions.csv")]:
            if os.path.exists(alt):
                rw_path = alt
                break
    with open(rw_path, "r", encoding="utf-8-sig") as f:
        rw_questions = list(csv.DictReader(f))
    print(f"  âœ… Re_Write_questions.csvï¼š{len(rw_questions)} é¡Œ")

    # â”€â”€ 2. åˆ‡å¡Š â”€â”€
    print(f"\nðŸ“¦ æ­¥é©Ÿ 2ï¼šæ»‘å‹•è¦–çª—åˆ‡å¡Šï¼ˆsize={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}ï¼‰")
    print("-" * 40)
    chunks = chunk_texts(data_files)
    print(f"\n  ðŸ“Š ç¸½è¨ˆï¼š{len(chunks)} å€‹åˆ‡å¡Š")

    # â”€â”€ 3. é€£æŽ¥ API & Qdrant â”€â”€
    print(f"\nðŸ”— æ­¥é©Ÿ 3ï¼šé€£æŽ¥ Embedding APIã€LLM APIã€Qdrant")
    print("-" * 40)

    _, dim = get_embedding(["æ¸¬è©¦"])
    if dim is None:
        print("âŒ Embedding API ä¸å¯ç”¨")
        return
    print(f"  âœ… Embedding APIï¼šç¶­åº¦ {dim}")

    test_llm = call_llm("ä½ å¥½", "å›žè¦†OK", temperature=0.1)
    if test_llm:
        print(f"  âœ… LLM APIï¼š{LLM_MODEL}")
    else:
        print("âŒ LLM API ä¸å¯ç”¨")
        return

    client = QdrantClient(url="http://localhost:6333")
    print("  âœ… Qdrant é€£æŽ¥æˆåŠŸ")

    # â”€â”€ 4. åµŒå…¥åˆ° Qdrantï¼ˆDenseï¼‰ â”€â”€
    print(f"\nðŸ“¤ æ­¥é©Ÿ 4ï¼šåµŒå…¥åˆ° Qdrantï¼ˆDense Vectorsï¼‰")
    print("-" * 40)

    # åˆªé™¤èˆŠ collection
    existing = [c.name for c in client.get_collections().collections]
    if COLLECTION_NAME in existing:
        client.delete_collection(COLLECTION_NAME)

    client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=VectorParams(size=dim, distance=Distance.COSINE),
    )

    all_points = []
    for i in range(0, len(chunks), 50):
        batch = chunks[i:i + 50]
        embs, _ = get_embedding([c["text"] for c in batch])
        if embs is None:
            continue
        for j, (chunk, emb) in enumerate(zip(batch, embs)):
            all_points.append(PointStruct(
                id=i + j, vector=emb,
                payload={"text": chunk["text"], "source": chunk["source"]},
            ))
        time.sleep(0.2)

    client.upsert(collection_name=COLLECTION_NAME, points=all_points)
    print(f"  âœ… Qdrantï¼š{len(all_points)} å€‹å‘é‡ï¼ˆDenseï¼‰")

    # â”€â”€ 5. å»ºç«‹ BM25 ç´¢å¼•ï¼ˆSparseï¼‰ â”€â”€
    print(f"\nðŸ“š æ­¥é©Ÿ 5ï¼šå»ºç«‹ BM25 ç´¢å¼•ï¼ˆSparse / é—œéµå­—æœå°‹ï¼‰")
    print("-" * 40)
    bm25_index = BM25Index(chunks)

    # â”€â”€ 6. è™•ç† questions.csvï¼ˆç›´æŽ¥å•é¡Œï¼‰ â”€â”€
    print(f"\nðŸ” æ­¥é©Ÿ 6ï¼šè™•ç† questions.csvï¼ˆ{len(questions)} é¡Œï¼‰")
    print(f"  æµç¨‹ï¼šHybrid Search â†’ ReRank â†’ LLM Answer")
    print("-" * 40)

    q_results = []
    for q in questions:
        q_id = q["é¡Œç›®_ID"]
        q_text = q["é¡Œç›®"]
        print(f"\n  Q{q_id}: {q_text[:50]}...")

        answer, source = rag_pipeline(
            q_text, client, bm25_index, chunks, label=f"Q{q_id}"
        )

        q_results.append({
            "é¡Œç›®_ID": q_id,
            "é¡Œç›®": q_text,
            "æ¨™æº–ç­”æ¡ˆ": answer,
            "ä¾†æºæ–‡ä»¶": source,
        })
        time.sleep(0.3)

    # è¼¸å‡º questions_answer.csv
    qa_path = os.path.join(SCRIPT_DIR, "questions_answer.csv")
    with open(qa_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=["é¡Œç›®_ID", "é¡Œç›®", "æ¨™æº–ç­”æ¡ˆ", "ä¾†æºæ–‡ä»¶"])
        writer.writeheader()
        writer.writerows(q_results)
    print(f"\n  âœ… questions_answer.csvï¼š{len(q_results)} ç­†")

    # â”€â”€ 7. è™•ç† Re_Write_questions.csvï¼ˆå¤šè¼ªå°è©±ï¼‰ â”€â”€
    print(f"\nðŸ”„ æ­¥é©Ÿ 7ï¼šè™•ç† Re_Write_questions.csvï¼ˆ{len(rw_questions)} é¡Œï¼‰")
    print(f"  æµç¨‹ï¼šQuery ReWrite â†’ Hybrid Search â†’ ReRank â†’ LLM Answer")
    print("-" * 40)

    # æŒ‰ conversation_id åˆ†çµ„
    conversations = {}
    for rw in rw_questions:
        cid = rw["conversation_id"]
        if cid not in conversations:
            conversations[cid] = []
        conversations[cid].append(rw)

    rw_results = []

    for cid, conv_questions in conversations.items():
        print(f"\n  ðŸ’¬ å°è©± {cid}ï¼ˆ{len(conv_questions)} è¼ªï¼‰")
        history = []  # ç´¯ç©å°è©±æ­·å²

        for rw in conv_questions:
            qid = rw["questions_id"]
            q_text = rw["questions"]
            print(f"    Q{cid}-{qid}: {q_text}")

            # æ˜¯å¦éœ€è¦ Query ReWrite
            if len(history) > 0:
                # æœ‰æ­·å² â†’ é‡å¯«æŸ¥è©¢
                rewritten = rewrite_query(prompt_rewrite, history, q_text)
                print(f"      ðŸ”„ ReWrite: {rewritten[:60]}...")
                search_query = rewritten
            else:
                # ç¬¬ä¸€è¼ª â†’ ç›´æŽ¥æœå°‹
                search_query = q_text

            # RAG Pipeline
            answer, source = rag_pipeline(
                search_query, client, bm25_index, chunks,
                label=f"C{cid}-Q{qid}"
            )

            rw_results.append({
                "conversation_id": cid,
                "questions_id": qid,
                "questions": q_text,
                "answer": answer,
                "source": source,
            })

            # ç´¯ç©æ­·å²
            history.append({"role": "user", "content": q_text})
            history.append({"role": "assistant", "content": answer})

            time.sleep(0.3)

    # è¼¸å‡º Re_Write_answer.csv
    rwa_path = os.path.join(SCRIPT_DIR, "Re_Write_answer.csv")
    with open(rwa_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f,
                                fieldnames=["conversation_id", "questions_id",
                                            "questions", "answer", "source"])
        writer.writeheader()
        writer.writerows(rw_results)
    print(f"\n  âœ… Re_Write_answer.csvï¼š{len(rw_results)} ç­†")

    # â”€â”€ 8. è¼¸å‡ºçµæžœæ‘˜è¦ â”€â”€
    print(f"""
{'=' * 65}
âœ… èª²å ‚ä½œæ¥­-04 å®Œæˆï¼
{'=' * 65}

ðŸ“‹ ç³»çµ±æž¶æ§‹ï¼š
  åˆ‡å¡Šæ–¹æ³•ï¼šæ»‘å‹•è¦–çª—ï¼ˆsize={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}ï¼‰
  åˆ‡å¡Šæ•¸é‡ï¼š{len(chunks)} å¡Š
  Dense Searchï¼šQdrant VDBï¼ˆå‘é‡ç¶­åº¦ {dim}ï¼‰
  Sparse Searchï¼šBM25ï¼ˆjieba ä¸­æ–‡åˆ†è©žï¼‰
  Hybrid Fusionï¼šReciprocal Rank Fusionï¼ˆk={RRF_K}ï¼‰
  ReRankï¼šLLM-based Relevance Scoringï¼ˆ{LLM_MODEL}ï¼‰
  Answer Genï¼šLLM Top-{RERANK_TOP_K} â†’ ç²¾æº–èƒå–

ðŸ“‹ è™•ç†çµæžœï¼š
  questions_answer.csvï¼š{len(q_results)} é¡Œ â†’ {qa_path}
  Re_Write_answer.csvï¼š{len(rw_results)} é¡Œ â†’ {rwa_path}

ðŸ“‹ API ä½¿ç”¨ï¼š
  Embeddingï¼š{EMBED_API_URL}
  LLMï¼š{LLM_API_URL}ï¼ˆ{LLM_MODEL}ï¼‰
""")

    # é¡¯ç¤ºç­”æ¡ˆé è¦½
    print("ðŸ“ questions_answer é è¦½ï¼š")
    for r in q_results:
        ans_preview = r["æ¨™æº–ç­”æ¡ˆ"][:60] + "..." if len(r["æ¨™æº–ç­”æ¡ˆ"]) > 60 else r["æ¨™æº–ç­”æ¡ˆ"]
        print(f"  Q{r['é¡Œç›®_ID']}: {ans_preview} [{r['ä¾†æºæ–‡ä»¶']}]")

    print(f"\nðŸ“ Re_Write_answer é è¦½ï¼š")
    for r in rw_results:
        ans_preview = r["answer"][:60] + "..." if len(r["answer"]) > 60 else r["answer"]
        print(f"  C{r['conversation_id']}-Q{r['questions_id']}: {ans_preview} [{r['source']}]")


if __name__ == "__main__":
    main()