"""
èª²å¾Œå¯¦æˆ° â€” è‡ªå‹•æŸ¥è­‰ AI v2.0 (å…¨é¢å‡ç´šç‰ˆ)
========================================

å‡ç´šå…§å®¹ï¼š
1. æŒä¹…åŒ–å¿«å– + TTL éæœŸæ©Ÿåˆ¶
2. é‡è©¦æ©Ÿåˆ¶ + æ›´å®Œå–„çš„éŒ¯èª¤è™•ç†
3. å¤šä¾†æºæ•´åˆé–±è®€ï¼ˆä¸åªè®€ç¬¬ä¸€ç­†ï¼‰
4. è±å¯Œçš„ State è¨­è¨ˆï¼ˆä¾†æºè¿½è¹¤ã€æœå°‹æ­·å²ã€ä¿¡å¿ƒåº¦ï¼‰
5. æ›´ç´°ç·»çš„ Planner æ±ºç­–ï¼ˆJSON è©•ä¼°ï¼‰
6. éåŒæ­¥ä½µç™¼è™•ç†ï¼ˆåŒæ™‚è®€å–å¤šå€‹ç¶²é ï¼‰
7. æœå°‹å»é‡ï¼ˆé¿å…é‡è¤‡æœå°‹ç›¸åŒé—œéµå­—ï¼‰

å®‰è£éœ€æ±‚ï¼š
pip install langgraph langchain langchain-openai playwright requests aiohttp
python -m playwright install-deps
python -m playwright install chromium

API é…ç½®ï¼š
- LLM: https://ws-03.wade0426.me/v1 (gpt-oss-120b)
- SearXNG: https://puli-8080.huannago.com/search
"""

import os
import time
import json
import base64
import hashlib
import asyncio
import requests
from typing import TypedDict, List, Optional, Dict, Any
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, asdict

# Playwright éåŒæ­¥ç‰ˆæœ¬
from playwright.async_api import async_playwright, Browser, Page

# LangChain / LangGraph ç›¸é—œ
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END

# ============================================================
# 1. è¨­å®šèˆ‡åˆå§‹åŒ–
# ============================================================

API_KEY = ""
BASE_URL = "https://ws-03.wade0426.me/v1"
MODEL_NAME = "/models/gpt-oss-120b"
SEARXNG_URL = "https://puli-8080.huannago.com/search"

# å¿«å–è¨­å®š
CACHE_DIR = Path("./cache")
CACHE_DIR.mkdir(exist_ok=True)
CACHE_TTL = 86400  # å¿«å–æœ‰æ•ˆæœŸï¼š24 å°æ™‚

# æœå°‹è¨­å®š
MAX_SEARCH_STEPS = 3
MAX_RETRIES = 3
VLM_READ_COUNT = 2  # æ¯æ¬¡æœå°‹è®€å–å‰å¹¾ç­†çµæœ

# åˆå§‹åŒ–å…±ç”¨ LLM
llm = ChatOpenAI(
    base_url=BASE_URL,
    api_key=API_KEY,
    model=MODEL_NAME,
    temperature=0.1
)


# ============================================================
# 2. æŒä¹…åŒ–å¿«å–ç³»çµ±
# ============================================================

@dataclass
class CacheEntry:
    """å¿«å–é …ç›®çµæ§‹"""
    question: str
    answer: str
    sources: List[str]
    timestamp: float
    confidence: float
    
    def is_expired(self, ttl: int = CACHE_TTL) -> bool:
        """æª¢æŸ¥æ˜¯å¦éæœŸ"""
        return time.time() - self.timestamp > ttl
    
    def to_dict(self) -> dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: dict) -> "CacheEntry":
        return cls(**data)


class PersistentCache:
    """æŒä¹…åŒ–å¿«å–ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: Path = CACHE_DIR, ttl: int = CACHE_TTL):
        self.cache_dir = cache_dir
        self.ttl = ttl
        self.cache_dir.mkdir(exist_ok=True)
        # è¨˜æ†¶é«”å¿«å–ï¼ˆåŠ é€Ÿè®€å–ï¼‰
        self._memory_cache: Dict[str, CacheEntry] = {}
    
    def _get_cache_key(self, question: str) -> str:
        """ç”Ÿæˆå¿«å– keyï¼ˆä½¿ç”¨ MD5 hashï¼‰"""
        # æ­£è¦åŒ–å•é¡Œï¼ˆå»é™¤å¤šé¤˜ç©ºç™½ï¼‰
        normalized = " ".join(question.strip().split())
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    def _get_cache_path(self, key: str) -> Path:
        """å–å¾—å¿«å–æª”æ¡ˆè·¯å¾‘"""
        return self.cache_dir / f"{key}.json"
    
    def get(self, question: str) -> Optional[CacheEntry]:
        """å–å¾—å¿«å–"""
        key = self._get_cache_key(question)
        
        # å…ˆæª¢æŸ¥è¨˜æ†¶é«”å¿«å–
        if key in self._memory_cache:
            entry = self._memory_cache[key]
            if not entry.is_expired(self.ttl):
                return entry
            else:
                # éæœŸå°±åˆªé™¤
                del self._memory_cache[key]
        
        # å†æª¢æŸ¥æª”æ¡ˆå¿«å–
        cache_path = self._get_cache_path(key)
        if cache_path.exists():
            try:
                data = json.loads(cache_path.read_text(encoding='utf-8'))
                entry = CacheEntry.from_dict(data)
                if not entry.is_expired(self.ttl):
                    # è¼‰å…¥åˆ°è¨˜æ†¶é«”å¿«å–
                    self._memory_cache[key] = entry
                    return entry
                else:
                    # éæœŸå°±åˆªé™¤æª”æ¡ˆ
                    cache_path.unlink()
            except (json.JSONDecodeError, KeyError) as e:
                print(f"    å¿«å–è®€å–å¤±æ•—: {e}")
                cache_path.unlink(missing_ok=True)
        
        return None
    
    def set(self, question: str, answer: str, sources: List[str], confidence: float = 0.8):
        """å„²å­˜å¿«å–"""
        key = self._get_cache_key(question)
        entry = CacheEntry(
            question=question,
            answer=answer,
            sources=sources,
            timestamp=time.time(),
            confidence=confidence
        )
        
        # å„²å­˜åˆ°è¨˜æ†¶é«”
        self._memory_cache[key] = entry
        
        # å„²å­˜åˆ°æª”æ¡ˆ
        cache_path = self._get_cache_path(key)
        cache_path.write_text(
            json.dumps(entry.to_dict(), ensure_ascii=False, indent=2),
            encoding='utf-8'
        )
    
    def clear_expired(self):
        """æ¸…ç†éæœŸå¿«å–"""
        cleared = 0
        for cache_file in self.cache_dir.glob("*.json"):
            try:
                data = json.loads(cache_file.read_text(encoding='utf-8'))
                entry = CacheEntry.from_dict(data)
                if entry.is_expired(self.ttl):
                    cache_file.unlink()
                    cleared += 1
            except:
                cache_file.unlink()
                cleared += 1
        return cleared
    
    def get_stats(self) -> dict:
        """å–å¾—å¿«å–çµ±è¨ˆ"""
        total = len(list(self.cache_dir.glob("*.json")))
        memory_count = len(self._memory_cache)
        return {
            "total_cached": total,
            "in_memory": memory_count,
            "cache_dir": str(self.cache_dir)
        }


# åˆå§‹åŒ–å¿«å–
cache = PersistentCache()


# ============================================================
# 3. æœå°‹å·¥å…·ï¼ˆå«é‡è©¦æ©Ÿåˆ¶ï¼‰
# ============================================================

def search_searxng(query: str, limit: int = 3, retries: int = MAX_RETRIES) -> List[dict]:
    """
    [å·¥å…·] åŸ·è¡Œ SearXNG æœå°‹ï¼ˆå«é‡è©¦æ©Ÿåˆ¶ï¼‰
    """
    print(f"    [SearXNG] æ­£åœ¨æœå°‹: {query}")
    
    params = {
        "q": query,
        "format": "json",
        "language": "zh-TW"
    }
    
    for attempt in range(retries):
        try:
            response = requests.get(SEARXNG_URL, params=params, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                # éæ¿¾æœ‰ URL çš„çµæœï¼Œä¸¦å»é™¤é‡è¤‡
                results = []
                seen_urls = set()
                
                for r in data.get('results', []):
                    url = r.get('url', '')
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        results.append({
                            'title': r.get('title', 'ç„¡æ¨™é¡Œ'),
                            'url': url,
                            'content': r.get('content', ''),
                            'engine': r.get('engine', 'unknown')
                        })
                
                print(f"    [SearXNG] æ‰¾åˆ° {len(results)} ç­†çµæœ")
                return results[:limit]
            
            elif response.status_code == 429:
                # Rate limitï¼Œç­‰å¾…å¾Œé‡è©¦
                wait_time = 2 ** attempt
                print(f"    [SearXNG] Rate limitedï¼Œç­‰å¾… {wait_time} ç§’...")
                time.sleep(wait_time)
            else:
                print(f"    [SearXNG] HTTP {response.status_code}")
                
        except requests.exceptions.Timeout:
            print(f"    [SearXNG] é€¾æ™‚ï¼Œé‡è©¦ {attempt + 1}/{retries}")
            time.sleep(1)
        except requests.exceptions.RequestException as e:
            print(f"    [SearXNG] é€£ç·šéŒ¯èª¤: {e}")
            time.sleep(1)
    
    return []


# ============================================================
# 4. VLM è¦–è¦ºé–±è®€ï¼ˆéåŒæ­¥ + é‡è©¦ï¼‰
# ============================================================

async def vlm_read_single_page(
    browser: Browser, 
    url: str, 
    title: str,
    max_screenshots: int = 2
) -> dict:
    """
    [éåŒæ­¥] è®€å–å–®ä¸€ç¶²é 
    """
    result = {
        "url": url,
        "title": title,
        "content": "",
        "success": False,
        "error": None
    }
    
    context = None
    try:
        context = await browser.new_context(
            viewport={'width': 1280, 'height': 1200},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )
        page = await context.new_page()
        
        # è¨­å®šè«‹æ±‚æ””æˆªï¼ˆé˜»æ“‹å»£å‘Šå’Œè¿½è¹¤å™¨ï¼‰
        await page.route("**/*", lambda route: (
            route.abort() if any(x in route.request.url for x in [
                'analytics', 'tracking', 'ads', 'doubleclick', 'facebook.com/tr'
            ]) else route.continue_()
        ))
        
        # å‰å¾€ç¶²é 
        await page.goto(url, wait_until="domcontentloaded", timeout=20000)
        await page.wait_for_timeout(2000)
        
        # æ³¨å…¥ CSS éš±è—å¹²æ“¾å…ƒç´ 
        await page.add_style_tag(content="""
            iframe, .ad, .ads, .advertisement, 
            [class*="cookie"], [class*="popup"], 
            [class*="modal"], [class*="overlay"] {
                opacity: 0 !important;
                pointer-events: none !important;
                display: none !important;
            }
        """)
        
        # æ»¾å‹•æˆªåœ–
        screenshots_b64 = []
        for i in range(max_screenshots):
            screenshot = await page.screenshot(type='png')
            b64 = base64.b64encode(screenshot).decode('utf-8')
            screenshots_b64.append(b64)
            await page.evaluate("window.scrollBy(0, 800)")
            await page.wait_for_timeout(800)
        
        # ä½¿ç”¨ VLM åˆ†æ
        if screenshots_b64:
            content = await analyze_screenshots_with_vlm(screenshots_b64, title)
            result["content"] = content
            result["success"] = True
            
    except Exception as e:
        result["error"] = str(e)
        print(f"    [VLM] è®€å–å¤±æ•— ({title}): {e}")
    finally:
        if context:
            await context.close()
    
    return result


async def analyze_screenshots_with_vlm(screenshots_b64: List[str], title: str) -> str:
    """
    [VLM] åˆ†ææˆªåœ–å…§å®¹
    """
    msg_content = [
        {
            "type": "text",
            "text": f"""é€™æ˜¯ç¶²é ã€Œ{title}ã€çš„æˆªåœ–ã€‚
è«‹é–±è®€ä¸¦æå–ï¼š
1. ä¸»è¦å…§å®¹å’Œé—œéµè³‡è¨Š
2. é‡è¦æ•¸æ“šæˆ–çµ±è¨ˆ
3. æ–°èé‡é»æˆ–çµè«–
å¿½ç•¥å»£å‘Šã€é¸å–®å’Œç„¡é—œå…§å®¹ã€‚ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"""
        }
    ]
    
    for img in screenshots_b64:
        msg_content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/png;base64,{img}"}
        })
    
    try:
        # LangChain çš„ invoke æ˜¯åŒæ­¥çš„ï¼Œé€™è£¡ç”¨ run_in_executor
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: llm.invoke([HumanMessage(content=msg_content)])
        )
        return response.content
    except Exception as e:
        return f"VLM åˆ†æå¤±æ•—: {e}"


async def vlm_read_websites_parallel(
    urls: List[dict],
    max_concurrent: int = 3
) -> List[dict]:
    """
    [éåŒæ­¥] ä¸¦è¡Œè®€å–å¤šå€‹ç¶²é 
    """
    print(f"    [VLM] å•Ÿå‹•ä¸¦è¡Œé–±è®€ {len(urls)} å€‹ç¶²é ...")
    
    results = []
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--disable-dev-shm-usage",
                "--no-sandbox"
            ]
        )
        
        # ä½¿ç”¨ä¿¡è™Ÿé‡æ§åˆ¶ä¸¦è¡Œæ•¸
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def read_with_semaphore(url_info):
            async with semaphore:
                return await vlm_read_single_page(
                    browser,
                    url_info['url'],
                    url_info['title']
                )
        
        tasks = [read_with_semaphore(url_info) for url_info in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        await browser.close()
    
    # è™•ç†çµæœ
    processed_results = []
    for r in results:
        if isinstance(r, Exception):
            processed_results.append({
                "url": "",
                "title": "",
                "content": f"è®€å–å¤±æ•—: {r}",
                "success": False
            })
        else:
            processed_results.append(r)
    
    success_count = sum(1 for r in processed_results if r.get('success'))
    print(f"    [VLM] å®Œæˆï¼ŒæˆåŠŸ {success_count}/{len(urls)}")
    
    return processed_results


def synthesize_sources(sources: List[dict], question: str) -> str:
    """
    [LLM] æ•´åˆå¤šä¾†æºè³‡è¨Š
    """
    if not sources:
        return "ç„¡æ³•å–å¾—ä»»ä½•è³‡è¨Šã€‚"
    
    # å»ºç«‹ä¾†æºæ‘˜è¦
    source_texts = []
    for i, s in enumerate(sources):
        if s.get('success') and s.get('content'):
            source_texts.append(f"""
ã€ä¾†æº {i+1}: {s['title']}ã€‘
URL: {s['url']}
å…§å®¹:
{s['content']}
""")
    
    if not source_texts:
        return "æ‰€æœ‰ç¶²é è®€å–å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è³‡è¨Šæ•´åˆå°ˆå®¶ã€‚
è«‹æ ¹æ“šä»¥ä¸‹å¤šå€‹ä¾†æºçš„è³‡è¨Šï¼Œé‡å°ä½¿ç”¨è€…å•é¡Œé€²è¡Œæ•´åˆåˆ†æã€‚

ä½¿ç”¨è€…å•é¡Œï¼š{question}

ä¾†æºè³‡è¨Šï¼š
{sources}

è«‹ï¼š
1. æ•´åˆå„ä¾†æºçš„é—œéµè³‡è¨Š
2. æ¨™è¨»è³‡è¨Šä¾†è‡ªå“ªå€‹ä¾†æº
3. å¦‚æœ‰çŸ›ç›¾ï¼ŒæŒ‡å‡ºä¸åŒèªªæ³•
4. ç”¨ç¹é«”ä¸­æ–‡å›ç­”""")
    ])
    
    try:
        result = (prompt | llm | StrOutputParser()).invoke({
            "question": question,
            "sources": "\n".join(source_texts)
        })
        return result
    except Exception as e:
        # å¦‚æœæ•´åˆå¤±æ•—ï¼Œç›´æ¥è¿”å›åŸå§‹å…§å®¹
        return "\n\n".join(source_texts)


def execute_search_tool(query: str, question: str, search_history: List[str]) -> dict:
    """
    [æ•´åˆå·¥å…·] æœå°‹ + VLM é–±è®€ + ä¾†æºæ•´åˆ
    """
    # æª¢æŸ¥æ˜¯å¦é‡è¤‡æœå°‹
    if query in search_history:
        print(f"    [æœå°‹] è·³éé‡è¤‡é—œéµå­—: {query}")
        return {
            "content": f"å·²æœå°‹éã€Œ{query}ã€ï¼Œè·³éé‡è¤‡æœå°‹ã€‚",
            "sources": [],
            "is_duplicate": True
        }
    
    # 1. åŸ·è¡Œæœå°‹
    results = search_searxng(query, limit=5)
    
    if not results:
        return {
            "content": "æœªæ‰¾åˆ°ç›¸é—œçµæœï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚",
            "sources": [],
            "is_duplicate": False
        }
    
    # 2. æº–å‚™è¦è®€å–çš„ URL
    urls_to_read = results[:VLM_READ_COUNT]
    
    # 3. ä¸¦è¡Œ VLM é–±è®€
    vlm_results = asyncio.run(vlm_read_websites_parallel(urls_to_read))
    
    # 4. æ•´åˆä¾†æº
    synthesized = synthesize_sources(vlm_results, question)
    
    # 5. æ”¶é›†æˆåŠŸçš„ä¾†æº URL
    source_urls = [r['url'] for r in vlm_results if r.get('success')]
    
    return {
        "content": synthesized,
        "sources": source_urls,
        "is_duplicate": False,
        "search_results_summary": [
            {"title": r['title'], "url": r['url']} 
            for r in results
        ]
    }


# ============================================================
# 5. å®šç¾© Stateï¼ˆè±å¯Œç‰ˆï¼‰
# ============================================================

class AgentState(TypedDict):
    # åŸºæœ¬æ¬„ä½
    question: str               # åŸå§‹å•é¡Œ
    knowledge_base: str         # ç´¯ç©çš„è³‡è¨Š
    search_query: str           # ç•¶å‰ç”Ÿæˆçš„é—œéµå­—
    steps: int                  # æœå°‹æ­¥æ•¸
    final_answer: str           # æœ€çµ‚ç­”æ¡ˆ
    is_sufficient: bool         # æ±ºç­–çµæœ
    
    # æ–°å¢æ¬„ä½
    sources: List[str]          # æ‰€æœ‰ä¾†æº URL
    search_history: List[str]   # æœå°‹æ­·å²ï¼ˆé¿å…é‡è¤‡ï¼‰
    confidence: float           # ç­”æ¡ˆä¿¡å¿ƒåº¦ (0-1)
    planner_analysis: dict      # Planner çš„è©³ç´°åˆ†æ


# ============================================================
# 6. å®šç¾© Nodes
# ============================================================

def check_cache_node(state: AgentState) -> dict:
    """[Node 1] å¿«å–æª¢æŸ¥"""
    question = state["question"]
    print(f"\n[1] å¿«å–æª¢æŸ¥: {question}")
    
    cached = cache.get(question)
    
    if cached:
        print(f"    âœ“ å‘½ä¸­å¿«å–ï¼(ä¿¡å¿ƒåº¦: {cached.confidence:.0%})")
        return {
            "final_answer": cached.answer,
            "sources": cached.sources,
            "confidence": cached.confidence,
            "is_sufficient": True
        }
    else:
        print("    âœ— ç„¡å¿«å–ï¼Œé€²å…¥æ±ºç­–æµç¨‹ã€‚")
        return {
            "knowledge_base": "",
            "steps": 0,
            "sources": [],
            "search_history": [],
            "confidence": 0.0,
            "is_sufficient": False
        }


def planner_node(state: AgentState) -> dict:
    """[Node 2] æ±ºç­–ï¼ˆç´°ç·»ç‰ˆï¼Œä½¿ç”¨ JSON è©•ä¼°ï¼‰"""
    print(f"[2] AI æ±ºç­– (Planner)... (æ­¥æ•¸: {state['steps']}/{MAX_SEARCH_STEPS})")
    
    # å®‰å…¨æ©Ÿåˆ¶ï¼šæœ€å¤šæœå°‹æ¬¡æ•¸
    if state["steps"] >= MAX_SEARCH_STEPS:
        print("    âš  å·²é”æœ€å¤§æœå°‹æ¬¡æ•¸ï¼Œå¼·åˆ¶å›ç­”ã€‚")
        return {
            "is_sufficient": True,
            "planner_analysis": {
                "completeness": 5,
                "credibility": 5,
                "need_more_search": False,
                "reason": "é”åˆ°æœ€å¤§æœå°‹æ¬¡æ•¸é™åˆ¶"
            }
        }
    
    # ç¬¬ä¸€æ¬¡æœå°‹å‰ï¼Œç›´æ¥åˆ¤å®šéœ€è¦æœå°‹
    if state["steps"] == 0 and not state.get("knowledge_base"):
        print("    â†’ é¦–æ¬¡æŸ¥è©¢ï¼Œéœ€è¦æœå°‹è³‡è¨Šã€‚")
        return {
            "is_sufficient": False,
            "planner_analysis": {
                "completeness": 0,
                "credibility": 0,
                "need_more_search": True,
                "reason": "å°šæœªæ”¶é›†ä»»ä½•è³‡è¨Š"
            }
        }
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€å€‹åš´è¬¹çš„æŸ¥è­‰æ±ºç­–è€…ã€‚è«‹è©•ä¼°ç›®å‰æ”¶é›†çš„è³‡è¨Šæ˜¯å¦è¶³ä»¥å›ç­”å•é¡Œã€‚

ä½¿ç”¨è€…å•é¡Œï¼š{question}

ç›®å‰æ”¶é›†åˆ°çš„è³‡è¨Šï¼š
---
{context}
---

å·²æœå°‹éçš„é—œéµå­—ï¼š{search_history}

è«‹ç”¨ JSON æ ¼å¼å›æ‡‰ï¼ˆä¸è¦åŠ  markdownï¼‰ï¼š
{{
    "completeness": <1-10 è³‡è¨Šå®Œæ•´åº¦>,
    "credibility": <1-10 ä¾†æºå¯ä¿¡åº¦>,
    "need_more_search": <true/false>,
    "reason": "<ç°¡çŸ­èªªæ˜ç†ç”±>",
    "suggested_query": "<å¦‚æœéœ€è¦æ›´å¤šæœå°‹ï¼Œå»ºè­°çš„é—œéµå­—>"
}}

è©•ä¼°æ¨™æº–ï¼š
- completeness >= 7 ä¸” credibility >= 6 æ‰ç®—è¶³å¤ 
- å¦‚æœå·²æœ‰è¶³å¤ è³‡è¨Šä½†ç¼ºä¹ä½è­‰ï¼Œå¯å†æœå°‹ä¸€æ¬¡é©—è­‰""")
    ])
    
    try:
        response = (prompt | llm | StrOutputParser()).invoke({
            "question": state["question"],
            "context": state.get("knowledge_base", "ï¼ˆç„¡è³‡è¨Šï¼‰"),
            "search_history": ", ".join(state.get("search_history", [])) or "ï¼ˆç„¡ï¼‰"
        })
        
        # è§£æ JSON
        # ç§»é™¤å¯èƒ½çš„ markdown æ¨™è¨˜
        response = response.strip()
        if response.startswith("```"):
            response = response.split("\n", 1)[1]
        if response.endswith("```"):
            response = response.rsplit("```", 1)[0]
        response = response.strip()
        
        analysis = json.loads(response)
        
        completeness = analysis.get("completeness", 0)
        credibility = analysis.get("credibility", 0)
        need_more = analysis.get("need_more_search", True)
        
        is_sufficient = (completeness >= 7 and credibility >= 6) or not need_more
        confidence = (completeness + credibility) / 20  # è½‰æ›ç‚º 0-1
        
        print(f"    å®Œæ•´åº¦: {completeness}/10, å¯ä¿¡åº¦: {credibility}/10")
        print(f"    æ±ºç­–: {'è³‡è¨Šè¶³å¤ ' if is_sufficient else 'éœ€è¦æ›´å¤šæœå°‹'}")
        if analysis.get("reason"):
            print(f"    ç†ç”±: {analysis['reason']}")
        
        return {
            "is_sufficient": is_sufficient,
            "confidence": confidence,
            "planner_analysis": analysis,
            # å¦‚æœ Planner æœ‰å»ºè­°é—œéµå­—ï¼Œå…ˆå­˜èµ·ä¾†
            "search_query": analysis.get("suggested_query", "")
        }
        
    except json.JSONDecodeError as e:
        print(f"    âš  JSON è§£æå¤±æ•—ï¼Œä½¿ç”¨ä¿å®ˆç­–ç•¥")
        # ä¿å®ˆç­–ç•¥ï¼šç¹¼çºŒæœå°‹
        return {
            "is_sufficient": state["steps"] >= 2,  # è‡³å°‘æœå°‹å…©æ¬¡
            "confidence": 0.5,
            "planner_analysis": {"error": str(e)}
        }


def query_gen_node(state: AgentState) -> dict:
    """[Node 3] ç”Ÿæˆæœå°‹é—œéµå­—"""
    print("[3] ç”Ÿæˆé—œéµå­— (Query Gen)...")
    
    # å¦‚æœ Planner å·²ç¶“å»ºè­°äº†é—œéµå­—ï¼Œç›´æ¥ä½¿ç”¨
    if state.get("search_query"):
        suggested = state["search_query"]
        if suggested not in state.get("search_history", []):
            print(f"    ä½¿ç”¨ Planner å»ºè­°: {suggested}")
            return {"search_query": suggested}
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€å€‹æœå°‹å°ˆå®¶ã€‚æ ¹æ“šå•é¡Œèˆ‡å·²çŸ¥è³‡è¨Šï¼Œç”Ÿæˆæœ€é©åˆçš„æœå°‹é—œéµå­—ã€‚

å•é¡Œï¼š{question}

å·²çŸ¥è³‡è¨Šï¼š
{context}

å·²æœå°‹éï¼ˆé¿å…é‡è¤‡ï¼‰ï¼š{search_history}

è¦å‰‡ï¼š
1. åªè¼¸å‡ºä¸€å€‹æœå°‹é—œéµå­—æˆ–çŸ­èª
2. ä¸è¦åŠ å¼•è™Ÿæˆ–å…¶ä»–ç¬¦è™Ÿ
3. ä½¿ç”¨ç¹é«”ä¸­æ–‡
4. é¿å…èˆ‡å·²æœå°‹éçš„é—œéµå­—é‡è¤‡æˆ–éæ–¼ç›¸ä¼¼
5. å¦‚æœéœ€è¦é©—è­‰è³‡è¨Šï¼Œå¯ä»¥æœå°‹ç›¸é—œçš„æ¬Šå¨ä¾†æº

è¼¸å‡ºé—œéµå­—ï¼š""")
    ])
    
    query = (prompt | llm | StrOutputParser()).invoke({
        "question": state["question"],
        "context": state.get("knowledge_base", "ï¼ˆç„¡ï¼‰")[:500],
        "search_history": ", ".join(state.get("search_history", [])) or "ï¼ˆç„¡ï¼‰"
    }).strip()
    
    # æ¸…ç†è¼¸å‡º
    query = query.replace('"', '').replace("'", "").strip()
    
    print(f"    ç”Ÿæˆ: {query}")
    return {"search_query": query}


def search_tool_node(state: AgentState) -> dict:
    """[Node 4] åŸ·è¡Œæœå°‹èˆ‡ VLM é–±è®€"""
    print("[4] åŸ·è¡Œæœå°‹èˆ‡è¦–è¦ºé–±è®€...")
    
    query = state["search_query"]
    search_history = state.get("search_history", [])
    
    # åŸ·è¡Œæ•´åˆå·¥å…·
    result = execute_search_tool(query, state["question"], search_history)
    
    # æ›´æ–°ç‹€æ…‹
    new_history = search_history + [query]
    new_sources = list(set(state.get("sources", []) + result.get("sources", [])))
    
    # æ›´æ–°çŸ¥è­˜åº«
    if not result.get("is_duplicate"):
        new_kb = state.get("knowledge_base", "")
        new_kb += f"\n\n=== æœå°‹ã€Œ{query}ã€çš„çµæœ ===\n{result['content']}"
    else:
        new_kb = state.get("knowledge_base", "")
    
    return {
        "knowledge_base": new_kb,
        "search_history": new_history,
        "sources": new_sources,
        "steps": state["steps"] + 1
    }


def final_answer_node(state: AgentState) -> dict:
    """[Node 5] ç”¢ç”Ÿæœ€çµ‚å ±å‘Š"""
    print("[5] æ•´ç†æœ€çµ‚å ±å‘Š...")
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æŸ¥è­‰å ±å‘Šæ’°å¯«è€…ã€‚è«‹æ ¹æ“šæ”¶é›†åˆ°çš„è³‡è¨Šå›ç­”ä½¿ç”¨è€…å•é¡Œã€‚

å•é¡Œï¼š{question}

æ”¶é›†åˆ°çš„è³‡è¨Šï¼š
{context}

åƒè€ƒä¾†æºï¼š{sources}

è«‹ï¼š
1. ç”¨ç¹é«”ä¸­æ–‡å®Œæ•´å›ç­”å•é¡Œ
2. æ•´ç†é—œéµç™¼ç¾å’Œé‡é»
3. å¦‚æœ‰ä¸ç¢ºå®šæˆ–çŸ›ç›¾çš„è³‡è¨Šï¼Œè«‹èªªæ˜
4. åœ¨å›ç­”æœ«å°¾åˆ—å‡ºä¸»è¦åƒè€ƒä¾†æº

å›ç­”ï¼š""")
    ])
    
    sources_text = "\n".join([f"- {url}" for url in state.get("sources", [])]) or "ç„¡å¤–éƒ¨ä¾†æº"
    
    answer = (prompt | llm | StrOutputParser()).invoke({
        "question": state["question"],
        "context": state.get("knowledge_base", "ï¼ˆç„¡è³‡è¨Šï¼‰"),
        "sources": sources_text
    })
    
    # è¨ˆç®—æœ€çµ‚ä¿¡å¿ƒåº¦
    confidence = state.get("confidence", 0.5)
    if state.get("planner_analysis"):
        analysis = state["planner_analysis"]
        if "completeness" in analysis and "credibility" in analysis:
            confidence = (analysis["completeness"] + analysis["credibility"]) / 20
    
    # å¯«å…¥å¿«å–
    cache.set(
        question=state["question"],
        answer=answer,
        sources=state.get("sources", []),
        confidence=confidence
    )
    
    print(f"    âœ“ å·²å­˜å…¥å¿«å– (ä¿¡å¿ƒåº¦: {confidence:.0%})")
    
    return {
        "final_answer": answer,
        "confidence": confidence
    }


# ============================================================
# 7. å»ºç«‹ Graph
# ============================================================

workflow = StateGraph(AgentState)

# åŠ å…¥ç¯€é»
workflow.add_node("check_cache", check_cache_node)
workflow.add_node("planner", planner_node)
workflow.add_node("query_gen", query_gen_node)
workflow.add_node("search_tool", search_tool_node)
workflow.add_node("final_answer", final_answer_node)

# è¨­å®šé€²å…¥é»
workflow.set_entry_point("check_cache")


# æ¢ä»¶é‚Šå‡½å¼
def route_cache(state: AgentState) -> str:
    """å¿«å–è·¯ç”±"""
    if state.get("final_answer"):
        return "end"
    return "planner"


def route_planner(state: AgentState) -> str:
    """æ±ºç­–è·¯ç”±"""
    if state.get("is_sufficient"):
        return "final_answer"
    return "query_gen"


# è¨­å®šæ¢ä»¶é‚Š
workflow.add_conditional_edges(
    "check_cache",
    route_cache,
    {"end": END, "planner": "planner"}
)

workflow.add_conditional_edges(
    "planner",
    route_planner,
    {"final_answer": "final_answer", "query_gen": "query_gen"}
)

# æ™®é€šé‚Š
workflow.add_edge("query_gen", "search_tool")
workflow.add_edge("search_tool", "planner")
workflow.add_edge("final_answer", END)

# ç·¨è­¯
app = workflow.compile()


# ============================================================
# 8. è¼”åŠ©å‡½å¼
# ============================================================

def print_banner():
    """å°å‡ºå•Ÿå‹•æ©«å¹…"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         è‡ªå‹•æŸ¥è­‰ AI v1.1 (åŠŸèƒ½æ“´å……ç‰ˆ)                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  åŠŸèƒ½ï¼š                                                    â•‘
â•‘  âœ“ æŒä¹…åŒ–å¿«å– + TTL éæœŸæ©Ÿåˆ¶                               â•‘
â•‘  âœ“ é‡è©¦æ©Ÿåˆ¶ + å®Œå–„éŒ¯èª¤è™•ç†                                 â•‘
â•‘  âœ“ å¤šä¾†æºä¸¦è¡Œ VLM é–±è®€                                     â•‘
â•‘  âœ“ ç´°ç·»çš„ Planner æ±ºç­– (JSON è©•ä¼°)                         â•‘
â•‘  âœ“ æœå°‹å»é‡ + ä¾†æºè¿½è¹¤                                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  æŒ‡ä»¤ï¼š                                                    â•‘
â•‘  q / exit    - é›¢é–‹ç¨‹å¼                                    â•‘
â•‘  /cache      - æŸ¥çœ‹å¿«å–çµ±è¨ˆ                                â•‘
â•‘  /clear      - æ¸…ç†éæœŸå¿«å–                                â•‘
â•‘  /graph      - é¡¯ç¤ºæµç¨‹åœ–                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")


def handle_command(cmd: str) -> bool:
    """è™•ç†ç‰¹æ®ŠæŒ‡ä»¤ï¼Œå›å‚³æ˜¯å¦ç‚ºæŒ‡ä»¤"""
    cmd = cmd.strip().lower()
    
    if cmd == "/cache":
        stats = cache.get_stats()
        print(f"\nğŸ“Š å¿«å–çµ±è¨ˆï¼š")
        print(f"   ç¸½å¿«å–æ•¸ï¼š{stats['total_cached']}")
        print(f"   è¨˜æ†¶é«”ä¸­ï¼š{stats['in_memory']}")
        print(f"   å¿«å–ç›®éŒ„ï¼š{stats['cache_dir']}")
        return True
    
    elif cmd == "/clear":
        cleared = cache.clear_expired()
        print(f"\nğŸ—‘ï¸  å·²æ¸…ç† {cleared} ç­†éæœŸå¿«å–")
        return True
    
    elif cmd == "/graph":
        print("\nğŸ“Š æµç¨‹åœ–ï¼š")
        print(app.get_graph().draw_ascii())
        return True
    
    return False


# ============================================================
# 9. ä¸»ç¨‹å¼
# ============================================================

if __name__ == "__main__":
    print_banner()
    
    # å•Ÿå‹•æ™‚æ¸…ç†éæœŸå¿«å–
    cleared = cache.clear_expired()
    if cleared:
        print(f"ğŸ—‘ï¸  å•Ÿå‹•æ™‚æ¸…ç†äº† {cleared} ç­†éæœŸå¿«å–\n")
    
    print("-" * 60)
    
    while True:
        try:
            q = input("\nè«‹è¼¸å…¥å•é¡Œ: ").strip()
            
            if not q:
                continue
            
            if q.lower() in ["q", "exit", "quit"]:
                print("\nğŸ‘‹ å†è¦‹ï¼")
                break
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºæŒ‡ä»¤
            if q.startswith("/"):
                if handle_command(q):
                    continue
            
            # åŸ·è¡ŒæŸ¥è­‰
            print("\n" + "â”€" * 60)
            
            start_time = time.time()
            result = app.invoke({"question": q})
            elapsed = time.time() - start_time
            
            print("\n" + "â•" * 60)
            print("ğŸ“‹ æœ€çµ‚å›ç­”")
            print("â•" * 60)
            print(result["final_answer"])
            print("â”€" * 60)
            print(f"â±ï¸  è€—æ™‚: {elapsed:.1f} ç§’")
            print(f"ğŸ¯ ä¿¡å¿ƒåº¦: {result.get('confidence', 0):.0%}")
            if result.get("sources"):
                print(f"ğŸ“š ä¾†æºæ•¸: {len(result['sources'])} å€‹")
            print("â•" * 60)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œå†è¦‹ï¼")
            break
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()