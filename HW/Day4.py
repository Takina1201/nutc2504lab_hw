"""
è‡ªå‹•æŸ¥è­‰ AI v1.1 (ç²¾æº–æŸ¥è­‰ç‰ˆ)
========================================

å…¨é¢å‡ç´šå…§å®¹ï¼š
1. ä¾†æºå¯ä¿¡åº¦è©•åˆ†ç³»çµ± â€” å€åˆ†å®˜æ–¹ã€æ–°èã€ç¶­åŸºã€è«–å£‡ç­‰
2. äº¤å‰é©—è­‰æ©Ÿåˆ¶ â€” è¦æ±‚å¤šå€‹ç¨ç«‹ä¾†æºç¢ºèªäº‹å¯¦
3. æ™‚æ•ˆæ€§æª¢æŸ¥ â€” å„ªå…ˆä½¿ç”¨è¼ƒæ–°è³‡è¨Šï¼Œæ¨™è¨»è³‡è¨Šæ—¥æœŸ
4. çŸ›ç›¾åµæ¸¬èˆ‡è§£æ±º â€” ç™¼ç¾çŸ›ç›¾æ™‚è‡ªå‹•æ·±å…¥æœå°‹
5. äº‹å¯¦è²æ˜æå– â€” å°‡è³‡è¨Šæ‹†è§£ç‚ºå¯é©—è­‰çš„è²æ˜
6. å¤šè§’åº¦æœå°‹ç­–ç•¥ â€” è‡ªå‹•ç”Ÿæˆä¸åŒè§’åº¦çš„æœå°‹é—œéµå­—
7. çµæ§‹åŒ–é©—è­‰å ±å‘Š â€” æ¸…æ™°å‘ˆç¾æ¯å€‹äº‹å¯¦çš„é©—è­‰ç‹€æ…‹

å®‰è£éœ€æ±‚ï¼š
pip install langgraph langchain langchain-openai playwright requests aiohttp
python -m playwright install-deps
python -m playwright install chromium

API é…ç½®ï¼š
- LLM: https://ws-03.wade0426.me/v1 (gpt-oss-120b)
- SearXNG: https://puli-8080.huannago.com/search

èª²å¾Œç·´ç¿’è¦å®šï¼š
- å¿…é ˆä½¿ç”¨å„ªåŒ–æ–¹å¼ï¼šâœ“ å¿«å–æ©Ÿåˆ¶
- å¿…è¦ç¯€é»ï¼šâœ“ planner, query_gen, search_tool
"""

import os
import re
import time
import json
import base64
import hashlib
import asyncio
import requests
from typing import TypedDict, List, Optional, Dict, Any, Tuple
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict, field
from urllib.parse import urlparse
from enum import Enum

# Playwright éåŒæ­¥ç‰ˆæœ¬
from playwright.async_api import async_playwright, Browser, Page

# LangChain / LangGraph ç›¸é—œ
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END


# ============================================================
# 1. è¨­å®šèˆ‡åˆå§‹åŒ–
# ============================================================

API_KEY = ""
BASE_URL = "https://ws-03.wade0426.me/v1"
MODEL_NAME = "/models/gpt-oss-120b"
SEARXNG_URL = "https://puli-8080.huannago.com/search"

# å¿«å–è¨­å®šï¼ˆå„ªåŒ–æ–¹å¼ 1ï¼šå¿«å–æ©Ÿåˆ¶ï¼‰
CACHE_DIR = Path("./cache_v3")
CACHE_DIR.mkdir(exist_ok=True)
CACHE_TTL = 43200  # å¿«å–æœ‰æ•ˆæœŸï¼š12 å°æ™‚

# æœå°‹è¨­å®š
MAX_SEARCH_ROUNDS = 4      # æœ€å¤§æœå°‹è¼ªæ•¸
MAX_RETRIES = 3            # API é‡è©¦æ¬¡æ•¸
VLM_READ_COUNT = 3         # æ¯è¼ªè®€å–ç¶²é æ•¸
MIN_SOURCES_FOR_CONFIDENCE = 2  # æœ€å°‘éœ€è¦å¹¾å€‹ä¾†æºæ‰æœ‰ä¿¡å¿ƒ

# åˆå§‹åŒ–å…±ç”¨ LLM
llm = ChatOpenAI(
    base_url=BASE_URL,
    api_key=API_KEY,
    model=MODEL_NAME,
    temperature=0.1
)

# ç”¨æ–¼è¤‡é›œæ¨ç†çš„ LLMï¼ˆæº«åº¦ç¨é«˜ï¼‰
llm_reasoning = ChatOpenAI(
    base_url=BASE_URL,
    api_key=API_KEY,
    model=MODEL_NAME,
    temperature=0.3
)


# ============================================================
# 2. ä¾†æºå¯ä¿¡åº¦è©•åˆ†ç³»çµ±
# ============================================================

class SourceCredibility(Enum):
    """ä¾†æºå¯ä¿¡åº¦ç­‰ç´š"""
    OFFICIAL = 5        # å®˜æ–¹ä¾†æºï¼ˆæ”¿åºœã€å®˜æ–¹ç¶²ç«™ï¼‰
    ACADEMIC = 5        # å­¸è¡“ä¾†æºï¼ˆ.eduã€å­¸è¡“æœŸåˆŠï¼‰
    MAJOR_NEWS = 4      # ä¸»æµåª’é«”ï¼ˆBBCã€Reutersã€è¯åˆå ±ç­‰ï¼‰
    WIKIPEDIA = 3       # ç¶­åŸºç™¾ç§‘ï¼ˆéœ€äº¤å‰é©—è­‰ï¼‰
    TECH_MEDIA = 3      # ç§‘æŠ€åª’é«”ï¼ˆTechCrunchã€æ•¸ä½æ™‚ä»£ç­‰ï¼‰
    LOCAL_NEWS = 3      # åœ°æ–¹æ–°è
    BLOG = 2            # éƒ¨è½æ ¼ã€Medium
    FORUM = 1           # è«–å£‡ã€PTTã€Dcard
    CONTENT_FARM = 0    # å…§å®¹è¾²å ´
    UNKNOWN = 1         # æœªçŸ¥ä¾†æº


# å¯ä¿¡ä¾†æºç¶²åŸŸå°ç…§è¡¨
CREDIBILITY_DOMAINS = {
    # å®˜æ–¹ä¾†æº
    SourceCredibility.OFFICIAL: [
        'gov.tw', 'gov.cn', 'gov.uk', 'gov', 'edu.tw', 'edu',
        'who.int', 'un.org', 'nasa.gov', 'nih.gov',
        'apple.com', 'google.com', 'microsoft.com', 'meta.com',
        'tesla.com', 'openai.com', 'anthropic.com',
    ],
    # å­¸è¡“ä¾†æº
    SourceCredibility.ACADEMIC: [
        'nature.com', 'science.org', 'ieee.org', 'acm.org',
        'arxiv.org', 'pubmed.ncbi.nlm.nih.gov', 'scholar.google',
        'researchgate.net', 'jstor.org',
    ],
    # ä¸»æµåª’é«”
    SourceCredibility.MAJOR_NEWS: [
        'reuters.com', 'apnews.com', 'bbc.com', 'bbc.co.uk',
        'nytimes.com', 'washingtonpost.com', 'theguardian.com',
        'cnn.com', 'bloomberg.com', 'wsj.com', 'economist.com',
        # å°ç£ä¸»æµåª’é«”
        'udn.com', 'ltn.com.tw', 'chinatimes.com', 'cna.com.tw',
        'tvbs.com.tw', 'ettoday.net', 'setn.com',
        # ä¸­åœ‹ä¸»æµåª’é«”
        'xinhuanet.com', 'people.com.cn', 'caixin.com',
        # æ—¥æœ¬
        'nhk.or.jp', 'asahi.com', 'nikkei.com',
    ],
    # ç¶­åŸºç™¾ç§‘
    SourceCredibility.WIKIPEDIA: [
        'wikipedia.org', 'wikimedia.org', 'wikidata.org',
    ],
    # ç§‘æŠ€åª’é«”
    SourceCredibility.TECH_MEDIA: [
        'techcrunch.com', 'theverge.com', 'wired.com', 'arstechnica.com',
        'engadget.com', 'cnet.com', 'zdnet.com', 'venturebeat.com',
        # å°ç£ç§‘æŠ€åª’é«”
        'bnext.com.tw', 'technews.tw', 'ithome.com.tw', 'inside.com.tw',
    ],
    # å…§å®¹è¾²å ´é»‘åå–®
    SourceCredibility.CONTENT_FARM: [
        'kknews.cc', 'read01.com', 'twgreatdaily.com',
        'bomb01.com', 'coco01.today', 'how01.com',
        'ptt01.cc', 'life.tw', 'push01.net',
    ],
}


def get_source_credibility(url: str) -> Tuple[SourceCredibility, int]:
    """
    è©•ä¼°ä¾†æºå¯ä¿¡åº¦
    å›å‚³ï¼š(å¯ä¿¡åº¦ç­‰ç´š, åˆ†æ•¸ 0-5)
    """
    try:
        domain = urlparse(url).netloc.lower()
        if domain.startswith('www.'):
            domain = domain[4:]
    except:
        return SourceCredibility.UNKNOWN, 1
    
    # æª¢æŸ¥å„é¡åˆ¥
    for credibility, domains in CREDIBILITY_DOMAINS.items():
        for d in domains:
            if d in domain:
                return credibility, credibility.value
    
    # ç‰¹æ®Šè¦å‰‡
    if '.gov' in domain or '.edu' in domain:
        return SourceCredibility.OFFICIAL, 5
    if 'wiki' in domain:
        return SourceCredibility.WIKIPEDIA, 3
    if 'news' in domain or 'times' in domain:
        return SourceCredibility.LOCAL_NEWS, 3
    
    return SourceCredibility.UNKNOWN, 1


# ============================================================
# 3. è³‡æ–™çµæ§‹å®šç¾©
# ============================================================

@dataclass
class FactClaim:
    """äº‹å¯¦è²æ˜"""
    claim: str
    sources: List[str] = field(default_factory=list)
    contradicting_sources: List[str] = field(default_factory=list)
    confidence: float = 0.0
    verified: bool = False
    verification_notes: str = ""


@dataclass
class SourceInfo:
    """ä¾†æºè³‡è¨Š"""
    url: str
    title: str
    content: str
    credibility: SourceCredibility
    credibility_score: int
    extracted_date: Optional[str] = None
    extraction_time: float = field(default_factory=time.time)
    success: bool = True
    error: Optional[str] = None
    
    def to_dict(self) -> dict:
        d = asdict(self)
        d['credibility'] = self.credibility.name
        return d


@dataclass
class CacheEntry:
    """å¿«å–é …ç›®çµæ§‹"""
    question: str
    answer: str
    sources: List[str]
    timestamp: float
    confidence: float
    fact_claims: List[dict] = field(default_factory=list)
    
    def is_expired(self, ttl: int = CACHE_TTL) -> bool:
        return time.time() - self.timestamp > ttl
    
    def to_dict(self) -> dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: dict) -> "CacheEntry":
        return cls(**data)


# ============================================================
# 4. æŒä¹…åŒ–å¿«å–ç³»çµ±ï¼ˆå„ªåŒ–æ–¹å¼ï¼‰
# ============================================================

class PersistentCache:
    """æŒä¹…åŒ–å¿«å–ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: Path = CACHE_DIR, ttl: int = CACHE_TTL):
        self.cache_dir = cache_dir
        self.ttl = ttl
        self.cache_dir.mkdir(exist_ok=True)
        self._memory_cache: Dict[str, CacheEntry] = {}
    
    def _get_cache_key(self, question: str) -> str:
        normalized = " ".join(question.strip().split())
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    def _get_cache_path(self, key: str) -> Path:
        return self.cache_dir / f"{key}.json"
    
    def get(self, question: str) -> Optional[CacheEntry]:
        key = self._get_cache_key(question)
        
        if key in self._memory_cache:
            entry = self._memory_cache[key]
            if not entry.is_expired(self.ttl):
                return entry
            else:
                del self._memory_cache[key]
        
        cache_path = self._get_cache_path(key)
        if cache_path.exists():
            try:
                data = json.loads(cache_path.read_text(encoding='utf-8'))
                entry = CacheEntry.from_dict(data)
                if not entry.is_expired(self.ttl):
                    self._memory_cache[key] = entry
                    return entry
                else:
                    cache_path.unlink()
            except (json.JSONDecodeError, KeyError, TypeError) as e:
                print(f"    å¿«å–è®€å–å¤±æ•—: {e}")
                cache_path.unlink(missing_ok=True)
        
        return None
    
    def set(self, question: str, answer: str, sources: List[str], 
            confidence: float = 0.8, fact_claims: List[dict] = None):
        key = self._get_cache_key(question)
        entry = CacheEntry(
            question=question,
            answer=answer,
            sources=sources,
            timestamp=time.time(),
            confidence=confidence,
            fact_claims=fact_claims or []
        )
        
        self._memory_cache[key] = entry
        cache_path = self._get_cache_path(key)
        cache_path.write_text(
            json.dumps(entry.to_dict(), ensure_ascii=False, indent=2),
            encoding='utf-8'
        )
    
    def clear_expired(self) -> int:
        cleared = 0
        for cache_file in self.cache_dir.glob("*.json"):
            try:
                data = json.loads(cache_file.read_text(encoding='utf-8'))
                entry = CacheEntry.from_dict(data)
                if entry.is_expired(self.ttl):
                    cache_file.unlink()
                    cleared += 1
            except:
                cache_file.unlink()
                cleared += 1
        return cleared
    
    def get_stats(self) -> dict:
        total = len(list(self.cache_dir.glob("*.json")))
        memory_count = len(self._memory_cache)
        return {
            "total_cached": total,
            "in_memory": memory_count,
            "cache_dir": str(self.cache_dir),
            "ttl_hours": self.ttl / 3600
        }


# åˆå§‹åŒ–å¿«å–
cache = PersistentCache()


# ============================================================
# 5. æœå°‹å·¥å…·å‡½æ•¸
# ============================================================

def search_searxng(query: str, limit: int = 5, retries: int = MAX_RETRIES) -> List[dict]:
    """
    åŸ·è¡Œ SearXNG æœå°‹ï¼ˆå«é‡è©¦æ©Ÿåˆ¶ï¼‰
    """
    print(f"    ğŸ” æœå°‹: {query}")
    
    params = {
        "q": query,
        "format": "json",
        "language": "zh-TW"
    }
    
    for attempt in range(retries):
        try:
            response = requests.get(SEARXNG_URL, params=params, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                results = []
                seen_urls = set()
                
                for r in data.get('results', []):
                    url = r.get('url', '')
                    if url and url not in seen_urls:
                        # è·³éå…§å®¹è¾²å ´
                        credibility, score = get_source_credibility(url)
                        if credibility == SourceCredibility.CONTENT_FARM:
                            continue
                        
                        seen_urls.add(url)
                        results.append({
                            'title': r.get('title', 'ç„¡æ¨™é¡Œ'),
                            'url': url,
                            'content': r.get('content', ''),
                            'engine': r.get('engine', 'unknown'),
                            'credibility': credibility.name,
                            'credibility_score': score
                        })
                
                # æŒ‰å¯ä¿¡åº¦æ’åº
                results.sort(key=lambda x: x['credibility_score'], reverse=True)
                
                print(f"    âœ“ æ‰¾åˆ° {len(results)} ç­†çµæœï¼ˆå·²éæ¿¾å…§å®¹è¾²å ´ï¼‰")
                return results[:limit]
            
            elif response.status_code == 429:
                wait_time = 2 ** attempt
                print(f"    â³ Rate limitedï¼Œç­‰å¾… {wait_time} ç§’...")
                time.sleep(wait_time)
            else:
                print(f"    âš  HTTP {response.status_code}")
                
        except requests.exceptions.Timeout:
            print(f"    â³ é€¾æ™‚ï¼Œé‡è©¦ {attempt + 1}/{retries}")
            time.sleep(1)
        except requests.exceptions.RequestException as e:
            print(f"    âŒ é€£ç·šéŒ¯èª¤: {e}")
            time.sleep(1)
    
    return []


def generate_multi_angle_queries(question: str, existing_queries: List[str]) -> List[str]:
    """
    ç”Ÿæˆå¤šè§’åº¦æœå°‹é—œéµå­—
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯æœå°‹ç­–ç•¥å°ˆå®¶ã€‚é‡å°ä½¿ç”¨è€…çš„å•é¡Œï¼Œç”Ÿæˆ 3 å€‹ä¸åŒè§’åº¦çš„æœå°‹é—œéµå­—ã€‚

å•é¡Œï¼š{question}

å·²ç¶“æœå°‹éï¼ˆé¿å…é‡è¤‡ï¼‰ï¼š{existing}

ç”Ÿæˆç­–ç•¥ï¼š
1. ç›´æ¥é—œéµå­—ï¼šå•é¡Œçš„æ ¸å¿ƒä¸»é¡Œ
2. é©—è­‰è§’åº¦ï¼šå°‹æ‰¾å®˜æ–¹æˆ–æ¬Šå¨ä¾†æº
3. åé¢é©—è­‰ï¼šæœå°‹å¯èƒ½çš„åé§æˆ–ä¸åŒè§€é»

è«‹ç”¨ JSON é™£åˆ—æ ¼å¼å›å‚³ 3 å€‹é—œéµå­—ï¼Œä¾‹å¦‚ï¼š
["é—œéµå­—1", "é—œéµå­—2", "é—œéµå­—3"]

åªè¼¸å‡º JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚""")
    ])
    
    try:
        response = (prompt | llm | StrOutputParser()).invoke({
            "question": question,
            "existing": ", ".join(existing_queries) if existing_queries else "ï¼ˆç„¡ï¼‰"
        })
        
        response = response.strip()
        if response.startswith("```"):
            response = response.split("\n", 1)[1]
        if response.endswith("```"):
            response = response.rsplit("```", 1)[0]
        
        queries = json.loads(response.strip())
        new_queries = [q for q in queries if q not in existing_queries]
        return new_queries[:3]
        
    except Exception as e:
        print(f"    âš  é—œéµå­—ç”Ÿæˆå¤±æ•—: {e}")
        return [question] if question not in existing_queries else []


def generate_contradiction_queries(
    question: str, 
    contradictions: List[str],
    existing_queries: List[str]
) -> List[str]:
    """
    é‡å°çŸ›ç›¾ç”Ÿæˆé©—è­‰é—œéµå­—
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ éœ€è¦è§£æ±ºæŸ¥è­‰éç¨‹ä¸­ç™¼ç¾çš„çŸ›ç›¾ã€‚

åŸå§‹å•é¡Œï¼š{question}

ç™¼ç¾çš„çŸ›ç›¾ï¼š
{contradictions}

å·²æœå°‹éï¼š{existing}

è«‹ç”Ÿæˆ 2 å€‹é‡å°æ€§çš„æœå°‹é—œéµå­—ï¼Œç”¨æ–¼ï¼š
1. æ‰¾åˆ°æ›´æ¬Šå¨çš„ä¾†æºä¾†ç¢ºèªäº‹å¯¦
2. æ‰¾åˆ°æœ€æ–°çš„è³‡è¨Šä¾†è§£æ±ºçŸ›ç›¾

ç”¨ JSON é™£åˆ—æ ¼å¼å›å‚³ï¼š["é—œéµå­—1", "é—œéµå­—2"]
åªè¼¸å‡º JSONã€‚""")
    ])
    
    try:
        response = (prompt | llm | StrOutputParser()).invoke({
            "question": question,
            "contradictions": "\n".join(contradictions),
            "existing": ", ".join(existing_queries)
        })
        
        response = response.strip()
        if "```" in response:
            response = response.split("```")[1] if "```json" not in response else response.split("```json")[1]
            response = response.split("```")[0]
        
        queries = json.loads(response.strip())
        return [q for q in queries if q not in existing_queries][:2]
    except:
        return []


# ============================================================
# 6. VLM è¦–è¦ºé–±è®€
# ============================================================

async def vlm_read_single_page(
    browser: Browser, 
    url: str, 
    title: str,
    max_screenshots: int = 3
) -> SourceInfo:
    """
    éåŒæ­¥è®€å–å–®ä¸€ç¶²é 
    """
    credibility, score = get_source_credibility(url)
    
    result = SourceInfo(
        url=url,
        title=title,
        content="",
        credibility=credibility,
        credibility_score=score,
        success=False
    )
    
    context = None
    try:
        context = await browser.new_context(
            viewport={'width': 1280, 'height': 1200},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )
        page = await context.new_page()
        
        # è¨­å®šè«‹æ±‚æ””æˆª
        await page.route("**/*", lambda route: (
            route.abort() if any(x in route.request.url for x in [
                'analytics', 'tracking', 'ads', 'doubleclick', 
                'facebook.com/tr', 'googlesyndication', 'adservice'
            ]) else route.continue_()
        ))
        
        await page.goto(url, wait_until="domcontentloaded", timeout=25000)
        await page.wait_for_timeout(2500)
        
        # å˜—è©¦æå–ç™¼å¸ƒæ—¥æœŸ
        extracted_date = await extract_publish_date(page)
        result.extracted_date = extracted_date
        
        # æ³¨å…¥ CSS éš±è—å¹²æ“¾å…ƒç´ 
        await page.add_style_tag(content="""
            iframe, .ad, .ads, .advertisement, 
            [class*="cookie"], [class*="popup"], 
            [class*="modal"], [class*="overlay"],
            [class*="banner"], [id*="banner"],
            header, footer, nav, aside,
            [class*="sidebar"], [class*="related"],
            [class*="recommend"], [class*="comment"] {
                opacity: 0 !important;
                pointer-events: none !important;
            }
        """)
        
        # æ»¾å‹•æˆªåœ–
        screenshots_b64 = []
        for i in range(max_screenshots):
            screenshot = await page.screenshot(type='png')
            b64 = base64.b64encode(screenshot).decode('utf-8')
            screenshots_b64.append(b64)
            await page.evaluate("window.scrollBy(0, 900)")
            await page.wait_for_timeout(600)
        
        # ä½¿ç”¨ VLM åˆ†æ
        if screenshots_b64:
            content = await analyze_screenshots_with_vlm(screenshots_b64, title, credibility.name)
            result.content = content
            result.success = True
            
    except Exception as e:
        result.error = str(e)
        print(f"    âŒ è®€å–å¤±æ•— ({title[:30]}...): {type(e).__name__}")
    finally:
        if context:
            await context.close()
    
    return result


async def extract_publish_date(page: Page) -> Optional[str]:
    """
    å˜—è©¦å¾ç¶²é æå–ç™¼å¸ƒæ—¥æœŸ
    """
    try:
        selectors = [
            'time[datetime]',
            '[class*="date"]',
            '[class*="time"]',
            'meta[property="article:published_time"]',
            'meta[name="publishdate"]',
        ]
        
        for selector in selectors:
            try:
                element = await page.query_selector(selector)
                if element:
                    datetime_attr = await element.get_attribute('datetime')
                    if datetime_attr:
                        return datetime_attr[:10]
                    
                    content_attr = await element.get_attribute('content')
                    if content_attr:
                        return content_attr[:10]
                    
                    text = await element.text_content()
                    if text and len(text) < 50:
                        return text.strip()
            except:
                continue
        
        return None
    except:
        return None


async def analyze_screenshots_with_vlm(
    screenshots_b64: List[str], 
    title: str,
    credibility: str
) -> str:
    """
    VLM åˆ†ææˆªåœ–å…§å®¹
    """
    msg_content = [
        {
            "type": "text",
            "text": f"""é€™æ˜¯ã€Œ{title}ã€ç¶²é çš„æˆªåœ–ï¼ˆä¾†æºå¯ä¿¡åº¦ï¼š{credibility}ï¼‰ã€‚

è«‹ä»”ç´°é–±è®€ä¸¦æå–ä»¥ä¸‹è³‡è¨Šï¼š

1. **æ ¸å¿ƒäº‹å¯¦**ï¼šæ–‡ç« çš„ä¸»è¦é™³è¿°å’Œé—œéµäº‹å¯¦
2. **å…·é«”æ•¸æ“š**ï¼šä»»ä½•æ•¸å­—ã€æ—¥æœŸã€çµ±è¨ˆè³‡æ–™
3. **è³‡è¨Šä¾†æº**ï¼šæ–‡ç« å¼•ç”¨çš„åŸå§‹ä¾†æºï¼ˆå¦‚æœ‰ï¼‰
4. **ç™¼å¸ƒæ™‚é–“**ï¼šæ–‡ç« çš„ç™¼å¸ƒæˆ–æ›´æ–°æ—¥æœŸï¼ˆå¦‚å¯è¦‹ï¼‰
5. **ä½œè€…/æ©Ÿæ§‹**ï¼šæ’°å¯«è€…æˆ–ç™¼å¸ƒæ©Ÿæ§‹

æ³¨æ„äº‹é …ï¼š
- åªæå–äº‹å¯¦æ€§å…§å®¹ï¼Œå¿½ç•¥å»£å‘Šå’Œæ„è¦‹
- å¦‚æœæ˜¯æ–°èï¼Œå€åˆ†äº‹å¯¦å ±å°å’Œè¨˜è€…è©•è«–
- å¦‚æœæœ‰å¼•ç”¨å…¶ä»–ä¾†æºï¼Œè«‹æ¨™è¨»
- ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”

è«‹ä»¥çµæ§‹åŒ–æ–¹å¼å‘ˆç¾æå–çš„è³‡è¨Šã€‚"""
        }
    ]
    
    for img in screenshots_b64:
        msg_content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/png;base64,{img}"}
        })
    
    try:
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: llm.invoke([HumanMessage(content=msg_content)])
        )
        return response.content
    except Exception as e:
        return f"VLM åˆ†æå¤±æ•—: {e}"


async def vlm_read_websites_parallel(
    urls: List[dict],
    max_concurrent: int = 3
) -> List[SourceInfo]:
    """
    ä¸¦è¡Œè®€å–å¤šå€‹ç¶²é 
    """
    if not urls:
        return []
    
    print(f"    ğŸ“– ä¸¦è¡Œé–±è®€ {len(urls)} å€‹ç¶²é ...")
    
    results = []
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--disable-dev-shm-usage",
                "--no-sandbox"
            ]
        )
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def read_with_semaphore(url_info):
            async with semaphore:
                return await vlm_read_single_page(
                    browser,
                    url_info['url'],
                    url_info['title']
                )
        
        tasks = [read_with_semaphore(url_info) for url_info in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        await browser.close()
    
    # è™•ç†çµæœ
    processed_results = []
    for i, r in enumerate(results):
        if isinstance(r, Exception):
            processed_results.append(SourceInfo(
                url=urls[i]['url'] if i < len(urls) else "",
                title=urls[i]['title'] if i < len(urls) else "",
                content=f"è®€å–å¤±æ•—: {r}",
                credibility=SourceCredibility.UNKNOWN,
                credibility_score=0,
                success=False,
                error=str(r)
            ))
        else:
            processed_results.append(r)
    
    success_count = sum(1 for r in processed_results if r.success)
    print(f"    âœ“ å®Œæˆï¼ŒæˆåŠŸ {success_count}/{len(urls)}")
    
    return processed_results


# ============================================================
# 7. äº‹å¯¦æå–èˆ‡é©—è­‰
# ============================================================

def extract_fact_claims(question: str, sources: List[SourceInfo]) -> List[FactClaim]:
    """
    å¾ä¾†æºä¸­æå–å¯é©—è­‰çš„äº‹å¯¦è²æ˜
    """
    if not sources:
        return []
    
    source_texts = []
    for i, s in enumerate(sources):
        if s.success and s.content:
            source_texts.append(f"""
ã€ä¾†æº {i+1}ã€‘{s.title}
å¯ä¿¡åº¦ï¼š{s.credibility.name} ({s.credibility_score}/5)
æ—¥æœŸï¼š{s.extracted_date or 'æœªçŸ¥'}
URLï¼š{s.url}
å…§å®¹æ‘˜è¦ï¼š
{s.content[:2000]}
""")
    
    if not source_texts:
        return []
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯äº‹å¯¦æŸ¥æ ¸å°ˆå®¶ã€‚è«‹å¾æä¾›çš„ä¾†æºä¸­æå–èˆ‡å•é¡Œç›¸é—œçš„ã€Œäº‹å¯¦è²æ˜ã€ã€‚

å•é¡Œï¼š{question}

ä¾†æºè³‡è¨Šï¼š
{sources}

è«‹æå– 3-5 å€‹é—œéµäº‹å¯¦è²æ˜ï¼Œç”¨ JSON æ ¼å¼å›å‚³ï¼š
[
    {{
        "claim": "å…·é«”çš„äº‹å¯¦è²æ˜",
        "supporting_sources": [ä¾†æºç·¨è™Ÿåˆ—è¡¨ï¼Œå¦‚ [1, 3]],
        "contradicting_sources": [å¦‚æœ‰çŸ›ç›¾çš„ä¾†æºç·¨è™Ÿ],
        "confidence": 0.0-1.0 çš„ä¿¡å¿ƒåº¦,
        "notes": "å‚™è¨»ï¼Œå¦‚ä¾†æºé–“çš„å·®ç•°"
    }}
]

è©•ä¼°æ¨™æº–ï¼š
- å¤šå€‹ç¨ç«‹ä¾†æºæ”¯æŒ â†’ é«˜ä¿¡å¿ƒåº¦
- åƒ…å–®ä¸€ä¾†æº â†’ ä¸­ç­‰ä¿¡å¿ƒåº¦
- ä¾†æºé–“æœ‰çŸ›ç›¾ â†’ ä½ä¿¡å¿ƒåº¦ï¼Œéœ€æ¨™è¨»

åªè¼¸å‡º JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚""")
    ])
    
    try:
        response = (prompt | llm_reasoning | StrOutputParser()).invoke({
            "question": question,
            "sources": "\n".join(source_texts)
        })
        
        response = response.strip()
        if response.startswith("```"):
            response = response.split("\n", 1)[1]
        if response.endswith("```"):
            response = response.rsplit("```", 1)[0]
        
        claims_data = json.loads(response.strip())
        
        claims = []
        for c in claims_data:
            supporting = c.get('supporting_sources', [])
            contradicting = c.get('contradicting_sources', [])
            
            claim = FactClaim(
                claim=c.get('claim', ''),
                sources=[sources[i-1].url for i in supporting if 0 < i <= len(sources)],
                contradicting_sources=[sources[i-1].url for i in contradicting if 0 < i <= len(sources)],
                confidence=c.get('confidence', 0.5),
                verified=len(supporting) >= 2,
                verification_notes=c.get('notes', '')
            )
            claims.append(claim)
        
        return claims
        
    except Exception as e:
        print(f"    âš  äº‹å¯¦æå–å¤±æ•—: {e}")
        return []


def detect_contradictions(claims: List[FactClaim]) -> Tuple[bool, List[str]]:
    """
    åµæ¸¬äº‹å¯¦è²æ˜ä¸­çš„çŸ›ç›¾
    """
    contradictions = []
    
    for claim in claims:
        if claim.contradicting_sources:
            contradictions.append(
                f"ã€Œ{claim.claim}ã€- æœ‰ {len(claim.contradicting_sources)} å€‹ä¾†æºæå‡ºä¸åŒèªªæ³•"
            )
        if claim.confidence < 0.5 and claim.verification_notes:
            contradictions.append(
                f"ã€Œ{claim.claim}ã€- {claim.verification_notes}"
            )
    
    return len(contradictions) > 0, contradictions


def calculate_confidence(
    sources: List[dict], 
    claims: List[dict], 
    contradictions: List[str],
    credibility_score: float
) -> float:
    """è¨ˆç®—æ•´é«”ä¿¡å¿ƒåº¦"""
    if not sources:
        return 0.0
    
    # åŸºç¤åˆ†æ•¸ï¼šä¾†æºå¯ä¿¡åº¦ (0-40%)
    base_score = min(credibility_score / 5 * 0.4, 0.4)
    
    # ä¾†æºæ•¸é‡åˆ†æ•¸ (0-20%)
    source_score = min(len(sources) / 5 * 0.2, 0.2)
    
    # äº‹å¯¦é©—è­‰åˆ†æ•¸ (0-30%)
    if claims:
        verified_ratio = sum(1 for c in claims if c.get('verified', False)) / len(claims)
        claim_score = verified_ratio * 0.3
    else:
        claim_score = 0.1
    
    # çŸ›ç›¾æ‡²ç½° (0-10%)
    contradiction_penalty = min(len(contradictions) * 0.05, 0.1)
    
    confidence = base_score + source_score + claim_score - contradiction_penalty
    return max(0.1, min(confidence, 1.0))


# ============================================================
# 8. å®šç¾© State
# ============================================================

class AgentState(TypedDict):
    # åŸºæœ¬æ¬„ä½
    question: str
    knowledge_base: str
    current_queries: List[str]
    search_round: int
    final_answer: str
    is_sufficient: bool
    
    # ä¾†æºè¿½è¹¤
    all_sources: List[dict]
    search_history: List[str]
    
    # äº‹å¯¦é©—è­‰
    fact_claims: List[dict]
    contradictions: List[str]
    
    # è©•ä¼°çµæœ
    confidence: float
    credibility_score: float
    verification_status: str


# ============================================================
# 9. å®šç¾© Nodesï¼ˆç¬¦åˆè¦å®šï¼šplanner, query_gen, search_toolï¼‰
# ============================================================

def check_cache_node(state: AgentState) -> dict:
    """[Node] å¿«å–æª¢æŸ¥ï¼ˆå„ªåŒ–æ©Ÿåˆ¶ï¼‰"""
    question = state["question"]
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ å•é¡Œ: {question}")
    print(f"{'='*60}")
    print(f"\n[Cache] ğŸ—„ï¸ æª¢æŸ¥å¿«å–...")
    
    cached = cache.get(question)
    
    if cached:
        print(f"    âœ“ å‘½ä¸­å¿«å–ï¼(ä¿¡å¿ƒåº¦: {cached.confidence:.0%})")
        return {
            "final_answer": cached.answer,
            "all_sources": [{"url": s} for s in cached.sources],
            "confidence": cached.confidence,
            "fact_claims": cached.fact_claims,
            "is_sufficient": True,
            "verification_status": "cached"
        }
    else:
        print("    âœ— ç„¡å¿«å–ï¼Œé–‹å§‹æŸ¥è­‰æµç¨‹")
        return {
            "knowledge_base": "",
            "search_round": 0,
            "all_sources": [],
            "search_history": [],
            "fact_claims": [],
            "contradictions": [],
            "confidence": 0.0,
            "credibility_score": 0.0,
            "is_sufficient": False,
            "verification_status": "pending"
        }


def planner_node(state: AgentState) -> dict:
    """
    [Node: planner] æ±ºç­–ç¯€é»
    
    è¦å®šè¦æ±‚ï¼šLLM æœƒåˆ¤æ–·ç•¶å‰è’é›†çš„è³‡è¨Šæ˜¯å¦è¶³ä»¥å›æ‡‰ä½¿ç”¨è€…
    """
    print(f"\n[Planner] ğŸ“Š è©•ä¼°æŸ¥è­‰é€²åº¦ (è¼ªæ¬¡ {state.get('search_round', 0)}/{MAX_SEARCH_ROUNDS})...")
    
    search_round = state.get("search_round", 0)
    sources = state.get("all_sources", [])
    claims = state.get("fact_claims", [])
    contradictions = state.get("contradictions", [])
    credibility_score = state.get("credibility_score", 0)
    
    # é”åˆ°æœ€å¤§è¼ªæ•¸
    if search_round >= MAX_SEARCH_ROUNDS:
        print(f"    é”åˆ°æœ€å¤§æœå°‹è¼ªæ•¸ ({MAX_SEARCH_ROUNDS})")
        return {"is_sufficient": True}
    
    # ç¬¬ä¸€è¼ªï¼Œå°šæœªæœå°‹
    if search_round == 0 and not sources:
        print("    é¦–æ¬¡æŸ¥è©¢ï¼Œéœ€è¦æœå°‹è³‡è¨Š")
        return {"is_sufficient": False}
    
    # æ²’æœ‰ä¾†æº
    if not sources:
        print("    ç„¡ä¾†æºï¼Œç¹¼çºŒæœå°‹...")
        return {"is_sufficient": False}
    
    # ä½¿ç”¨ LLM é€²è¡Œæ±ºç­–è©•ä¼°
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€å€‹åš´è¬¹çš„æŸ¥è­‰æ±ºç­–è€…ã€‚è«‹è©•ä¼°ç›®å‰æ”¶é›†çš„è³‡è¨Šæ˜¯å¦è¶³ä»¥å›ç­”å•é¡Œã€‚

ä½¿ç”¨è€…å•é¡Œï¼š{question}

ç›®å‰æ”¶é›†åˆ°çš„è³‡è¨Šæ‘˜è¦ï¼š
- ä¾†æºæ•¸é‡ï¼š{source_count}
- å¹³å‡å¯ä¿¡åº¦ï¼š{credibility}/5
- å·²é©—è­‰äº‹å¯¦æ•¸ï¼š{verified_claims}
- é«˜ä¿¡å¿ƒäº‹å¯¦æ•¸ï¼š{high_confidence_claims}
- ç™¼ç¾çš„çŸ›ç›¾æ•¸ï¼š{contradiction_count}

è³‡è¨Šå…§å®¹é è¦½ï¼š
{knowledge_preview}

è«‹ç”¨ JSON æ ¼å¼å›æ‡‰ï¼š
{{
    "completeness": <1-10 è³‡è¨Šå®Œæ•´åº¦>,
    "credibility": <1-10 ä¾†æºå¯ä¿¡åº¦>,
    "need_more_search": <true/false>,
    "reason": "<ç°¡çŸ­èªªæ˜ç†ç”±>"
}}

è©•ä¼°æ¨™æº–ï¼š
- completeness >= 7 ä¸” credibility >= 6 æ‰ç®—è¶³å¤ 
- å¦‚æœ‰æœªè§£æ±ºçš„çŸ›ç›¾ï¼Œæ‡‰ç¹¼çºŒæœå°‹
- å¦‚æœåªæœ‰å–®ä¸€ä¾†æºï¼Œå¯ä¿¡åº¦æ‡‰é™ä½

åªè¼¸å‡º JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚""")
    ])
    
    source_count = len(sources)
    verified_claims = sum(1 for c in claims if c.get('verified', False))
    high_confidence_claims = sum(1 for c in claims if c.get('confidence', 0) >= 0.7)
    
    try:
        response = (prompt | llm | StrOutputParser()).invoke({
            "question": state["question"],
            "source_count": source_count,
            "credibility": f"{credibility_score:.1f}",
            "verified_claims": verified_claims,
            "high_confidence_claims": high_confidence_claims,
            "contradiction_count": len(contradictions),
            "knowledge_preview": state.get("knowledge_base", "")[:1500]
        })
        
        # è§£æ JSON
        response = response.strip()
        if response.startswith("```"):
            response = response.split("\n", 1)[1]
        if response.endswith("```"):
            response = response.rsplit("```", 1)[0]
        
        analysis = json.loads(response.strip())
        
        completeness = analysis.get("completeness", 0)
        cred = analysis.get("credibility", 0)
        need_more = analysis.get("need_more_search", True)
        reason = analysis.get("reason", "")
        
        print(f"    å®Œæ•´åº¦: {completeness}/10, å¯ä¿¡åº¦: {cred}/10")
        print(f"    ç†ç”±: {reason}")
        
        # åˆ¤æ–·æ˜¯å¦è¶³å¤ 
        is_sufficient = (completeness >= 7 and cred >= 6) and not need_more
        
        # å¦‚æœæœ‰çŸ›ç›¾ä¸”é‚„æœ‰æœå°‹æ¬¡æ•¸ï¼Œç¹¼çºŒæœå°‹
        if contradictions and search_round < MAX_SEARCH_ROUNDS - 1:
            is_sufficient = False
            print("    â†’ éœ€è¦è§£æ±ºçŸ›ç›¾ï¼Œç¹¼çºŒæœå°‹")
        
        # è¨ˆç®—ä¿¡å¿ƒåº¦
        confidence = calculate_confidence(sources, claims, contradictions, credibility_score)
        
        if is_sufficient:
            print(f"    âœ“ è³‡è¨Šè¶³å¤ ï¼Œæº–å‚™ç”Ÿæˆå ±å‘Š (ä¿¡å¿ƒåº¦: {confidence:.0%})")
        else:
            print(f"    âœ— è³‡è¨Šä¸è¶³ï¼Œç¹¼çºŒæœå°‹ (ç•¶å‰ä¿¡å¿ƒåº¦: {confidence:.0%})")
        
        return {
            "is_sufficient": is_sufficient,
            "confidence": confidence
        }
        
    except Exception as e:
        print(f"    âš  æ±ºç­–åˆ†æå¤±æ•—: {e}ï¼Œä½¿ç”¨é è¨­é‚è¼¯")
        
        # é è¨­é‚è¼¯
        is_sufficient = False
        if source_count >= MIN_SOURCES_FOR_CONFIDENCE and credibility_score >= 3:
            if not contradictions or search_round >= 2:
                is_sufficient = True
        
        confidence = calculate_confidence(sources, claims, contradictions, credibility_score)
        
        return {
            "is_sufficient": is_sufficient,
            "confidence": confidence
        }


def query_gen_node(state: AgentState) -> dict:
    """
    [Node: query_gen] é—œéµå­—ç”Ÿæˆç¯€é»
    
    è¦å®šè¦æ±‚ï¼šLLM æœƒåˆ†æä½¿ç”¨è€…çš„å•é¡Œç”Ÿæˆæª¢ç´¢é—œéµå­—
    """
    print(f"\n[Query Gen] ğŸ¯ ç”Ÿæˆæœå°‹é—œéµå­—...")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰çŸ›ç›¾éœ€è¦è§£æ±º
    if state.get("contradictions"):
        print("    ç™¼ç¾çŸ›ç›¾ï¼Œç”Ÿæˆé©—è­‰é—œéµå­—...")
        contradiction_queries = generate_contradiction_queries(
            state["question"],
            state["contradictions"],
            state.get("search_history", [])
        )
        if contradiction_queries:
            for q in contradiction_queries:
                print(f"      â€¢ {q}")
            return {"current_queries": contradiction_queries}
    
    # ç”Ÿæˆå¤šè§’åº¦æœå°‹é—œéµå­—
    queries = generate_multi_angle_queries(
        state["question"],
        state.get("search_history", [])
    )
    
    if not queries:
        if state["question"] not in state.get("search_history", []):
            queries = [state["question"]]
        else:
            queries = []
    
    print(f"    ç”Ÿæˆ {len(queries)} å€‹æœå°‹é—œéµå­—ï¼š")
    for q in queries:
        print(f"      â€¢ {q}")
    
    return {"current_queries": queries}


def search_tool_node(state: AgentState) -> dict:
    """
    [Node: search_tool] æœå°‹å·¥å…·ç¯€é»
    
    è¦å®šè¦æ±‚ï¼šæª¢ç´¢+æ–‡å­—è™•ç†
    """
    print(f"\n[Search Tool] ğŸ” åŸ·è¡Œæœå°‹èˆ‡ VLM é–±è®€...")
    
    queries = state.get("current_queries", [])
    if not queries:
        print("    âš  ç„¡æœå°‹é—œéµå­—")
        return {
            "search_round": state["search_round"] + 1
        }
    
    all_search_results = []
    new_history = list(state.get("search_history", []))
    
    # åŸ·è¡Œæ‰€æœ‰æœå°‹
    for query in queries:
        if query in new_history:
            continue
        results = search_searxng(query, limit=4)
        all_search_results.extend(results)
        new_history.append(query)
    
    if not all_search_results:
        print("    âš  æœªæ‰¾åˆ°ä»»ä½•çµæœ")
        return {
            "search_history": new_history,
            "search_round": state["search_round"] + 1
        }
    
    # å»é‡ä¸¦æŒ‰å¯ä¿¡åº¦æ’åº
    seen_urls = set(s.get('url', '') for s in state.get("all_sources", []))
    unique_results = []
    for r in all_search_results:
        if r['url'] not in seen_urls:
            seen_urls.add(r['url'])
            unique_results.append(r)
    
    unique_results.sort(key=lambda x: x.get('credibility_score', 0), reverse=True)
    
    # é¸å–è¦è®€å–çš„ç¶²é 
    urls_to_read = unique_results[:VLM_READ_COUNT]
    
    print(f"    é¸å– {len(urls_to_read)} å€‹ç¶²é é€²è¡Œæ·±åº¦é–±è®€ï¼š")
    for u in urls_to_read:
        print(f"      â€¢ [{u.get('credibility', '?')}] {u['title'][:40]}...")
    
    # ä¸¦è¡Œ VLM é–±è®€
    vlm_results = asyncio.run(vlm_read_websites_parallel(urls_to_read))
    
    # æ›´æ–°çŸ¥è­˜åº«
    new_kb = state.get("knowledge_base", "")
    new_sources = list(state.get("all_sources", []))
    
    for source in vlm_results:
        if source.success and source.content:
            new_kb += f"""

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“° {source.title}
ğŸ”— {source.url}
ğŸ“Š å¯ä¿¡åº¦: {source.credibility.name} ({source.credibility_score}/5)
ğŸ“… æ—¥æœŸ: {source.extracted_date or 'æœªçŸ¥'}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{source.content}
"""
            new_sources.append(source.to_dict())
    
    # è¨ˆç®—å¹³å‡å¯ä¿¡åº¦åˆ†æ•¸
    if new_sources:
        avg_credibility = sum(s.get('credibility_score', 0) for s in new_sources) / len(new_sources)
    else:
        avg_credibility = 0
    
    return {
        "knowledge_base": new_kb,
        "all_sources": new_sources,
        "search_history": new_history,
        "search_round": state["search_round"] + 1,
        "credibility_score": avg_credibility
    }


def fact_extraction_node(state: AgentState) -> dict:
    """[Node] äº‹å¯¦æå–èˆ‡çŸ›ç›¾åµæ¸¬"""
    print(f"\n[Fact Extraction] ğŸ”¬ æå–äº‹å¯¦è²æ˜...")
    
    # å°‡ dict è½‰æ›ç‚º SourceInfo
    sources = []
    for s in state.get("all_sources", []):
        try:
            source = SourceInfo(
                url=s.get('url', ''),
                title=s.get('title', ''),
                content=s.get('content', ''),
                credibility=SourceCredibility[s.get('credibility', 'UNKNOWN')],
                credibility_score=s.get('credibility_score', 0),
                extracted_date=s.get('extracted_date'),
                success=s.get('success', True)
            )
            sources.append(source)
        except:
            continue
    
    if not sources:
        return {
            "fact_claims": [],
            "contradictions": []
        }
    
    # æå–äº‹å¯¦è²æ˜
    claims = extract_fact_claims(state["question"], sources)
    
    # åµæ¸¬çŸ›ç›¾
    has_contradictions, contradiction_list = detect_contradictions(claims)
    
    print(f"    æå–äº† {len(claims)} å€‹äº‹å¯¦è²æ˜")
    for c in claims:
        status = "âœ“" if c.verified else "?"
        print(f"      {status} {c.claim[:50]}... (ä¿¡å¿ƒåº¦: {c.confidence:.0%})")
    
    if has_contradictions:
        print(f"    âš  ç™¼ç¾ {len(contradiction_list)} å€‹çŸ›ç›¾")
        for cont in contradiction_list:
            print(f"      â€¢ {cont[:60]}...")
    
    return {
        "fact_claims": [asdict(c) for c in claims],
        "contradictions": contradiction_list
    }


def final_answer_node(state: AgentState) -> dict:
    """[Node] ç”¢ç”Ÿæœ€çµ‚å ±å‘Š"""
    print(f"\n[Final Answer] ğŸ“ ç”ŸæˆæŸ¥è­‰å ±å‘Š...")
    
    claims = state.get("fact_claims", [])
    sources = state.get("all_sources", [])
    contradictions = state.get("contradictions", [])
    
    # æº–å‚™ä¾†æºåˆ—è¡¨
    source_list = []
    for s in sources:
        cred = s.get('credibility', 'UNKNOWN')
        score = s.get('credibility_score', 0)
        date = s.get('extracted_date', 'æœªçŸ¥')
        source_list.append(f"â€¢ [{cred} {score}/5] {s.get('title', 'æœªçŸ¥')} ({date})\n  {s.get('url', '')}")
    
    # æº–å‚™äº‹å¯¦è²æ˜æ‘˜è¦
    claims_summary = []
    for c in claims:
        status = "âœ… å·²é©—è­‰" if c.get('verified') else "âš ï¸ å¾…ç¢ºèª"
        conf = c.get('confidence', 0)
        claims_summary.append(f"{status} ({conf:.0%}) {c.get('claim', '')}")
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯å°ˆæ¥­çš„äº‹å¯¦æŸ¥æ ¸å ±å‘Šæ’°å¯«è€…ã€‚è«‹æ ¹æ“šæŸ¥è­‰çµæœæ’°å¯«å ±å‘Šã€‚

å•é¡Œï¼š{question}

å·²æ”¶é›†è³‡è¨Šï¼š
{knowledge}

äº‹å¯¦è²æ˜é©—è­‰çµæœï¼š
{claims}

ç™¼ç¾çš„çŸ›ç›¾æˆ–ä¸ä¸€è‡´ï¼š
{contradictions}

åƒè€ƒä¾†æºï¼š
{sources}

è«‹æ’°å¯«ä¸€ä»½å®Œæ•´çš„æŸ¥è­‰å ±å‘Šï¼ŒåŒ…å«ï¼š

## æŸ¥è­‰çµè«–
ï¼ˆä¸€å¥è©±ç¸½çµæŸ¥è­‰çµæœï¼‰

## é—œéµç™¼ç¾
ï¼ˆåˆ—å‡ºæœ€é‡è¦çš„äº‹å¯¦ï¼Œæ¨™è¨»ä¿¡å¿ƒåº¦ï¼‰

## è©³ç´°åˆ†æ
ï¼ˆè§£é‡‹æŸ¥è­‰éç¨‹å’Œæ¨ç†é‚è¼¯ï¼‰

## ä¾†æºè©•ä¼°
ï¼ˆè©•ä¼°ä¾†æºçš„å¯ä¿¡åº¦å’Œä¸€è‡´æ€§ï¼‰

## æ³¨æ„äº‹é …
ï¼ˆå¦‚æœ‰çŸ›ç›¾æˆ–ä¸ç¢ºå®šä¹‹è™•ï¼Œè«‹èªªæ˜ï¼‰

## åƒè€ƒä¾†æº
ï¼ˆåˆ—å‡ºä¸»è¦ä¾†æºï¼‰

è«‹ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ï¼Œä¿æŒå®¢è§€ä¸­ç«‹ã€‚""")
    ])
    
    answer = (prompt | llm_reasoning | StrOutputParser()).invoke({
        "question": state["question"],
        "knowledge": state.get("knowledge_base", "ï¼ˆç„¡è³‡è¨Šï¼‰")[:8000],
        "claims": "\n".join(claims_summary) if claims_summary else "ï¼ˆæœªæå–äº‹å¯¦è²æ˜ï¼‰",
        "contradictions": "\n".join(contradictions) if contradictions else "ï¼ˆç„¡çŸ›ç›¾ï¼‰",
        "sources": "\n".join(source_list) if source_list else "ï¼ˆç„¡ä¾†æºï¼‰"
    })
    
    # è¨ˆç®—æœ€çµ‚ä¿¡å¿ƒåº¦
    confidence = state.get("confidence", 0.5)
    
    # æ±ºå®šé©—è­‰ç‹€æ…‹
    if confidence >= 0.7:
        verification_status = "verified"
    elif confidence >= 0.5:
        verification_status = "partially_verified"
    else:
        verification_status = "unverified"
    
    # å¯«å…¥å¿«å–
    source_urls = [s.get('url', '') for s in sources if s.get('url')]
    cache.set(
        question=state["question"],
        answer=answer,
        sources=source_urls,
        confidence=confidence,
        fact_claims=claims
    )
    
    print(f"    âœ“ å ±å‘Šå·²ç”Ÿæˆä¸¦å­˜å…¥å¿«å–")
    
    return {
        "final_answer": answer,
        "confidence": confidence,
        "verification_status": verification_status
    }


# ============================================================
# 10. å»ºç«‹ Graphï¼ˆç¬¦åˆè¦å®šçš„ç¯€é»åç¨±ï¼‰
# ============================================================

workflow = StateGraph(AgentState)

# åŠ å…¥ç¯€é»ï¼ˆç¬¦åˆè¦å®šï¼šplanner, query_gen, search_toolï¼‰
workflow.add_node("check_cache", check_cache_node)      # å„ªåŒ–ï¼šå¿«å–æ©Ÿåˆ¶
workflow.add_node("planner", planner_node)              # è¦å®šç¯€é»ï¼šæ±ºç­–
workflow.add_node("query_gen", query_gen_node)          # è¦å®šç¯€é»ï¼šé—œéµå­—ç”Ÿæˆ
workflow.add_node("search_tool", search_tool_node)      # è¦å®šç¯€é»ï¼šæœå°‹å·¥å…·
workflow.add_node("fact_extraction", fact_extraction_node)  # é¡å¤–ï¼šäº‹å¯¦æå–
workflow.add_node("final_answer", final_answer_node)    # æœ€çµ‚ç­”æ¡ˆ

# è¨­å®šé€²å…¥é»
workflow.set_entry_point("check_cache")


# æ¢ä»¶é‚Šå‡½å¼
def route_cache(state: AgentState) -> str:
    if state.get("final_answer"):
        return "end"
    return "planner"


def route_planner(state: AgentState) -> str:
    if state.get("is_sufficient"):
        return "final_answer"
    return "query_gen"


# è¨­å®šæ¢ä»¶é‚Š
workflow.add_conditional_edges(
    "check_cache",
    route_cache,
    {"end": END, "planner": "planner"}
)

workflow.add_conditional_edges(
    "planner",
    route_planner,
    {"final_answer": "final_answer", "query_gen": "query_gen"}
)

# æ™®é€šé‚Š
workflow.add_edge("query_gen", "search_tool")
workflow.add_edge("search_tool", "fact_extraction")
workflow.add_edge("fact_extraction", "planner")
workflow.add_edge("final_answer", END)

# ç·¨è­¯
app = workflow.compile()


# ============================================================
# 11. è¼”åŠ©å‡½å¼èˆ‡ä¸»ç¨‹å¼
# ============================================================

def print_banner():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           ğŸ” è‡ªå‹•æŸ¥è­‰ AI v1.1 (ç²¾æº–æŸ¥è­‰ç‰ˆ)                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ç¬¦åˆèª²å¾Œç·´ç¿’è¦å®šï¼š                                            â•‘
â•‘  âœ“ å„ªåŒ–æ–¹å¼ï¼šå¿«å–æ©Ÿåˆ¶ï¼ˆcheck_cache ç¯€é»ï¼‰                       â•‘
â•‘  âœ“ å¿…è¦ç¯€é»ï¼šplanner, query_gen, search_tool                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  å…¨æ–°åŠŸèƒ½ï¼š                                                    â•‘
â•‘  âœ“ ä¾†æºå¯ä¿¡åº¦è©•åˆ†ï¼ˆå®˜æ–¹/å­¸è¡“/æ–°è/è«–å£‡...ï¼‰                     â•‘
â•‘  âœ“ äº¤å‰é©—è­‰æ©Ÿåˆ¶ï¼ˆå¤šä¾†æºç¢ºèªäº‹å¯¦ï¼‰                               â•‘
â•‘  âœ“ æ™‚æ•ˆæ€§æª¢æŸ¥ï¼ˆæå–ç™¼å¸ƒæ—¥æœŸï¼‰                                   â•‘
â•‘  âœ“ çŸ›ç›¾åµæ¸¬èˆ‡è§£æ±ºï¼ˆè‡ªå‹•æ·±å…¥æœå°‹ï¼‰                               â•‘
â•‘  âœ“ äº‹å¯¦è²æ˜æå–ï¼ˆçµæ§‹åŒ–é©—è­‰ï¼‰                                   â•‘
â•‘  âœ“ å¤šè§’åº¦æœå°‹ç­–ç•¥ï¼ˆç›´æ¥/é©—è­‰/åé¢ï¼‰                             â•‘
â•‘  âœ“ å…§å®¹è¾²å ´éæ¿¾ï¼ˆè‡ªå‹•æ’é™¤ä½å“è³ªä¾†æºï¼‰                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  æŒ‡ä»¤ï¼š                                                        â•‘
â•‘  q / exit    - é›¢é–‹ç¨‹å¼                                        â•‘
â•‘  /cache      - æŸ¥çœ‹å¿«å–çµ±è¨ˆ                                    â•‘
â•‘  /clear      - æ¸…ç†éæœŸå¿«å–                                    â•‘
â•‘  /graph      - é¡¯ç¤ºæµç¨‹åœ–                                      â•‘
â•‘  /domains    - é¡¯ç¤ºå¯ä¿¡ä¾†æºç¶²åŸŸ                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")


def handle_command(cmd: str) -> bool:
    cmd = cmd.strip().lower()
    
    if cmd == "/cache":
        stats = cache.get_stats()
        print(f"\nğŸ“Š å¿«å–çµ±è¨ˆï¼š")
        print(f"   ç¸½å¿«å–æ•¸ï¼š{stats['total_cached']}")
        print(f"   è¨˜æ†¶é«”ä¸­ï¼š{stats['in_memory']}")
        print(f"   å¿«å–ç›®éŒ„ï¼š{stats['cache_dir']}")
        print(f"   æœ‰æ•ˆæœŸï¼š{stats['ttl_hours']:.1f} å°æ™‚")
        return True
    
    elif cmd == "/clear":
        cleared = cache.clear_expired()
        print(f"\nğŸ—‘ï¸  å·²æ¸…ç† {cleared} ç­†éæœŸå¿«å–")
        return True
    
    elif cmd == "/graph":
        print("\nğŸ“Š æµç¨‹åœ–ï¼š")
        print(app.get_graph().draw_ascii())
        return True
    
    elif cmd == "/domains":
        print("\nğŸ“‹ å¯ä¿¡ä¾†æºç¶²åŸŸå°ç…§è¡¨ï¼š")
        for cred, domains in CREDIBILITY_DOMAINS.items():
            if cred != SourceCredibility.CONTENT_FARM:
                print(f"\n  ã€{cred.name}ã€‘(åˆ†æ•¸: {cred.value}/5)")
                for d in domains[:5]:
                    print(f"    â€¢ {d}")
                if len(domains) > 5:
                    print(f"    ... ç­‰å…± {len(domains)} å€‹")
        
        print(f"\n  ã€å…§å®¹è¾²å ´é»‘åå–®ã€‘(è‡ªå‹•éæ¿¾)")
        for d in CREDIBILITY_DOMAINS[SourceCredibility.CONTENT_FARM][:5]:
            print(f"    â€¢ {d}")
        return True
    
    return False


def format_verification_status(status: str) -> str:
    """æ ¼å¼åŒ–é©—è­‰ç‹€æ…‹"""
    status_map = {
        "verified": "âœ… å·²é©—è­‰",
        "partially_verified": "âš ï¸ éƒ¨åˆ†é©—è­‰",
        "unverified": "â“ æœªé©—è­‰",
        "cached": "ğŸ“¦ ä¾†è‡ªå¿«å–"
    }
    return status_map.get(status, status)


if __name__ == "__main__":
    print_banner()
    
    # å•Ÿå‹•æ™‚æ¸…ç†éæœŸå¿«å–
    cleared = cache.clear_expired()
    if cleared:
        print(f"ğŸ—‘ï¸  å•Ÿå‹•æ™‚æ¸…ç†äº† {cleared} ç­†éæœŸå¿«å–\n")
    
    print("-" * 65)
    
    while True:
        try:
            q = input("\nè«‹è¼¸å…¥è¦æŸ¥è­‰çš„å•é¡Œ: ").strip()
            
            if not q:
                continue
            
            if q.lower() in ["q", "exit", "quit"]:
                print("\nğŸ‘‹ å†è¦‹ï¼")
                break
            
            if q.startswith("/"):
                if handle_command(q):
                    continue
            
            # åŸ·è¡ŒæŸ¥è­‰
            start_time = time.time()
            result = app.invoke({"question": q})
            elapsed = time.time() - start_time
            
            # é¡¯ç¤ºçµæœ
            print("\n" + "â•" * 65)
            print("ğŸ“‹ æŸ¥è­‰å ±å‘Š")
            print("â•" * 65)
            print(result["final_answer"])
            print("â•" * 65)
            
            # é¡¯ç¤ºçµ±è¨ˆ
            print(f"\nğŸ“Š æŸ¥è­‰çµ±è¨ˆ")
            print(f"   â±ï¸  è€—æ™‚: {elapsed:.1f} ç§’")
            print(f"   ğŸ¯ ä¿¡å¿ƒåº¦: {result.get('confidence', 0):.0%}")
            print(f"   ğŸ“Œ ç‹€æ…‹: {format_verification_status(result.get('verification_status', 'unknown'))}")
            
            sources = result.get("all_sources", [])
            if sources:
                print(f"   ğŸ“š ä¾†æºæ•¸: {len(sources)} å€‹")
                avg_cred = sum(s.get('credibility_score', 0) for s in sources) / len(sources)
                print(f"   â­ å¹³å‡å¯ä¿¡åº¦: {avg_cred:.1f}/5")
            
            claims = result.get("fact_claims", [])
            if claims:
                verified = sum(1 for c in claims if c.get('verified'))
                print(f"   âœ“ å·²é©—è­‰äº‹å¯¦: {verified}/{len(claims)}")
            
            print("â•" * 65)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œå†è¦‹ï¼")
            break
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()