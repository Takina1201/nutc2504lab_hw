"""
HW Day6ï¼šRAG AI å®¢æœåŠ©æ‰‹ + DeepEval è©•ä¼°
==========================================
å°ç£è‡ªä¾†æ°´å…¬å¸ QA ç³»çµ±
æµç¨‹ï¼š
  1. qa_data.txt â†’ æ»‘å‹•è¦–çª—åˆ‡å¡Š â†’ Qdrant VDB + BM25 ç´¢å¼•
  2. Hybrid Searchï¼ˆDense + BM25ï¼‰ â†’ RRF èåˆ â†’ ReRank â†’ LLM ç­”æ¡ˆ
  3. DeepEval è©•ä¼° 5 é …æŒ‡æ¨™
  4. è¼¸å‡º day6_HW_questions.csv
"""

import os
import re
import csv
import json
import time
import math
import random
import asyncio
import subprocess
import requests
import numpy as np
import jieba
import openpyxl
from rank_bm25 import BM25Okapi
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# DeepEval
from openai import OpenAI
from deepeval.models import DeepEvalBaseLLM
from deepeval.metrics import (
    FaithfulnessMetric,
    AnswerRelevancyMetric,
    ContextualRecallMetric,
    ContextualPrecisionMetric,
    ContextualRelevancyMetric,
)
from deepeval.test_case import LLMTestCase


# ============================================================
# è¨­å®š
# ============================================================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# API
EMBED_API_URL = "https://ws-04.wade0426.me/embed"
LLM_API_URL = "https://ws-05.huannago.com/v1"
LLM_MODEL = "google/gemma-3-27b-it"

# åˆ‡å¡Šåƒæ•¸
CHUNK_SIZE = 500
CHUNK_OVERLAP = 100

# æª¢ç´¢åƒæ•¸
DENSE_TOP_K = 10
BM25_TOP_K = 10
HYBRID_TOP_K = 10
RERANK_TOP_K = 3
RRF_K = 60

COLLECTION_NAME = "day6_water_qa"

# æŠ½æ¨£è¨­å®šï¼ˆå¾ 30 é¡Œä¸­éš¨æ©ŸæŠ½ N é¡Œï¼Œè¨­ç‚º 0 æˆ– 30 å‰‡å…¨éƒ¨è·‘ï¼‰
SAMPLE_N = 5


# ============================================================
# DeepEval è‡ªè¨‚ LLM
# ============================================================
class CustomLLM(DeepEvalBaseLLM):
    """ä½¿ç”¨èª²ç¨‹æä¾›çš„ LLM API ä½œç‚º DeepEval è©•ä¼°æ¨¡å‹ï¼ˆå«é‡è©¦æ©Ÿåˆ¶ï¼‰"""

    def __init__(self, base_url=LLM_API_URL, model_name=LLM_MODEL):
        self.base_url = base_url
        self.model_name = model_name
        self.max_retries = 5          # æœ€å¤šé‡è©¦ 5 æ¬¡
        self.base_delay = 10          # åŸºç¤ç­‰å¾… 10 ç§’
        self.call_count = 0           # è¿½è¹¤å‘¼å«æ¬¡æ•¸

    def load_model(self):
        return OpenAI(api_key="NoNeed", base_url=self.base_url)

    def generate(self, prompt: str) -> str:
        self.call_count += 1
        client = self.load_model()

        for attempt in range(self.max_retries):
            try:
                response = client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.1,
                    max_tokens=2048,
                    timeout=180,  # 3 åˆ†é˜è¶…æ™‚
                )
                content = response.choices[0].message.content

                # æª¢æŸ¥æ˜¯å¦å›å‚³äº† HTML éŒ¯èª¤é é¢ï¼ˆ524 timeout ç­‰ï¼‰
                if content and "<html" in content.lower()[:100]:
                    raise Exception("API å›å‚³ HTML éŒ¯èª¤é é¢ï¼ˆå¯èƒ½æ˜¯ 524 Timeoutï¼‰")

                # æ¯ 5 æ¬¡å‘¼å«æš«åœ 2 ç§’ï¼Œé¿å…éåº¦å£“åŠ›
                if self.call_count % 5 == 0:
                    time.sleep(2)

                return content or ""

            except Exception as e:
                err_msg = str(e)[:120]
                # åˆ¤æ–·æ˜¯å¦ç‚º HTML éŒ¯èª¤ï¼ˆ524 Timeoutï¼‰
                if "<html" in err_msg.lower() or "524" in err_msg or "timeout" in err_msg.lower():
                    wait = self.base_delay * (2 ** attempt)  # 10, 20, 40, 80, 160 ç§’
                    print(f"  â³ API Timeoutï¼ˆç¬¬ {attempt+1}/{self.max_retries} æ¬¡ï¼‰ï¼Œç­‰å¾… {wait} ç§’...")
                    time.sleep(wait)
                else:
                    wait = self.base_delay * (attempt + 1)
                    print(f"  âš ï¸ LLM errorï¼ˆç¬¬ {attempt+1} æ¬¡ï¼‰: {err_msg}ï¼Œç­‰å¾… {wait} ç§’...")
                    time.sleep(wait)

        print(f"  âŒ LLM é‡è©¦ {self.max_retries} æ¬¡å¾Œä»å¤±æ•—")
        return ""

    async def a_generate(self, prompt: str) -> str:
        return self.generate(prompt)

    def get_model_name(self):
        return f"CustomLLM ({self.model_name})"


# ============================================================
# å·¥å…·å‡½æ•¸
# ============================================================
def get_embedding(texts: list[str]) -> tuple:
    """Embedding API"""
    data = {"texts": texts, "normalize": True, "batch_size": 32}
    for attempt in range(3):
        try:
            resp = requests.post(EMBED_API_URL, json=data, timeout=120)
            if resp.status_code == 200:
                result = resp.json()
                return result["embeddings"], result["dimension"]
            print(f"  âš ï¸ Embedding API {resp.status_code}, retry {attempt+1}")
        except Exception as e:
            print(f"  âš ï¸ Embedding error: {e}, retry {attempt+1}")
        time.sleep(2)
    return None, None


def call_llm(system_prompt: str, user_prompt: str,
             temperature: float = 0.1, max_tokens: int = 1024) -> str:
    """LLM API"""
    payload = {
        "model": LLM_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    for attempt in range(3):
        try:
            resp = requests.post(f"{LLM_API_URL}/chat/completions",
                                 json=payload, timeout=120)
            if resp.status_code == 200:
                return resp.json()["choices"][0]["message"]["content"].strip()
            print(f"  âš ï¸ LLM {resp.status_code}, retry {attempt+1}")
        except Exception as e:
            print(f"  âš ï¸ LLM error: {e}, retry {attempt+1}")
        time.sleep(2)
    return ""


# ============================================================
# æ–‡æœ¬åˆ‡å¡Š
# ============================================================
def chunk_qa_data(text: str) -> list[dict]:
    """å°‡ QA è³‡æ–™åˆ‡å¡Š"""
    splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n\n", "\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", ""],
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
    )
    chunks = splitter.split_text(text)
    result = []
    for c in chunks:
        result.append({"text": c, "source": "qa_data.txt"})
    return result


# ============================================================
# BM25 ç´¢å¼•
# ============================================================
def tokenize_chinese(text: str) -> list[str]:
    """ä¸­æ–‡åˆ†è©"""
    words = jieba.lcut(text)
    stop_words = {"çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±",
                  "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€å€‹", "ä¸Š", "ä¹Ÿ", "å¾ˆ",
                  "åˆ°", "èªª", "è¦", "å»", "ä½ ", "æœƒ", "è‘—", "æ²’æœ‰",
                  "çœ‹", "å¥½", "è‡ªå·±", "é€™", "ä»–", "å¥¹", "å®ƒ", "å€‘",
                  "é‚£", "è¢«", "å¾", "å°", "ç‚º", "èˆ‡", "ç­‰", "ä½†",
                  "è€Œ", "åŠ", "æˆ–", "ä¹‹", "å…¶", "ä¸­", "æ‰€", "ä»¥",
                  "å¯", "èƒ½", "å°‡", "é‚„", "å› ", "æ­¤", "å‰‡", "å¦‚",
                  "æ–¼", "å€‹", "æ¯", "åˆ", "æŠŠ", "è®“", "ç”¨", "åš",
                  "å—", "å‘¢", "å•Š", "å§", "å–”", "å‘€", "è€¶"}
    return [w for w in words if len(w) > 1 and w not in stop_words]


class BM25Index:
    def __init__(self, chunks):
        self.chunks = chunks
        self.tokenized = [tokenize_chinese(c["text"]) for c in chunks]
        self.bm25 = BM25Okapi(self.tokenized)

    def search(self, query, top_k=BM25_TOP_K):
        tokens = tokenize_chinese(query)
        scores = self.bm25.get_scores(tokens)
        top_idx = np.argsort(scores)[::-1][:top_k]
        results = []
        for idx in top_idx:
            if scores[idx] > 0:
                results.append({
                    "index": int(idx),
                    "text": self.chunks[idx]["text"],
                    "source": self.chunks[idx]["source"],
                    "bm25_score": float(scores[idx]),
                })
        return results


# ============================================================
# Hybrid Search + RRF
# ============================================================
def rrf_fusion(dense_results, bm25_results, k=RRF_K, top_k=HYBRID_TOP_K):
    rrf_scores = {}
    doc_info = {}
    for rank, hit in enumerate(dense_results):
        idx = hit["index"]
        rrf_scores[idx] = rrf_scores.get(idx, 0) + 1.0 / (k + rank + 1)
        doc_info[idx] = {"text": hit["text"], "source": hit["source"]}
    for rank, hit in enumerate(bm25_results):
        idx = hit["index"]
        rrf_scores[idx] = rrf_scores.get(idx, 0) + 1.0 / (k + rank + 1)
        doc_info[idx] = {"text": hit["text"], "source": hit["source"]}
    sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)
    return [{"index": idx, "text": doc_info[idx]["text"],
             "source": doc_info[idx]["source"], "rrf_score": sc}
            for idx, sc in sorted_docs[:top_k]]


# ============================================================
# ReRank
# ============================================================
RERANK_SYSTEM = """ä½ æ˜¯ä¸€å€‹æ–‡ä»¶ç›¸é—œæ€§è©•ä¼°å°ˆå®¶ã€‚è«‹åˆ¤æ–·ã€Œæ–‡ä»¶æ®µè½ã€èˆ‡ã€ŒæŸ¥è©¢å•é¡Œã€çš„ç›¸é—œç¨‹åº¦ã€‚
è©•åˆ†æ¨™æº–ï¼ˆ0-10 åˆ†ï¼‰ï¼š
- 9-10ï¼šæ–‡ä»¶ç›´æ¥ä¸”å®Œæ•´åœ°å›ç­”äº†å•é¡Œ
- 7-8ï¼šæ–‡ä»¶åŒ…å«å¤§éƒ¨åˆ†é—œéµè³‡è¨Š
- 5-6ï¼šæ–‡ä»¶éƒ¨åˆ†ç›¸é—œ
- 3-4ï¼šç•¥æœ‰ç›¸é—œ
- 0-2ï¼šå®Œå…¨ç„¡é—œ
è«‹åªè¼¸å‡ºä¸€å€‹æ•´æ•¸ï¼ˆ0-10ï¼‰ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""


def rerank(query, candidates, top_k=RERANK_TOP_K):
    scored = []
    for cand in candidates:
        prompt = f"ã€å•é¡Œã€‘{query}\n\nã€æ–‡ä»¶ã€‘{cand['text']}"
        resp = call_llm(RERANK_SYSTEM, prompt, temperature=0.0, max_tokens=16)
        try:
            nums = re.findall(r'\d+', resp)
            score = min(max(float(nums[0]), 0), 10) if nums else 5.0
        except:
            score = 5.0
        scored.append({**cand, "rerank_score": score})
        time.sleep(0.1)
    scored.sort(key=lambda x: x["rerank_score"], reverse=True)
    return scored[:top_k]


# ============================================================
# Query ReWriteï¼ˆå£èªåŒ– â†’ æœå°‹èªå¥ï¼‰
# ============================================================
REWRITE_SYSTEM = """ä½ æ˜¯ä¸€å€‹ RAG æŸ¥è©¢é‡å¯«å°ˆå®¶ã€‚è«‹å°‡ä½¿ç”¨è€…çš„å£èªåŒ–å•é¡Œé‡å¯«ç‚ºé©åˆå‘é‡è³‡æ–™åº«æœå°‹çš„ç²¾ç¢ºæŸ¥è©¢èªå¥ã€‚

è¦å‰‡ï¼š
1. å°‡å£èªè¡¨é”è½‰ç‚ºæ­£å¼ç”¨èªï¼ˆä¾‹å¦‚ã€Œé‚£å€‹ç´™å¼µçš„å–®å­ã€â†’ã€Œç´™æœ¬å¸³å–®ã€ï¼‰
2. è£œå……é—œéµè©ï¼ˆä¾‹å¦‚ã€Œç™½ç™½çš„ã€â†’ã€Œç™½æ¿ ç©ºæ°£ æ°£æ³¡ã€ï¼‰
3. ä¿ç•™åŸæ„ï¼Œä¸è¦å›ç­”å•é¡Œ
4. ä½¿ç”¨ç¹é«”ä¸­æ–‡
5. åªè¼¸å‡ºé‡å¯«å¾Œçš„èªå¥ï¼Œä¸è¦è§£é‡‹"""


def rewrite_query(question: str) -> str:
    """å£èªåŒ–å•é¡Œ â†’ æœå°‹èªå¥"""
    rewritten = call_llm(REWRITE_SYSTEM, question, temperature=0.1, max_tokens=128)
    return rewritten.strip() if rewritten else question


# ============================================================
# LLM ç­”æ¡ˆç”Ÿæˆ
# ============================================================
ANSWER_SYSTEM = """ä½ æ˜¯å°ç£è‡ªä¾†æ°´å…¬å¸çš„ AI å®¢æœåŠ©æ‰‹ã€‚è«‹æ ¹æ“šã€Œåƒè€ƒè³‡æ–™ã€ç²¾æº–å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚

è¦å‰‡ï¼š
1. åªæ ¹æ“šåƒè€ƒè³‡æ–™ä¸­çš„å…§å®¹å›ç­”ï¼Œä¸è¦ç·¨é€ 
2. å›ç­”è¦è¦ªåˆ‡ã€å°ˆæ¥­ã€å®Œæ•´ï¼Œé©åˆä¸€èˆ¬æ°‘çœ¾ç†è§£
3. åŒ…å«æ‰€æœ‰ç›¸é—œçš„é—œéµè³‡è¨Šï¼ˆé‡‘é¡ã€æœŸé™ã€æµç¨‹ç­‰ï¼‰
4. ä½¿ç”¨ç¹é«”ä¸­æ–‡
5. ä¸è¦åŠ ã€Œæ ¹æ“šåƒè€ƒè³‡æ–™ã€ç­‰å‰ç¶´ï¼Œç›´æ¥å›ç­”"""


def generate_answer(question, chunks):
    context = ""
    for i, c in enumerate(chunks):
        context += f"ã€è³‡æ–™ {i+1}ã€‘\n{c['text']}\n\n"
    prompt = f"ã€åƒè€ƒè³‡æ–™ã€‘\n{context}\nã€ç”¨æˆ¶å•é¡Œã€‘\n{question}\n\nè«‹å›ç­”ï¼š"
    return call_llm(ANSWER_SYSTEM, prompt)


# ============================================================
# å®Œæ•´ RAG Pipeline
# ============================================================
def rag_pipeline(query, qdrant_client, bm25_index, chunks,
                 use_rewrite=True) -> tuple[str, list[str]]:
    """
    å®Œæ•´ RAG æµç¨‹
    å›å‚³ (answer, retrieval_context_list)
    """
    # Query ReWrite
    search_query = rewrite_query(query) if use_rewrite else query

    # Dense Search
    emb, _ = get_embedding([search_query])
    if emb is None:
        return "", []

    dense_raw = qdrant_client.query_points(
        collection_name=COLLECTION_NAME, query=emb[0], limit=DENSE_TOP_K
    )
    dense_results = [{"index": p.id, "text": p.payload["text"],
                      "source": p.payload["source"], "dense_score": p.score}
                     for p in dense_raw.points]

    # BM25 Search
    bm25_results = bm25_index.search(search_query, BM25_TOP_K)

    # RRF Fusion
    hybrid = rrf_fusion(dense_results, bm25_results, top_k=HYBRID_TOP_K)

    # ReRank
    reranked = rerank(query, hybrid, RERANK_TOP_K)

    # æ”¶é›† contextï¼ˆDeepEval éœ€è¦ï¼‰
    retrieval_context = [c["text"] for c in reranked]

    # LLM Answer
    answer = generate_answer(query, reranked)

    return answer, retrieval_context


# ============================================================
# DeepEval è©•ä¼°
# ============================================================
def evaluate_with_deepeval(question, answer, expected_answer,
                           retrieval_context, eval_llm):
    """ä½¿ç”¨ DeepEval è©•ä¼° 5 é …æŒ‡æ¨™"""

    test_case = LLMTestCase(
        input=question,
        actual_output=answer,
        expected_output=expected_answer,
        retrieval_context=retrieval_context,
    )

    metrics = {
        "Faithfulness": FaithfulnessMetric(model=eval_llm, threshold=0.5),
        "Answer_Relevancy": AnswerRelevancyMetric(model=eval_llm, threshold=0.5),
        "Contextual_Recall": ContextualRecallMetric(model=eval_llm, threshold=0.5),
        "Contextual_Precision": ContextualPrecisionMetric(model=eval_llm, threshold=0.5),
        "Contextual_Relevancy": ContextualRelevancyMetric(model=eval_llm, threshold=0.5),
    }

    scores = {}
    for name, metric in metrics.items():
        try:
            metric.measure(test_case)
            scores[name] = round(metric.score, 4)
        except Exception as e:
            err_msg = str(e)[:120]
            print(f"    âš ï¸ {name} è©•ä¼°å¤±æ•—: {err_msg}")
            # å¦‚æœæ˜¯ timeout/JSON éŒ¯èª¤ï¼Œç­‰å¾…å¾Œé‡è©¦ä¸€æ¬¡
            if "timeout" in err_msg.lower() or "JSON" in err_msg or "524" in err_msg:
                print(f"    ğŸ”„ ç­‰å¾… 15 ç§’å¾Œé‡è©¦ {name}...")
                time.sleep(15)
                try:
                    metric.measure(test_case)
                    scores[name] = round(metric.score, 4)
                    print(f"    âœ… {name} é‡è©¦æˆåŠŸ: {scores[name]}")
                except Exception as e2:
                    print(f"    âŒ {name} é‡è©¦ä»å¤±æ•—: {str(e2)[:80]}")
                    scores[name] = 0.0
            else:
                scores[name] = 0.0
        time.sleep(2)  # æ¯å€‹æŒ‡æ¨™é–“éš” 2 ç§’

    return scores


# ============================================================
# ä¸»ç¨‹å¼
# ============================================================
def main():
    print("=" * 65)
    print("HW Day6ï¼šRAG AI å®¢æœåŠ©æ‰‹ + DeepEval è©•ä¼°")
    print("å°ç£è‡ªä¾†æ°´å…¬å¸ QA ç³»çµ±")
    print("=" * 65)

    # â”€â”€ 1. è®€å–è³‡æ–™ â”€â”€
    print("\nğŸ“‚ æ­¥é©Ÿ 1ï¼šè®€å–è³‡æ–™")
    print("-" * 40)

    # è®€å– qa_dataï¼ˆæ”¯æ´ .txt å’Œ .docxï¼‰
    qa_txt_path = os.path.join(SCRIPT_DIR, "qa_data.txt")
    qa_docx_path = os.path.join(SCRIPT_DIR, "qa_data.docx")
    if os.path.exists(qa_txt_path):
        with open(qa_txt_path, "r", encoding="utf-8") as f:
            qa_text = f.read()
    elif os.path.exists(qa_docx_path):
        # ç”¨ pandoc å°‡ docx è½‰ç‚ºç´”æ–‡å­—
        result = subprocess.run(
            ["pandoc", qa_docx_path, "-t", "plain", "--wrap=none"],
            capture_output=True, text=True
        )
        qa_text = result.stdout
        # ä¹Ÿå­˜ä¸€ä»½ txt ä¾›å¾ŒçºŒä½¿ç”¨
        with open(qa_txt_path, "w", encoding="utf-8") as f:
            f.write(qa_text)
    else:
        print("âŒ æ‰¾ä¸åˆ° qa_data.txt æˆ– qa_data.docx")
        return
    print(f"  âœ… qa_dataï¼š{len(qa_text)} å­—å…ƒ")

    # è®€å– questionsï¼ˆæ”¯æ´ .csv å’Œ .xlsxï¼‰
    q_csv_path = os.path.join(SCRIPT_DIR, "questions.csv")
    q_xlsx_path = os.path.join(SCRIPT_DIR, "day6_HW_questions.csv.xlsx")
    if os.path.exists(q_csv_path):
        with open(q_csv_path, "r", encoding="utf-8-sig") as f:
            questions = list(csv.DictReader(f))
    elif os.path.exists(q_xlsx_path):
        questions = []
        wb = openpyxl.load_workbook(q_xlsx_path)
        ws = wb.active
        headers = [cell.value for cell in next(ws.iter_rows(min_row=1, max_row=1))]
        for row in ws.iter_rows(min_row=2, values_only=True):
            d = {}
            for h, v in zip(headers, row):
                if isinstance(v, float) and v == int(v):
                    d[h] = int(v)
                elif v is None:
                    d[h] = ""
                else:
                    d[h] = v
            questions.append(d)
    else:
        print("âŒ æ‰¾ä¸åˆ° questions.csv æˆ– day6_HW_questions.csv.xlsx")
        return
    print(f"  âœ… questionsï¼š{len(questions)} é¡Œ")

    # è®€å–åƒè€ƒç­”æ¡ˆï¼ˆæ”¯æ´ .csv å’Œ .xlsxï¼‰
    qa_ans_csv = os.path.join(SCRIPT_DIR, "questions_answer.csv")
    qa_ans_xlsx = os.path.join(SCRIPT_DIR, "questions_answer.csv.xlsx")
    ref_answers = {}
    if os.path.exists(qa_ans_csv):
        with open(qa_ans_csv, "r", encoding="utf-8-sig") as f:
            for row in csv.DictReader(f):
                ref_answers[int(float(row["q_id"]))] = row["answer"]
    elif os.path.exists(qa_ans_xlsx):
        wb = openpyxl.load_workbook(qa_ans_xlsx)
        ws = wb.active
        for row in ws.iter_rows(min_row=2, values_only=True):
            q_id = int(float(row[0])) if row[0] else 0
            answer = row[2] if row[2] else ""
            ref_answers[q_id] = answer
    else:
        print("âš ï¸ æ‰¾ä¸åˆ°åƒè€ƒç­”æ¡ˆæª”æ¡ˆï¼ŒDeepEval éƒ¨åˆ†æŒ‡æ¨™å¯èƒ½å—å½±éŸ¿")
    print(f"  âœ… åƒè€ƒç­”æ¡ˆï¼š{len(ref_answers)} é¡Œ")

    # éš¨æ©ŸæŠ½æ¨£ N é¡Œ
    if 0 < SAMPLE_N < len(questions):
        random.seed(42)  # å›ºå®š seed è®“çµæœå¯é‡ç¾
        questions = random.sample(questions, SAMPLE_N)
        sampled_ids = [int(float(q["q_id"])) for q in questions]
        print(f"\n  ğŸ² éš¨æ©ŸæŠ½æ¨£ {SAMPLE_N} é¡Œï¼šQ{sampled_ids}")

    # â”€â”€ 2. åˆ‡å¡Š â”€â”€
    print(f"\nğŸ“¦ æ­¥é©Ÿ 2ï¼šæ»‘å‹•è¦–çª—åˆ‡å¡Šï¼ˆsize={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}ï¼‰")
    print("-" * 40)
    chunks = chunk_qa_data(qa_text)
    print(f"  âœ… å…± {len(chunks)} å€‹åˆ‡å¡Š")

    # â”€â”€ 3. Qdrant + BM25 â”€â”€
    print(f"\nğŸ”— æ­¥é©Ÿ 3ï¼šå»ºç«‹å‘é‡è³‡æ–™åº« + BM25 ç´¢å¼•")
    print("-" * 40)

    _, dim = get_embedding(["æ¸¬è©¦"])
    print(f"  âœ… Embedding ç¶­åº¦ï¼š{dim}")

    client = QdrantClient(url="http://localhost:6333")

    existing = [c.name for c in client.get_collections().collections]
    if COLLECTION_NAME in existing:
        client.delete_collection(COLLECTION_NAME)
    client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=VectorParams(size=dim, distance=Distance.COSINE),
    )

    all_points = []
    batch_size = 50
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i + batch_size]
        embs, _ = get_embedding([c["text"] for c in batch])
        if embs is None:
            continue
        for j, (chunk, emb) in enumerate(zip(batch, embs)):
            all_points.append(PointStruct(
                id=i + j, vector=emb,
                payload={"text": chunk["text"], "source": chunk["source"]},
            ))
        time.sleep(0.3)

    client.upsert(collection_name=COLLECTION_NAME, points=all_points)
    print(f"  âœ… Qdrantï¼š{len(all_points)} å€‹å‘é‡")

    bm25_index = BM25Index(chunks)
    print(f"  âœ… BM25 ç´¢å¼•å»ºç«‹å®Œæˆ")

    # â”€â”€ 4. RAG å›ç­” 30 é¡Œ â”€â”€
    print(f"\nğŸ” æ­¥é©Ÿ 4ï¼šRAG å›ç­” {len(questions)} é¡Œ")
    print(f"  æµç¨‹ï¼šQuery ReWrite â†’ Hybrid Search â†’ ReRank â†’ LLM Answer")
    print("-" * 40)

    # å˜—è©¦è¼‰å…¥å·²å®Œæˆçš„ RAG çµæœï¼ˆé¿å…é‡è¤‡å‘¼å«ï¼‰
    rag_checkpoint_path = os.path.join(SCRIPT_DIR, "rag_checkpoint.json")
    results = []
    loaded_rag = {}
    if os.path.exists(rag_checkpoint_path):
        with open(rag_checkpoint_path, "r", encoding="utf-8") as f:
            loaded_rag = {str(r["q_id"]): r for r in json.load(f)}
        print(f"  ğŸ“Œ æ‰¾åˆ° RAG é€²åº¦æª”ï¼Œå·²å®Œæˆ {len(loaded_rag)} é¡Œ")

    for q in questions:
        q_id = int(float(q["q_id"]))
        q_text = q["questions"]
        q_key = str(q_id)

        # å¦‚æœå·²æœ‰ RAG çµæœï¼Œè·³é
        if q_key in loaded_rag:
            r = loaded_rag[q_key]
            print(f"\n  â­ï¸ Q{q_id}: ä½¿ç”¨å·²æœ‰çµæœ")
            results.append(r)
            continue

        print(f"\n  Q{q_id}: {q_text[:50]}...")

        answer, context = rag_pipeline(q_text, client, bm25_index, chunks)
        print(f"    âœ… ç­”æ¡ˆï¼š{answer[:60]}...")

        results.append({
            "q_id": q_id,
            "questions": q_text,
            "answer": answer,
            "context": context,
            "expected": ref_answers.get(q_id, ""),
        })
        time.sleep(0.5)

    # å„²å­˜ RAG çµæœ
    with open(rag_checkpoint_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\n  ğŸ’¾ RAG çµæœå·²å„²å­˜è‡³ rag_checkpoint.json")

    # â”€â”€ 5. DeepEval è©•ä¼° â”€â”€
    print(f"\nğŸ“Š æ­¥é©Ÿ 5ï¼šDeepEval è©•ä¼° 5 é …æŒ‡æ¨™")
    print("-" * 40)

    eval_llm = CustomLLM()
    print(f"  âœ… è©•ä¼°æ¨¡å‹ï¼š{eval_llm.get_model_name()}")

    # è¼‰å…¥å·²æœ‰çš„è©•ä¼°é€²åº¦ï¼ˆæ–·é»çºŒè·‘ï¼‰
    checkpoint_path = os.path.join(SCRIPT_DIR, "eval_checkpoint.json")
    existing_scores = {}
    if os.path.exists(checkpoint_path):
        with open(checkpoint_path, "r", encoding="utf-8") as f:
            existing_scores = json.load(f)
        print(f"  ğŸ“Œ æ‰¾åˆ°è©•ä¼°é€²åº¦æª”ï¼Œå·²å®Œæˆ {len(existing_scores)} é¡Œï¼Œå¾æ–·é»ç¹¼çºŒ...")

    for i, r in enumerate(results):
        q_id = r["q_id"]
        q_key = str(q_id)

        # å¦‚æœå·²æœ‰è©•ä¼°çµæœï¼Œè·³é
        if q_key in existing_scores:
            r["scores"] = existing_scores[q_key]
            print(f"\n  â­ï¸ Q{q_id} å·²æœ‰è©•ä¼°çµæœï¼Œè·³é")
            for name, val in r["scores"].items():
                print(f"    {name}: {val}")
            continue

        print(f"\n  ğŸ“Š è©•ä¼° Q{q_id}ï¼ˆ{i+1}/{len(results)}ï¼‰...")

        # æ¯é¡Œè©•ä¼°å‰æš«åœï¼Œé¿å… API éè¼‰
        if i > 0:
            delay = 5  # æ¯é¡Œé–“éš” 5 ç§’
            print(f"  â³ ç­‰å¾… {delay} ç§’é¿å… API éè¼‰...")
            time.sleep(delay)

        scores = evaluate_with_deepeval(
            question=r["questions"],
            answer=r["answer"],
            expected_answer=r["expected"],
            retrieval_context=r["context"],
            eval_llm=eval_llm,
        )

        r["scores"] = scores
        for name, val in scores.items():
            print(f"    {name}: {val}")

        # å³æ™‚å„²å­˜é€²åº¦ï¼ˆæ–·é»ï¼‰
        existing_scores[q_key] = scores
        with open(checkpoint_path, "w", encoding="utf-8") as f:
            json.dump(existing_scores, f, ensure_ascii=False, indent=2)

    # è©•ä¼°å®Œæˆï¼Œåˆªé™¤ checkpoint
    if os.path.exists(checkpoint_path):
        os.remove(checkpoint_path)
    if os.path.exists(rag_checkpoint_path):
        os.remove(rag_checkpoint_path)
    print(f"\n  âœ… å…¨éƒ¨è©•ä¼°å®Œæˆï¼Œå·²æ¸…ç† checkpoint æª”æ¡ˆ")

    # â”€â”€ 6. è¼¸å‡º CSV â”€â”€
    print(f"\nğŸ’¾ æ­¥é©Ÿ 6ï¼šè¼¸å‡º day6_HW_questions.csv")
    print("-" * 40)

    csv_path = os.path.join(SCRIPT_DIR, "day6_HW_questions.csv")
    with open(csv_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([
            "q_id", "questions", "answer",
            "Faithfulness", "Answer_Relevancy",
            "Contextual_Recall", "Contextual_Precision", "Contextual_Relevancy"
        ])
        for r in results:
            s = r.get("scores", {})
            writer.writerow([
                r["q_id"], r["questions"], r["answer"],
                s.get("Faithfulness", 0),
                s.get("Answer_Relevancy", 0),
                s.get("Contextual_Recall", 0),
                s.get("Contextual_Precision", 0),
                s.get("Contextual_Relevancy", 0),
            ])
    print(f"  âœ… {csv_path}")

    # â”€â”€ 7. æ‘˜è¦ â”€â”€
    print(f"""
{'=' * 65}
âœ… HW Day6 å®Œæˆï¼
{'=' * 65}

ğŸ“‹ ç³»çµ±æ¶æ§‹ï¼š
  è³‡æ–™ä¾†æºï¼šqa_data.txtï¼ˆ{len(qa_text)} å­—å…ƒ â†’ {len(chunks)} å¡Šï¼‰
  åˆ‡å¡Šæ–¹æ³•ï¼šæ»‘å‹•è¦–çª—ï¼ˆsize={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}ï¼‰
  Dense Searchï¼šQdrant VDBï¼ˆ{dim} ç¶­ï¼‰
  Sparse Searchï¼šBM25ï¼ˆjieba ä¸­æ–‡åˆ†è©ï¼‰
  Hybrid Fusionï¼šRRFï¼ˆk={RRF_K}ï¼‰
  ReRankï¼šLLM ç›¸é—œæ€§è©•åˆ†
  Answer Genï¼š{LLM_MODEL}
  Evaluationï¼šDeepEvalï¼ˆ5 metricsï¼‰

ğŸ“Š DeepEval å¹³å‡åˆ†æ•¸ï¼š""")

    metric_names = ["Faithfulness", "Answer_Relevancy",
                    "Contextual_Recall", "Contextual_Precision",
                    "Contextual_Relevancy"]
    for m in metric_names:
        vals = [r["scores"].get(m, 0) for r in results if "scores" in r]
        avg = sum(vals) / len(vals) if vals else 0
        print(f"  {m}: {avg:.4f}")


if __name__ == "__main__":
    main()