"""
HW Day5ï¼šRAG æ–‡æœ¬åˆ‡å¡Šèˆ‡æª¢ç´¢è©•ä¼°
==================================
1. è®€å– data_01~05.txt
2. å¯¦ä½œä¸‰ç¨®åˆ‡å¡Šæ–¹æ³•ï¼šå›ºå®šå¤§å°ã€æ»‘å‹•è¦–çª—ã€èªæ„åˆ‡å¡Š
3. ä½¿ç”¨ Embedding API åµŒå…¥åˆ° Qdrant VDB
4. å° questions.csv ä¸­çš„ 20 é¡Œé€²è¡Œæª¢ç´¢
5. ä½¿ç”¨ API å–å¾—åˆ†æ•¸
6. è¼¸å‡º CSVï¼ˆ20é¡Œ Ã— 3æ–¹æ³• = 60ç­†ï¼‰
"""

import os
import re
import csv
import time
import requests
import numpy as np
from langchain_text_splitters import (
    CharacterTextSplitter,
    RecursiveCharacterTextSplitter,
)
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct


# ============================================================
# è¨­å®š
# ============================================================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# è‡ªå‹•å°‹æ‰¾ data è³‡æ–™å¤¾ï¼ˆæ”¯æ´å¤šç¨®ç›®éŒ„çµæ§‹ï¼‰
if os.path.isdir(os.path.join(SCRIPT_DIR, "data")):
    DATA_DIR = os.path.join(SCRIPT_DIR, "data")
elif os.path.isdir(os.path.join(SCRIPT_DIR, "..", "data")):
    DATA_DIR = os.path.join(SCRIPT_DIR, "..", "data")
else:
    DATA_DIR = SCRIPT_DIR

# è‡ªå‹•å°‹æ‰¾ questions.csv
QUESTIONS_PATH = None
for p in [
    os.path.join(SCRIPT_DIR, "questions.csv"),
    os.path.join(SCRIPT_DIR, "..", "questions.csv"),
    os.path.join(DATA_DIR, "questions.csv"),
]:
    if os.path.exists(p):
        QUESTIONS_PATH = p
        break

# API è¨­å®š
EMBED_API_URL = "https://ws-04.wade0426.me/embed"
# TODO: è«‹æ ¹æ“š API èªªæ˜æ–‡ä»¶ç¢ºèªè©•åˆ† API çš„ URL å’Œ Payload æ ¼å¼
SCORE_API_URL = "https://ws-04.wade0426.me/score"

# åˆ‡å¡Šåƒæ•¸
FIXED_CHUNK_SIZE = 300
FIXED_CHUNK_OVERLAP = 0
SLIDING_CHUNK_SIZE = 300
SLIDING_CHUNK_OVERLAP = 100
SEMANTIC_SIMILARITY_THRESHOLD = 0.5

STUDENT_ID = "1411232019"  # TODO: è«‹å¡«å…¥ä½ çš„å­¸è™Ÿ


# ============================================================
# å·¥å…·å‡½æ•¸
# ============================================================
def get_embedding(texts: list[str]) -> tuple:
    """ä½¿ç”¨ Embedding API å–å¾—æ–‡æœ¬å‘é‡ï¼Œå›å‚³ (embeddings, dimension)"""
    data = {"texts": texts, "normalize": True, "batch_size": 32}
    try:
        resp = requests.post(EMBED_API_URL, json=data, timeout=60)
        if resp.status_code == 200:
            result = resp.json()
            return result["embeddings"], result["dimension"]
        print(f"  âŒ Embedding API éŒ¯èª¤: {resp.status_code}")
        return None, None
    except Exception as e:
        print(f"  âŒ Embedding API é€£ç·šå¤±æ•—: {e}")
        return None, None


def get_score_from_api(question: str, retrieve_text: str, source: str):
    """
    ä½¿ç”¨è©•åˆ† API å–å¾—åˆ†æ•¸
    TODO: è«‹æ ¹æ“š API èªªæ˜æ–‡ä»¶èª¿æ•´ URL å’Œ payload æ ¼å¼
    å¦‚æœ API ä¸å¯ç”¨ï¼Œå›å‚³ Noneï¼ˆå°‡æ”¹ç”¨å‘é‡ç›¸ä¼¼åº¦ï¼‰
    """
    payload = {
        "question": question,
        "retrieve_text": retrieve_text,
        "source": source,
    }
    try:
        resp = requests.post(SCORE_API_URL, json=payload, timeout=60)
        if resp.status_code == 200:
            return resp.json().get("score", 0.0)
        return None
    except Exception:
        return None


def cosine_similarity(vec1, vec2) -> float:
    """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
    v1, v2 = np.array(vec1), np.array(vec2)
    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-10))


def read_data_files(data_dir: str) -> dict:
    """è®€å– data è³‡æ–™å¤¾ä¸­çš„ data_*.txt"""
    data = {}
    for fn in sorted(os.listdir(data_dir)):
        if fn.startswith("data_") and fn.endswith(".txt"):
            path = os.path.join(data_dir, fn)
            with open(path, "r", encoding="utf-8") as f:
                data[fn] = f.read()
            print(f"  âœ… {fn}ï¼š{len(data[fn])} å­—å…ƒ")
    return data


def read_questions(csv_path: str) -> list[dict]:
    """è®€å– questions.csv"""
    questions = []
    with open(csv_path, "r", encoding="utf-8-sig") as f:
        for row in csv.DictReader(f):
            questions.append(row)
    return questions


def build_csv(results: list[dict], output_path: str):
    """å»ºç«‹ CSVï¼ˆutf-8-sig ç·¨ç¢¼ï¼‰"""
    fields = ["id", "q_id", "method", "retrieve_text", "score", "source"]
    with open(output_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fields)
        writer.writeheader()
        writer.writerows(results)
    print(f"  âœ… CSVï¼š{output_path}ï¼ˆ{len(results)} ç­†ï¼‰")


# ============================================================
# ä¸‰ç¨®åˆ‡å¡Šæ–¹æ³•
# ============================================================
def fixed_size_chunking(text: str, source: str) -> list[dict]:
    """
    å›ºå®šå¤§å°åˆ‡å¡Š (Fixed-size Chunking)
    - ç´”ç²¹æŒ‰å­—å…ƒæ•¸åˆ‡å‰²ï¼Œä¸è€ƒæ…®èªæ„é‚Šç•Œ
    - chunk_size=300, overlap=0
    """
    splitter = CharacterTextSplitter(
        separator="",
        chunk_size=FIXED_CHUNK_SIZE,
        chunk_overlap=FIXED_CHUNK_OVERLAP,
        length_function=len,
    )
    return [{"text": c, "source": source, "method": "å›ºå®šå¤§å°"}
            for c in splitter.split_text(text)]


def sliding_window_chunking(text: str, source: str) -> list[dict]:
    """
    æ»‘å‹•è¦–çª—åˆ‡å¡Š (Sliding Window)
    - ä½¿ç”¨ä¸­æ–‡èªæ„é‚Šç•Œåˆ†éš”ç¬¦
    - chunk_size=300, overlap=100
    """
    splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", ""],
        chunk_size=SLIDING_CHUNK_SIZE,
        chunk_overlap=SLIDING_CHUNK_OVERLAP,
        length_function=len,
    )
    return [{"text": c, "source": source, "method": "æ»‘å‹•è¦–çª—"}
            for c in splitter.split_text(text)]


def semantic_chunking(text: str, source: str) -> list[dict]:
    """
    èªæ„åˆ‡å¡Š (Semantic Chunking)
    1. æŒ‰å¥å­åˆ‡åˆ†
    2. è¨ˆç®—ç›¸é„°å¥å­çš„ embedding é¤˜å¼¦ç›¸ä¼¼åº¦
    3. åœ¨ç›¸ä¼¼åº¦ä½æ–¼é–€æª»è™•æ–·é–‹ â†’ èªæ„æ®µè½
    4. éé•·æ®µè½å†ç´°åˆ‡
    """
    # æŒ‰ä¸­æ–‡å¥è™Ÿ/æ›è¡Œåˆ‡åˆ†æˆå¥å­
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ\n])', text)
    sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 5]

    if len(sentences) <= 2:
        return [{"text": text.strip(), "source": source, "method": "èªæ„åˆ‡å¡Š"}]

    # åˆ†æ‰¹å–å¾— embedding
    all_embs = []
    for i in range(0, len(sentences), 50):
        batch = sentences[i:i + 50]
        embs, _ = get_embedding(batch)
        if embs is None:
            print(f"    âš ï¸ embedding å¤±æ•—ï¼Œæ”¹ç”¨æ»‘å‹•è¦–çª—")
            return sliding_window_chunking(text, source)
        all_embs.extend(embs)
        time.sleep(0.2)

    # è¨ˆç®—ç›¸é„°å¥å­ç›¸ä¼¼åº¦ â†’ æ‰¾æ–·é»
    chunks_text = []
    current = [sentences[0]]

    for i in range(len(all_embs) - 1):
        sim = cosine_similarity(all_embs[i], all_embs[i + 1])
        if sim < SEMANTIC_SIMILARITY_THRESHOLD:
            chunk = "".join(current).strip()
            if chunk:
                chunks_text.append(chunk)
            current = [sentences[i + 1]]
        else:
            current.append(sentences[i + 1])

    if current:
        chunk = "".join(current).strip()
        if chunk:
            chunks_text.append(chunk)

    # å¤ªé•·çš„ chunk å†ç´°åˆ‡
    final = []
    for c in chunks_text:
        if len(c) > 500:
            sub = RecursiveCharacterTextSplitter(
                separators=["ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", ""],
                chunk_size=400, chunk_overlap=50, length_function=len,
            )
            final.extend(sub.split_text(c))
        else:
            final.append(c)

    return [{"text": c, "source": source, "method": "èªæ„åˆ‡å¡Š"} for c in final]


# ============================================================
# VDB æ“ä½œ
# ============================================================
def build_collection(client: QdrantClient, name: str,
                     chunks: list[dict], dim: int):
    """åµŒå…¥åˆ‡å¡Šåˆ° Qdrant Collection"""
    existing = [c.name for c in client.get_collections().collections]
    if name in existing:
        client.delete_collection(name)

    client.create_collection(
        collection_name=name,
        vectors_config=VectorParams(size=dim, distance=Distance.COSINE),
    )

    all_points = []
    for i in range(0, len(chunks), 50):
        batch = chunks[i:i + 50]
        embs, _ = get_embedding([c["text"] for c in batch])
        if embs is None:
            continue
        for j, (chunk, emb) in enumerate(zip(batch, embs)):
            all_points.append(PointStruct(
                id=i + j, vector=emb,
                payload={"text": chunk["text"], "source": chunk["source"]},
            ))
        time.sleep(0.2)

    client.upsert(collection_name=name, points=all_points)
    print(f"  âœ… {name}ï¼š{len(all_points)} å€‹å‘é‡")


def search_top1(client: QdrantClient, collection: str, query: str) -> dict:
    """æœå°‹ Top-1 æœ€ç›¸ä¼¼åˆ‡å¡Š"""
    emb, _ = get_embedding([query])
    if emb is None:
        return {"text": "", "source": "", "score": 0.0}

    res = client.query_points(collection_name=collection, query=emb[0], limit=1)
    if res.points:
        p = res.points[0]
        return {"text": p.payload["text"], "source": p.payload["source"], "score": p.score}
    return {"text": "", "source": "", "score": 0.0}


# ============================================================
# ä¸»ç¨‹å¼
# ============================================================
def main():
    print("=" * 60)
    print("HW Day5ï¼šRAG æ–‡æœ¬åˆ‡å¡Šèˆ‡æª¢ç´¢è©•ä¼°")
    print("=" * 60)

    # â”€â”€ 1. è®€å–è³‡æ–™ â”€â”€
    print(f"\nğŸ“‚ æ­¥é©Ÿ 1ï¼šè®€å–è³‡æ–™")
    print(f"  DATA_DIR = {DATA_DIR}")
    print(f"  QUESTIONS = {QUESTIONS_PATH}")
    print("-" * 40)

    data_files = read_data_files(DATA_DIR)
    if not data_files:
        print("âŒ æ‰¾ä¸åˆ° data_01~05.txtï¼")
        return

    questions = read_questions(QUESTIONS_PATH)
    print(f"\n  å…± {len(data_files)} å€‹æª”æ¡ˆã€{len(questions)} å€‹å•é¡Œ")

    # â”€â”€ 2. ä¸‰ç¨®åˆ‡å¡Š â”€â”€
    print(f"\nğŸ“¦ æ­¥é©Ÿ 2ï¼šä¸‰ç¨®åˆ‡å¡Šæ–¹æ³•")
    print("-" * 40)

    all_chunks = {"å›ºå®šå¤§å°": [], "æ»‘å‹•è¦–çª—": [], "èªæ„åˆ‡å¡Š": []}

    for filename, content in data_files.items():
        print(f"\n  ğŸ“„ {filename}")

        fc = fixed_size_chunking(content, filename)
        all_chunks["å›ºå®šå¤§å°"].extend(fc)
        print(f"     å›ºå®šå¤§å°ï¼š{len(fc)} å¡Š")

        sc = sliding_window_chunking(content, filename)
        all_chunks["æ»‘å‹•è¦–çª—"].extend(sc)
        print(f"     æ»‘å‹•è¦–çª—ï¼š{len(sc)} å¡Š")

        sec = semantic_chunking(content, filename)
        all_chunks["èªæ„åˆ‡å¡Š"].extend(sec)
        print(f"     èªæ„åˆ‡å¡Šï¼š{len(sec)} å¡Š")

    for m, chunks in all_chunks.items():
        print(f"\n  ğŸ“Š {m} ç¸½è¨ˆï¼š{len(chunks)} å¡Š")

    # â”€â”€ 3. é€£æ¥ Qdrant â”€â”€
    print(f"\nğŸ”— æ­¥é©Ÿ 3ï¼šé€£æ¥ Qdrant & Embedding API")
    print("-" * 40)

    _, dim = get_embedding(["æ¸¬è©¦"])
    if dim is None:
        print("âŒ Embedding API ä¸å¯ç”¨ï¼Œè«‹ç¢ºèª API é€£ç·š")
        return
    print(f"  å‘é‡ç¶­åº¦ï¼š{dim}")

    client = QdrantClient(url="http://localhost:6333")
    print("  âœ… Qdrant é€£æ¥æˆåŠŸ")

    # â”€â”€ 4. åµŒå…¥ VDB â”€â”€
    print(f"\nğŸ“¤ æ­¥é©Ÿ 4ï¼šåµŒå…¥åˆ° Qdrant")
    print("-" * 40)

    collection_map = {
        "å›ºå®šå¤§å°": "fixed_chunks",
        "æ»‘å‹•è¦–çª—": "sliding_chunks",
        "èªæ„åˆ‡å¡Š": "semantic_chunks",
    }

    for method, col_name in collection_map.items():
        build_collection(client, col_name, all_chunks[method], dim)

    # â”€â”€ 5. æª¢ç´¢ & è©•åˆ† â”€â”€
    print(f"\nğŸ” æ­¥é©Ÿ 5ï¼šæª¢ç´¢ 20 é¡Œ Ã— 3 æ–¹æ³• = 60 ç­†")
    print("-" * 40)

    results = []
    row_id = 1
    score_api_available = None  # ç¬¬ä¸€æ¬¡å˜—è©¦å¾Œè¨˜ä½

    for q in questions:
        q_id = q["q_id"]
        q_text = q["questions"]
        print(f"\n  Q{q_id}: {q_text[:45]}...")

        for method, col_name in collection_map.items():
            hit = search_top1(client, col_name, q_text)

            # å˜—è©¦è©•åˆ† APIï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡å˜—è©¦ï¼‰
            if score_api_available is None:
                api_score = get_score_from_api(q_text, hit["text"], hit["source"])
                score_api_available = (api_score is not None)
                if score_api_available:
                    print("  ğŸ“¡ è©•åˆ† API å¯ç”¨ï¼")
                else:
                    print("  âš ï¸ è©•åˆ† API ä¸å¯ç”¨ï¼Œæ”¹ç”¨å‘é‡ç›¸ä¼¼åº¦åˆ†æ•¸")
                score = api_score if score_api_available else hit["score"]
            elif score_api_available:
                score = get_score_from_api(q_text, hit["text"], hit["source"]) or hit["score"]
            else:
                score = hit["score"]

            results.append({
                "id": row_id,
                "q_id": q_id,
                "method": method,
                "retrieve_text": hit["text"],
                "score": round(score, 6),
                "source": hit["source"],
            })
            print(f"      {method}: {score:.4f} | {hit['source']}")
            row_id += 1

        time.sleep(0.2)

    # â”€â”€ 6. è¼¸å‡º CSV â”€â”€
    print(f"\n{'=' * 60}")
    print("ğŸ“ æ­¥é©Ÿ 6ï¼šè¼¸å‡º CSV")
    print("=" * 60)

    csv_path = os.path.join(SCRIPT_DIR, f"{STUDENT_ID}_RAG_HW_01.csv")
    build_csv(results, csv_path)

    # â”€â”€ 7. çµ±è¨ˆåˆ†æ â”€â”€
    print(f"\nğŸ“Š å„æ–¹æ³•å¹³å‡åˆ†æ•¸")
    print("-" * 40)

    best_avg, best_method = 0, ""
    for method in collection_map:
        scores = [r["score"] for r in results if r["method"] == method]
        avg = sum(scores) / len(scores) if scores else 0
        print(f"  {method}ï¼šå¹³å‡ {avg:.6f}")
        if avg > best_avg:
            best_avg, best_method = avg, method

    print(f"\n  ğŸ† æœ€ä½³æ–¹æ³•ï¼š{best_method}ï¼ˆå¹³å‡ {best_avg:.6f}ï¼‰")

    print(f"""
{'=' * 60}
âœ… HW Day5 å®Œæˆï¼
{'=' * 60}

ğŸ“‹ åˆ‡å¡Šåƒæ•¸ï¼š
  å›ºå®šå¤§å°ï¼šchunk_size={FIXED_CHUNK_SIZE}, overlap={FIXED_CHUNK_OVERLAP}
  æ»‘å‹•è¦–çª—ï¼šchunk_size={SLIDING_CHUNK_SIZE}, overlap={SLIDING_CHUNK_OVERLAP}
  èªæ„åˆ‡å¡Šï¼šsimilarity_threshold={SEMANTIC_SIMILARITY_THRESHOLD}

ğŸ“Š åˆ‡å¡Šæ•¸é‡ï¼š
  å›ºå®šå¤§å°ï¼š{len(all_chunks['å›ºå®šå¤§å°'])} å¡Š
  æ»‘å‹•è¦–çª—ï¼š{len(all_chunks['æ»‘å‹•è¦–çª—'])} å¡Š
  èªæ„åˆ‡å¡Šï¼š{len(all_chunks['èªæ„åˆ‡å¡Š'])} å¡Š

ğŸ“ è¼¸å‡ºï¼š{csv_path}ï¼ˆ{len(results)} ç­†ï¼‰

""")


if __name__ == "__main__":
    main()