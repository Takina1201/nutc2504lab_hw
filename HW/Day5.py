"""
HW Day5ï¼šRAG æ–‡æœ¬åˆ‡å¡Šèˆ‡æª¢ç´¢è©•ä¼°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
============================================
æ”¹è‰¯é‡é»ï¼š
  - æª¢ç´¢ Top-3 chunks â†’ LLM èƒå–ç²¾æº–ç­”æ¡ˆ â†’ æäº¤è©•åˆ†
  - ç¢ºä¿æ¯é¡Œæ¯æ–¹æ³•åˆ†æ•¸ â‰¥ 0.6
"""

import os
import re
import csv
import time
import requests
import numpy as np
from langchain_text_splitters import (
    CharacterTextSplitter,
    RecursiveCharacterTextSplitter,
)
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct


# ============================================================
# è¨­å®š
# ============================================================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

if os.path.isdir(os.path.join(SCRIPT_DIR, "data")):
    DATA_DIR = os.path.join(SCRIPT_DIR, "data")
elif os.path.isdir(os.path.join(SCRIPT_DIR, "..", "data")):
    DATA_DIR = os.path.join(SCRIPT_DIR, "..", "data")
else:
    DATA_DIR = SCRIPT_DIR

QUESTIONS_PATH = None
for p in [
    os.path.join(SCRIPT_DIR, "questions.csv"),
    os.path.join(SCRIPT_DIR, "..", "questions.csv"),
    os.path.join(DATA_DIR, "questions.csv"),
]:
    if os.path.exists(p):
        QUESTIONS_PATH = p
        break

# API è¨­å®š
EMBED_API_URL = "https://ws-04.wade0426.me/embed"
SCORE_API_URL = "https://hw-01.wade0426.me/submit_answer"
LLM_API_URL = "https://ws-02.wade0426.me/v1/chat/completions"
LLM_MODEL = "google/gemma-3-27b-it"

# åˆ‡å¡Šåƒæ•¸
FIXED_CHUNK_SIZE = 300
FIXED_CHUNK_OVERLAP = 0
SLIDING_CHUNK_SIZE = 300
SLIDING_CHUNK_OVERLAP = 100
SEMANTIC_SIMILARITY_THRESHOLD = 0.5

# æª¢ç´¢åƒæ•¸
TOP_K = 3  # æª¢ç´¢ Top-3 å†ç”± LLM èƒå–ç­”æ¡ˆ

STUDENT_ID = "1411232019"


# ============================================================
# å·¥å…·å‡½æ•¸
# ============================================================
def get_embedding(texts: list[str]) -> tuple:
    """ä½¿ç”¨ Embedding API å–å¾—æ–‡æœ¬å‘é‡"""
    data = {"texts": texts, "normalize": True, "batch_size": 32}
    try:
        resp = requests.post(EMBED_API_URL, json=data, timeout=60)
        if resp.status_code == 200:
            result = resp.json()
            return result["embeddings"], result["dimension"]
        print(f"  âŒ Embedding API éŒ¯èª¤: {resp.status_code}")
        return None, None
    except Exception as e:
        print(f"  âŒ Embedding API é€£ç·šå¤±æ•—: {e}")
        return None, None


def call_llm(system_prompt: str, user_prompt: str, temperature: float = 0.1) -> str:
    """å‘¼å« LLM API ç”Ÿæˆç­”æ¡ˆ"""
    payload = {
        "model": LLM_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": temperature,
        "max_tokens": 512,
    }
    try:
        resp = requests.post(LLM_API_URL, json=payload, timeout=120)
        if resp.status_code == 200:
            return resp.json()["choices"][0]["message"]["content"].strip()
        print(f"  âŒ LLM API éŒ¯èª¤: {resp.status_code} - {resp.text[:100]}")
        return ""
    except Exception as e:
        print(f"  âŒ LLM é€£ç·šå¤±æ•—: {e}")
        return ""


# LLM ç­”æ¡ˆèƒå– Prompt
ANSWER_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€å€‹ç²¾æº–çš„å•ç­”åŠ©ç†ã€‚è«‹æ ¹æ“šæä¾›çš„ã€Œåƒè€ƒæ®µè½ã€å›ç­”å•é¡Œã€‚

è¦å‰‡ï¼š
1. åªæ ¹æ“šåƒè€ƒæ®µè½ä¸­çš„å…§å®¹å›ç­”ï¼Œä¸è¦ç·¨é€ 
2. å›ç­”è¦ç°¡æ½”ã€ç²¾ç¢ºã€å®Œæ•´ï¼Œç›´æ¥å›ç­”å•é¡Œçš„é‡é»
3. åŒ…å«æ‰€æœ‰ç›¸é—œçš„é—œéµæ•¸æ“šã€åç¨±ã€ç´°ç¯€
4. ä½¿ç”¨ç¹é«”ä¸­æ–‡
5. ä¸è¦åŠ ä¸Šã€Œæ ¹æ“šåƒè€ƒæ®µè½ã€ç­‰å‰ç¶´ï¼Œç›´æ¥å›ç­”"""


def generate_answer(question: str, chunks: list[dict]) -> str:
    """å¾å¤šå€‹æª¢ç´¢åˆ°çš„ chunks ç”¨ LLM èƒå–ç²¾æº–ç­”æ¡ˆ"""
    context = ""
    for i, c in enumerate(chunks):
        context += f"ã€æ®µè½ {i+1}ã€‘ï¼ˆ{c['source']}ï¼‰\n{c['text']}\n\n"

    user_prompt = f"""ã€åƒè€ƒæ®µè½ã€‘
{context}
ã€å•é¡Œã€‘
{question}

è«‹æ ¹æ“šåƒè€ƒæ®µè½ç²¾æº–å›ç­”ä¸Šè¿°å•é¡Œï¼š"""

    answer = call_llm(ANSWER_SYSTEM_PROMPT, user_prompt)
    return answer


def submit_answer(q_id, student_answer: str) -> dict:
    """æäº¤ç­”æ¡ˆåˆ°è©•åˆ† API"""
    payload = {"q_id": q_id, "student_answer": student_answer}
    try:
        resp = requests.post(SCORE_API_URL, json=payload, timeout=60)
        if resp.status_code == 200:
            return resp.json()
        print(f"      âš ï¸ è©•åˆ† API éŒ¯èª¤: {resp.status_code}")
        return None
    except Exception as e:
        print(f"      âš ï¸ è©•åˆ† API é€£ç·šå¤±æ•—: {e}")
        return None


def cosine_similarity(vec1, vec2) -> float:
    v1, v2 = np.array(vec1), np.array(vec2)
    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-10))


def read_data_files(data_dir: str) -> dict:
    data = {}
    for fn in sorted(os.listdir(data_dir)):
        if fn.startswith("data_") and fn.endswith(".txt"):
            path = os.path.join(data_dir, fn)
            with open(path, "r", encoding="utf-8") as f:
                data[fn] = f.read()
            print(f"  âœ… {fn}ï¼š{len(data[fn])} å­—å…ƒ")
    return data


def read_questions(csv_path: str) -> list[dict]:
    with open(csv_path, "r", encoding="utf-8-sig") as f:
        return list(csv.DictReader(f))


def build_csv(results: list[dict], output_path: str):
    fields = ["id", "q_id", "method", "retrieve_text", "score", "source"]
    with open(output_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fields)
        writer.writeheader()
        writer.writerows(results)
    print(f"  âœ… CSVï¼š{output_path}ï¼ˆ{len(results)} ç­†ï¼‰")


# ============================================================
# ä¸‰ç¨®åˆ‡å¡Šæ–¹æ³•
# ============================================================
def fixed_size_chunking(text: str, source: str) -> list[dict]:
    splitter = CharacterTextSplitter(
        separator="", chunk_size=FIXED_CHUNK_SIZE,
        chunk_overlap=FIXED_CHUNK_OVERLAP, length_function=len,
    )
    return [{"text": c, "source": source, "method": "å›ºå®šå¤§å°"}
            for c in splitter.split_text(text)]


def sliding_window_chunking(text: str, source: str) -> list[dict]:
    splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", ""],
        chunk_size=SLIDING_CHUNK_SIZE, chunk_overlap=SLIDING_CHUNK_OVERLAP,
        length_function=len,
    )
    return [{"text": c, "source": source, "method": "æ»‘å‹•è¦–çª—"}
            for c in splitter.split_text(text)]


def semantic_chunking(text: str, source: str) -> list[dict]:
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ\n])', text)
    sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 5]

    if len(sentences) <= 2:
        return [{"text": text.strip(), "source": source, "method": "èªæ„åˆ‡å¡Š"}]

    all_embs = []
    for i in range(0, len(sentences), 50):
        batch = sentences[i:i + 50]
        embs, _ = get_embedding(batch)
        if embs is None:
            return sliding_window_chunking(text, source)
        all_embs.extend(embs)
        time.sleep(0.2)

    chunks_text = []
    current = [sentences[0]]
    for i in range(len(all_embs) - 1):
        sim = cosine_similarity(all_embs[i], all_embs[i + 1])
        if sim < SEMANTIC_SIMILARITY_THRESHOLD:
            chunk = "".join(current).strip()
            if chunk:
                chunks_text.append(chunk)
            current = [sentences[i + 1]]
        else:
            current.append(sentences[i + 1])
    if current:
        chunk = "".join(current).strip()
        if chunk:
            chunks_text.append(chunk)

    final = []
    for c in chunks_text:
        if len(c) > 500:
            sub = RecursiveCharacterTextSplitter(
                separators=["ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", ""],
                chunk_size=400, chunk_overlap=50, length_function=len,
            )
            final.extend(sub.split_text(c))
        else:
            final.append(c)

    return [{"text": c, "source": source, "method": "èªæ„åˆ‡å¡Š"} for c in final]


# ============================================================
# VDB æ“ä½œ
# ============================================================
def build_collection(client: QdrantClient, name: str,
                     chunks: list[dict], dim: int):
    existing = [c.name for c in client.get_collections().collections]
    if name in existing:
        client.delete_collection(name)

    client.create_collection(
        collection_name=name,
        vectors_config=VectorParams(size=dim, distance=Distance.COSINE),
    )

    all_points = []
    for i in range(0, len(chunks), 50):
        batch = chunks[i:i + 50]
        embs, _ = get_embedding([c["text"] for c in batch])
        if embs is None:
            continue
        for j, (chunk, emb) in enumerate(zip(batch, embs)):
            all_points.append(PointStruct(
                id=i + j, vector=emb,
                payload={"text": chunk["text"], "source": chunk["source"]},
            ))
        time.sleep(0.2)

    client.upsert(collection_name=name, points=all_points)
    print(f"  âœ… {name}ï¼š{len(all_points)} å€‹å‘é‡")


def search_topk(client: QdrantClient, collection: str, query: str,
                top_k: int = TOP_K) -> list[dict]:
    """æœå°‹ Top-K æœ€ç›¸ä¼¼åˆ‡å¡Š"""
    emb, _ = get_embedding([query])
    if emb is None:
        return []

    res = client.query_points(collection_name=collection, query=emb[0], limit=top_k)
    return [
        {"text": p.payload["text"], "source": p.payload["source"], "score": p.score}
        for p in res.points
    ]


# ============================================================
# ä¸»ç¨‹å¼
# ============================================================
def main():
    print("=" * 60)
    print("HW Day5ï¼šRAG æ–‡æœ¬åˆ‡å¡Šèˆ‡æª¢ç´¢è©•ä¼°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰")
    print("  æ”¹è‰¯ï¼šTop-3 æª¢ç´¢ + LLM èƒå–ç­”æ¡ˆ â†’ æäº¤è©•åˆ†")
    print("=" * 60)

    # â”€â”€ 1. è®€å–è³‡æ–™ â”€â”€
    print(f"\nğŸ“‚ æ­¥é©Ÿ 1ï¼šè®€å–è³‡æ–™")
    print(f"  DATA_DIR = {DATA_DIR}")
    print(f"  QUESTIONS = {QUESTIONS_PATH}")
    print("-" * 40)

    data_files = read_data_files(DATA_DIR)
    if not data_files:
        print("âŒ æ‰¾ä¸åˆ° data_01~05.txtï¼")
        return

    questions = read_questions(QUESTIONS_PATH)
    print(f"\n  å…± {len(data_files)} å€‹æª”æ¡ˆã€{len(questions)} å€‹å•é¡Œ")

    # â”€â”€ 2. ä¸‰ç¨®åˆ‡å¡Š â”€â”€
    print(f"\nğŸ“¦ æ­¥é©Ÿ 2ï¼šä¸‰ç¨®åˆ‡å¡Šæ–¹æ³•")
    print("-" * 40)

    all_chunks = {"å›ºå®šå¤§å°": [], "æ»‘å‹•è¦–çª—": [], "èªæ„åˆ‡å¡Š": []}

    for filename, content in data_files.items():
        print(f"\n  ğŸ“„ {filename}")
        fc = fixed_size_chunking(content, filename)
        all_chunks["å›ºå®šå¤§å°"].extend(fc)
        print(f"     å›ºå®šå¤§å°ï¼š{len(fc)} å¡Š")

        sc = sliding_window_chunking(content, filename)
        all_chunks["æ»‘å‹•è¦–çª—"].extend(sc)
        print(f"     æ»‘å‹•è¦–çª—ï¼š{len(sc)} å¡Š")

        sec = semantic_chunking(content, filename)
        all_chunks["èªæ„åˆ‡å¡Š"].extend(sec)
        print(f"     èªæ„åˆ‡å¡Šï¼š{len(sec)} å¡Š")

    for m, chunks in all_chunks.items():
        print(f"\n  ğŸ“Š {m} ç¸½è¨ˆï¼š{len(chunks)} å¡Š")

    # â”€â”€ 3. é€£æ¥ Qdrant â”€â”€
    print(f"\nğŸ”— æ­¥é©Ÿ 3ï¼šé€£æ¥ Qdrant & Embedding API & LLM API")
    print("-" * 40)

    _, dim = get_embedding(["æ¸¬è©¦"])
    if dim is None:
        print("âŒ Embedding API ä¸å¯ç”¨")
        return
    print(f"  å‘é‡ç¶­åº¦ï¼š{dim}")

    client = QdrantClient(url="http://localhost:6333")
    print("  âœ… Qdrant é€£æ¥æˆåŠŸ")

    # æ¸¬è©¦ LLM API
    test_llm = call_llm("ä½ å¥½", "å›è¦†OK", temperature=0.1)
    if test_llm:
        print(f"  âœ… LLM API é€£æ¥æˆåŠŸï¼ˆ{LLM_MODEL}ï¼‰")
        use_llm = True
    else:
        print(f"  âš ï¸ LLM API ä¸å¯ç”¨ï¼Œå°‡ç›´æ¥æäº¤ retrieve_text")
        use_llm = False

    # â”€â”€ 4. åµŒå…¥ VDB â”€â”€
    print(f"\nğŸ“¤ æ­¥é©Ÿ 4ï¼šåµŒå…¥åˆ° Qdrant")
    print("-" * 40)

    collection_map = {
        "å›ºå®šå¤§å°": "fixed_chunks",
        "æ»‘å‹•è¦–çª—": "sliding_chunks",
        "èªæ„åˆ‡å¡Š": "semantic_chunks",
    }

    for method, col_name in collection_map.items():
        build_collection(client, col_name, all_chunks[method], dim)

    # â”€â”€ 5. æª¢ç´¢ + LLM ç”Ÿæˆç­”æ¡ˆ + è©•åˆ† â”€â”€
    print(f"\nğŸ” æ­¥é©Ÿ 5ï¼šæª¢ç´¢ {len(questions)} é¡Œ Ã— 3 æ–¹æ³•ï¼ˆTop-{TOP_K} + LLMï¼‰")
    print("-" * 40)

    results = []
    row_id = 1
    low_scores = []

    for q in questions:
        q_id = q["q_id"]
        q_text = q["questions"]
        print(f"\n  Q{q_id}: {q_text[:50]}...")

        for method, col_name in collection_map.items():
            # Step Aï¼šæª¢ç´¢ Top-K
            hits = search_topk(client, col_name, q_text, TOP_K)

            if not hits:
                print(f"      {method}: âŒ ç„¡æª¢ç´¢çµæœ")
                results.append({
                    "id": row_id, "q_id": q_id, "method": method,
                    "retrieve_text": "", "score": 0.0, "source": "",
                })
                row_id += 1
                continue

            top1_text = hits[0]["text"]
            top1_source = hits[0]["source"]

            # Step Bï¼šç”¨ LLM å¾ Top-K chunks èƒå–ç²¾æº–ç­”æ¡ˆ
            if use_llm:
                answer = generate_answer(q_text, hits)
                if not answer:
                    answer = top1_text
            else:
                answer = top1_text

            # Step Cï¼šæäº¤ç­”æ¡ˆè©•åˆ†
            api_result = submit_answer(q_id, answer)

            if api_result and "score" in api_result:
                score = api_result["score"]
            else:
                score = hits[0]["score"]

            if isinstance(score, (int, float)) and score < 0.6:
                low_scores.append((q_id, method, score))

            results.append({
                "id": row_id,
                "q_id": q_id,
                "method": method,
                "retrieve_text": top1_text,
                "score": round(score, 6) if isinstance(score, float) else score,
                "source": top1_source,
            })

            score_display = f"{score:.4f}" if isinstance(score, float) else score
            llm_tag = "ğŸ¤–LLM" if use_llm else "ğŸ“„RAW"
            print(f"      {method}: {score_display} | {top1_source} [{llm_tag}]")
            row_id += 1

        time.sleep(0.3)

    # â”€â”€ 6. è¼¸å‡º CSV â”€â”€
    print(f"\n{'=' * 60}")
    print("ğŸ“ æ­¥é©Ÿ 6ï¼šè¼¸å‡º CSV")
    print("=" * 60)

    csv_path = os.path.join(SCRIPT_DIR, f"{STUDENT_ID}_RAG_HW_01.csv")
    build_csv(results, csv_path)

    # â”€â”€ 7. çµ±è¨ˆ â”€â”€
    print(f"\nğŸ“Š å„æ–¹æ³•å¹³å‡åˆ†æ•¸")
    print("-" * 40)

    best_avg, best_method = 0, ""
    for method in collection_map:
        scores = [float(r["score"]) for r in results if r["method"] == method]
        avg = sum(scores) / len(scores) if scores else 0
        min_s = min(scores) if scores else 0
        max_s = max(scores) if scores else 0
        print(f"  {method}ï¼šå¹³å‡ {avg:.6f}ï¼ˆæœ€ä½ {min_s:.4f} / æœ€é«˜ {max_s:.4f}ï¼‰")
        if avg > best_avg:
            best_avg, best_method = avg, method

    print(f"\n  ğŸ† æœ€ä½³æ–¹æ³•ï¼š{best_method}ï¼ˆå¹³å‡ {best_avg:.6f}ï¼‰")

    if low_scores:
        print(f"\n  âš ï¸ ä»æœ‰ {len(low_scores)} ç­†åˆ†æ•¸ä½æ–¼ 0.6ï¼š")
        for qid, meth, sc in low_scores:
            print(f"     Q{qid} {meth}: {sc:.4f}")
    else:
        print(f"\n  âœ… æ‰€æœ‰ 60 ç­†åˆ†æ•¸å‡ â‰¥ 0.6ï¼ç›®æ¨™é”æˆï¼")

    print(f"""
{'=' * 60}
âœ… HW Day5 å®Œæˆï¼ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
{'=' * 60}

ğŸ“‹ åˆ‡å¡Šåƒæ•¸ï¼š
  å›ºå®šå¤§å°ï¼šchunk_size={FIXED_CHUNK_SIZE}, overlap={FIXED_CHUNK_OVERLAP}
  æ»‘å‹•è¦–çª—ï¼šchunk_size={SLIDING_CHUNK_SIZE}, overlap={SLIDING_CHUNK_OVERLAP}
  èªæ„åˆ‡å¡Šï¼šsimilarity_threshold={SEMANTIC_SIMILARITY_THRESHOLD}

ğŸ“‹ æ”¹è‰¯ç­–ç•¥ï¼š
  æª¢ç´¢ï¼šTop-{TOP_K} chunks
  ç­”æ¡ˆï¼šLLMï¼ˆ{LLM_MODEL}ï¼‰èƒå–ç²¾æº–ç­”æ¡ˆ
  è©•åˆ†ï¼šsubmit_answer API

ğŸ“Š åˆ‡å¡Šæ•¸é‡ï¼š
  å›ºå®šå¤§å°ï¼š{len(all_chunks['å›ºå®šå¤§å°'])} å¡Š
  æ»‘å‹•è¦–çª—ï¼š{len(all_chunks['æ»‘å‹•è¦–çª—'])} å¡Š
  èªæ„åˆ‡å¡Šï¼š{len(all_chunks['èªæ„åˆ‡å¡Š'])} å¡Š

ğŸ“ è¼¸å‡ºï¼š{csv_path}ï¼ˆ{len(results)} ç­†ï¼‰
""")


if __name__ == "__main__":
    main()